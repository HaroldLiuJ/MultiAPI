{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "facebook/dino-vits8", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "Vision Transformer (ViT) model trained using the DINO method. It was introduced in the paper Emerging Properties in Self-Supervised Vision Transformers by Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, Armand Joulin and first released in this repository.", "function_name": "facebook_dino_vits8", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def facebook_dino_vits8(image_path):\n    from transformers import ViTFeatureExtractor, ViTModel\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    feature_extractor = ViTFeatureExtractor.from_pretrained('facebook/dino-vits8')\n    model = ViTModel.from_pretrained('facebook/dino-vits8')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    return last_hidden_states\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "google/vit-base-patch16-224-in21k", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "The Vision Transformer (ViT) is a transformer encoder model (BERT-like) pretrained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. and first released in this repository. However, the weights were converted from the timm repository by Ross Wightman, who already converted the weights from JAX to PyTorch. Credits go to him.", "function_name": "google_vit_base_patch16_224_in21k", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed", "default_value": ""}}, "function_code": "def google_vit_base_patch16_224_in21k(image_path):\n    from transformers import ViTImageProcessor, ViTModel\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n    model = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n    inputs = processor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    last_hidden_states = outputs.last_hidden_state\n\n    return last_hidden_states\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Feature Extraction", "api_name": "microsoft/xclip-base-patch16-zero-shot", "python_environment_requirements": ["transformers"], "description": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.", "function_name": "microsoft_xclip_base_patch16_zero_shot", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file", "default_value": ""}, "labels": {"type": "str", "description": "Labels are the class labels or target labels associated with the videos or text used for classification or retrieval tasks in the X-CLIP model.", "default_value": ""}}, "function_code": "def microsoft_xclip_base_patch16_zero_shot(video_path, labels):\n    import av\n    import torch\n    import numpy as np\n\n    from transformers import AutoProcessor, AutoModel\n\n    def read_video_pyav(container, indices):\n        '''\n        Decode the video with PyAV decoder.\n        Args:\n            container (`av.container.input.InputContainer`): PyAV container.\n            indices (`List[int]`): List of frame indices to decode.\n        Returns:\n            result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n        '''\n        frames = []\n        container.seek(0)\n        start_index = indices[0]\n        end_index = indices[-1]\n        for i, frame in enumerate(container.decode(video=0)):\n            if i > end_index:\n                break\n            if i >= start_index and i in indices:\n                frames.append(frame)\n        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        '''\n        Sample a given number of frame indices from the video.\n        Args:\n            clip_len (`int`): Total number of frames to sample.\n            frame_sample_rate (`int`): Sample every n-th frame.\n            seg_len (`int`): Maximum allowed index of sample's last frame.\n        Returns:\n            indices (`List[int]`): List of sampled frame indices\n        '''\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    container = av.open(video_path)\n\n    # sample 8 frames\n    frame_sample_rate = container.streams.video[0].frames // 8\n    indices = sample_frame_indices(clip_len=8, frame_sample_rate=frame_sample_rate,\n                                   seg_len=container.streams.video[0].frames)\n    video = read_video_pyav(container, indices)\n\n    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch16-zero-shot\")\n    model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch16-zero-shot\").to(\"cuda\")\n\n    inputs = processor(\n        text=labels,\n        videos=list(video),\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(\"cuda\")\n\n    # forward pass\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n    probs = logits_per_video.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Generation", "api_name": "CompVis/stable-diffusion-v1-4", "python_environment_requirements": ["diffusers", "transformers", "scipy"], "description": "Stable Diffusion is a latent text-to-image diffusion model capable of generating photo-realistic images given any text input. The Stable-Diffusion-v1-4 checkpoint was fine-tuned on 225k steps at resolution 512x512 on laion-aesthetics v2 5+ and 10% dropping of the text-conditioning to improve classifier-free guidance sampling. This model is intended for research purposes and can be used for generating artworks, design, educational or creative tools, and research on generative models.", "function_name": "CompVis_stable_diffusion_v1_4", "function_arguments": {"prompt": {"type": "str", "description": "The text input prompt for generating images", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def CompVis_stable_diffusion_v1_4(prompt, output_path):\n    import torch\n    import os\n    from diffusers import StableDiffusionPipeline\n\n    model_id = 'CompVis/stable-diffusion-v1-4'\n    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n    pipe = pipe.to('cuda')\n\n    image = pipe(prompt).images[0]\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "stabilityai/sd-vae-ft-mse", "python_environment_requirements": ["diffusers"], "description": "This model is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It is designed to be used with the diffusers library and can be integrated into existing workflows by including a vae argument to the StableDiffusionPipeline. The model has been finetuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets and has been evaluated on COCO 2017 and LAION-Aesthetics 5+ datasets.", "function_name": "stabilityai_sd_vae_ft_mse", "function_arguments": {"prompt": {"type": "str", "description": "The prompt given to the VAE decoder for generating output", "default_value": ""}, "output_path": {"type": "str", "description": "The file path to save the output of the Stable Diffusion Pipeline model.", "default_value": ""}}, "function_code": "def stabilityai_sd_vae_ft_mse(prompt, output_path):\n    import torch\n    import os\n    from diffusers import StableDiffusionPipeline\n\n    model_id = 'stabilityai/sd-vae-ft-mse-original'\n    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n    pipe = pipe.to('cuda')\n\n    image = pipe(prompt).images[0]\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Realistic_Vision_V1.4", "python_environment_requirements": ["transformers", "torch"], "description": "Realistic_Vision_V1.4 is a text-to-image model that generates high-quality and detailed images based on textual prompts. It can be used for various applications such as generating realistic portraits, landscapes, and other types of images.", "function_name": "Realistic_Vision_V1_4", "function_arguments": {"prompt": {"type": "str", "description": "A textual prompt that specifies the desired image", "default_value": ""}, "negative_prompt": {"type": "str", "description": "The textual prompt for generating a negative image", "default_value": ""}}, "function_code": "def Realistic_Vision_V1_4(prompt, negative_prompt):\n    from transformers import pipeline\n\n    model = pipeline('text-to-image', model='SG161222/Realistic_Vision_V1.4')\n\n    result = model(prompt, negative_prompt=negative_prompt)\n\n    return result\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "stabilityai/sd-vae-ft-ema", "python_environment_requirements": ["diffusers"], "description": "This is a fine-tuned VAE decoder for the Stable Diffusion Pipeline. It has been fine-tuned on a 1:1 ratio of LAION-Aesthetics and LAION-Humans datasets. The decoder can be used as a drop-in replacement for the existing autoencoder.", "function_name": "stabilityai_sd_vae_ft_ema", "function_arguments": {}, "function_code": "def stabilityai_sd_vae_ft_ema():\n    from diffusers.models import AutoencoderKL\n    from diffusers import StableDiffusionPipeline\n    model = \"CompVis/stable-diffusion-v1-4\"\n    vae = AutoencoderKL.from_pretrained(\"stabilityai/sd-vae-ft-ema\")\n    pipe = StableDiffusionPipeline.from_pretrained(model, vae=vae)\n\n    return pipe\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "EimisAnimeDiffusion_1.0v", "python_environment_requirements": ["huggingface_hub"], "description": "EimisAnimeDiffusion_1.0v is a text-to-image model trained with high-quality and detailed anime images. It works well on anime and landscape generations and supports a Gradio Web UI.", "function_name": "EimisAnimeDiffusion_1_0v", "function_arguments": {"prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}}, "function_code": "def EimisAnimeDiffusion_1_0v(prompt):\n    from huggingface_hub import hf_hub_download\n    hf_hub_download('eimiss/EimisAnimeDiffusion_1.0v', prompt)\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Linaqruf/anything-v3.0", "python_environment_requirements": ["transformers"], "description": "A text-to-image model that generates images from text descriptions.", "function_name": "Linaqruf_anything_v3_0", "function_arguments": {"prompt": {"type": "str", "description": "A text description that serves as the prompt for generating images.", "default_value": ""}, "max_length": {"type": "int", "description": "The maximum length of the text description", "default_value": "512"}}, "function_code": "def Linaqruf_anything_v3_0(prompt, max_length=512):\n    import torch\n    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n    model_name_or_path = \"Linaqruf/anything-v3.0\"\n    # Load the model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained(model_name_or_path)\n    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n\n    # Tokenize the input text\n    inputs = tokenizer(prompt, truncation=True, padding=True, max_length=max_length, return_tensors=\"pt\")\n\n    # Set the device\n    model.to(\"cuda\")\n    inputs.to(\"cuda\")\n\n    # Make the prediction\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    # Return the logits\n    return logits\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "wavymulder/Analog-Diffusion", "python_environment_requirements": ["transformers"], "description": "Analog Diffusion is a dreambooth model trained on a diverse set of analog photographs. It can generate images based on text prompts with an analog style. Use the activation token 'analog style' in your prompt to get the desired output. The model is available on the Hugging Face Inference API and can be used with the transformers library.", "function_name": "text_to_image", "function_arguments": {"prompt": {"type": "str", "description": "The prompt text to generate an analog-style image", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the generated image", "default_value": ""}}, "function_code": "def text_to_image(prompt, output_path):\n    from transformers import CLIPProcessor, CLIPModel\n    import torch\n    import os\n    from PIL import Image\n\n    # Load the CLIP model and processor\n    model = CLIPModel.from_pretrained(\"wavymulder/Analog-Diffusion\")\n    processor = CLIPProcessor.from_pretrained(\"wavymulder/Analog-Diffusion\")\n\n    # Tokenize the text\n    inputs = processor(prompt, return_tensors=\"pt\", padding=True)\n\n    # Generate the image\n    with torch.no_grad():\n        outputs = model.generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n\n    # Convert the image tensor to PIL image\n    image = Image.fromarray(outputs[0].numpy().astype(\"uint8\"))\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "Lykon/DreamShaper", "python_environment_requirements": ["transformers, torch"], "description": "Dream Shaper is a text-to-image model that generates artistic images based on the given input text. Read more about this model here: https://civitai.com/models/4384/dreamshaper", "function_name": "Lykon_DreamShaper", "function_arguments": {"prompt": {"type": "str", "description": "The input text prompt for generating artistic images", "default_value": ""}}, "function_code": "def Lykon_DreamShaper(prompt):\n    import requests\n\n    url = \"https://api-inference.huggingface.co/models/Lykon/DreamShaper\"\n\n    headers = {\n        \"Authorization\": \"Bearer api_key\",\n        \"Content-Type\": \"application/json\",\n    }\n\n    data = {\n        \"inputs\": prompt\n    }\n\n    response = requests.post(url, headers=headers, json=data)\n    output = response.json()\n\n    return output\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "darkstorm2150/Protogen_v2.2_Official_Release", "python_environment_requirements": ["diffusers", "torch"], "description": "Protogen v2.2 is a text-to-image model that generates high-quality images based on text prompts. It was warm-started with Stable Diffusion v1-5 and fine-tuned on a large amount of data from large datasets new and trending on civitai.com. The model can be used with the Stable Diffusion Pipeline and supports trigger words like 'modelshoot style' to enforce camera capture.", "function_name": "darkstorm2150_Protogen_v2_2_Official_Release", "function_arguments": {"prompt": {"type": "str", "description": "The text prompt to generate images", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def darkstorm2150_Protogen_v2_2_Official_Release(prompt, output_path):\n    from diffusers import StableDiffusionPipeline, DPMSolverMultistepScheduler\n    import torch\n    import os\n\n    model_id = \"darkstorm2150/Protogen_v2.2_Official_Release\"\n    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16, cache_dir=\"D:\\python\\data\")\n    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config, cache_dir=\"D:\\python\\data\")\n    pipe = pipe.to(\"cuda\")\n\n    image = pipe(prompt, num_inference_steps=25).images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "gsdf/Counterfeit-V2.5", "python_environment_requirements": ["transformers"], "description": "Counterfeit-V2.5 is a text-to-image model that generates anime-style images based on text prompts. It has been updated for ease of use and can be used with negative prompts to create high-quality images.", "function_name": "gsdf_Counterfeit_V2_5", "function_arguments": {"prompt": {"type": "str", "description": "The text prompt for generating anime-style images", "default_value": ""}}, "function_code": "def gsdf_Counterfeit_V2_5(prompt):\n    from transformers import pipeline\n\n    # Extracting arguments\n    model = 'gsdf/Counterfeit-V2.5'\n    tokenizer = 'gsdf/Counterfeit-V2.5'\n    device = 'cuda'\n\n    # Initializing the pipeline\n    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=device)\n\n    # Generating the text\n    text = generator(prompt, max_length=100)[0]['generated_text']\n\n    return text\n", "executable": false}
{"domain": "Multimodal Text-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image", "api_name": "vintedois-diffusion-v0-1", "python_environment_requirements": ["transformers"], "description": "Vintedois (22h) Diffusion model trained by Predogl and piEsposito with open weights, configs and prompts. This model generates beautiful images without a lot of prompt engineering. It can also generate high fidelity faces with a little amount of steps.", "function_name": "vintedois_diffusion_v0_1", "function_arguments": {"prompt": {"type": "str", "description": "The prompt used as input for the model.", "default_value": ""}, "output_path": {"type": "str", "description": "The output path where the generated images will be saved.", "default_value": ""}}, "function_code": "def vintedois_diffusion_v0_1(prompt, output_path):\n    import torch\n    import os\n    from diffusers import StableDiffusionPipeline\n\n    model_id = \"22h/vintedois-diffusion-v0-1\"\n    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n    pipe = pipe.to(\"cuda\")\n\n    image = pipe(prompt).images[0]\n\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "kha-white/manga-ocr-base", "python_environment_requirements": ["transformers"], "description": "Optical character recognition for Japanese text, with the main focus being Japanese manga. It uses Vision Encoder Decoder framework. Manga OCR can be used as a general purpose printed Japanese OCR, but its main goal was to provide a high quality text recognition, robust against various scenarios specific to manga: both vertical and horizontal text, text with furigana, text overlaid on images, wide variety of fonts and font styles, and low quality images.", "function_name": "kha_white_manga_ocr_base", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that contains the Japanese manga text to be recognized.", "default_value": ""}}, "function_code": "def kha_white_manga_ocr_base(image_path):\n    import torch\n    from PIL import Image\n    from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel\n\n    model = VisionEncoderDecoderModel.from_pretrained(\"kha-white/manga-ocr-base\")\n    processor = GPT2TokenizerFast.from_pretrained(\"kha-white/manga-ocr-base\")\n    image_processor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n\n    image = Image.open(image_path)\n    inputs = image_processor(image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Get the predicted tokens and labels\n    predicted_tokens = processor.tokenizer.convert_ids_to_tokens(outputs.logits.argmax(dim=2)[0])\n    predicted_labels = processor.tokenizer.convert_ids_to_tokens(outputs.logits.argmax(dim=2)[0])\n\n    return predicted_tokens, predicted_labels\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "blip-image-captioning-base", "python_environment_requirements": ["requests", "PIL", "transformers"], "description": "BLIP (Bootstrapping Language-Image Pre-training) is a new vision-language pre-training (VLP) framework that transfers flexibly to both vision-language understanding and generation tasks. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. This model is pre-trained on the COCO dataset with a base architecture (ViT base backbone).", "function_name": "blip_image_captioning_base", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "prompt": {"type": "str", "description": "The prompt or input text for generating image captions.", "default_value": ""}}, "function_code": "def blip_image_captioning_base(image_path, prompt):\n    from PIL import Image\n    from transformers import BlipProcessor, BlipForConditionalGeneration\n\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n\n    raw_image = Image.open(image_path)\n    inputs = processor(raw_image, prompt, return_tensors=\"pt\")\n    out = model.generate(**inputs)\n    caption = processor.decode(out[0], skip_special_tokens=True)\n\n    return caption\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Transformers", "functionality": "Image Captioning", "api_name": "blip-image-captioning-large", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "BLIP is a Vision-Language Pre-training (VLP) framework that achieves state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval, image captioning, and VQA. It effectively utilizes noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones.", "function_name": "blip_image_captioning_large", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be captioned.", "default_value": ""}, "prompt": {"type": "str", "description": "The prompt for generating image captions", "default_value": ""}}, "function_code": "def blip_image_captioning_large(image_path, prompt):\n    from PIL import Image\n    from transformers import BlipProcessor, BlipForConditionalGeneration\n\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-large\")\n\n    raw_image = Image.open(image_path)\n    inputs = processor(raw_image, prompt, return_tensors=\"pt\")\n    out = model.generate(**inputs)\n\n    return processor.decode(out[0], skip_special_tokens=True)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-base-printed", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.", "function_name": "microsoft_trocr_base_printed", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be recognized", "default_value": ""}}, "function_code": "def microsoft_trocr_base_printed(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n\n    image = Image.open(image_path)\n    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-base-printed\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-base-printed\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-opt-2.7b", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "BLIP-2 model, leveraging OPT-2.7b (a large language model with 2.7 billion parameters). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, given the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.", "function_name": "blip2_opt_2_7b", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that contains the image to be used as a prompt for the BLIP-2 model.", "default_value": ""}, "prompt": {"type": "str", "description": "The query embeddings and the previous text prompt.", "default_value": ""}}, "function_code": "def blip2_opt_2_7b(image_path, prompt):\n    from PIL import Image\n    import torch\n    from transformers import Blip2Processor, Blip2ForConditionalGeneration\n    processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\n        \"Salesforce/blip2-opt-2.7b\", device_map={\"\": 0}, torch_dtype=torch.float16)\n\n    image = Image.open(image_path)\n\n    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n\n    generated_ids = model.generate(**inputs)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-handwritten", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository.", "function_name": "microsoft_trocr_small_handwritten", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed", "default_value": ""}}, "function_code": "def microsoft_trocr_small_handwritten(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path).convert('RGB')\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-handwritten')\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-handwritten')\n    pixel_values = processor(images=image, return_tensors='pt').pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "naver-clova-ix/donut-base", "python_environment_requirements": ["transformers"], "description": "Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder.", "function_name": "naver_clova_ix_donut_base", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "prompt": {"type": "str", "description": "The image prompt provided to the function", "default_value": ""}}, "function_code": "def naver_clova_ix_donut_base(image_path, prompt):\n    import torch\n    import re\n    from PIL import Image\n    from transformers import DonutProcessor, VisionEncoderDecoderModel\n    # Load the model and tokenizer\n    processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-docvqa\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n    image = Image.open(image_path)\n\n    # prepare decoder inputs\n    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n    question = \"When is the coffee break?\"\n    prompt = task_prompt.replace(\"{user_input}\", prompt)\n    decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n    outputs = model.generate(\n        pixel_values.to(device),\n        decoder_input_ids=decoder_input_ids.to(device),\n        max_length=model.decoder.config.max_position_embeddings,\n        pad_token_id=processor.tokenizer.pad_token_id,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        use_cache=True,\n        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n        return_dict_in_generate=True,\n    )\n\n    sequence = processor.batch_decode(outputs.sequences)[0]\n    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n\n    return processor.token2json(sequence)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/git-base-coco", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, base-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is a Transformer decoder conditioned on both CLIP image tokens and text tokens. It can be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).", "function_name": "microsoft_git_base_coco", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image that will be used for generating the text output", "default_value": ""}}, "function_code": "def microsoft_git_base_coco(image_path):\n    from transformers import AutoProcessor, AutoModelForCausalLM\n    from PIL import Image\n\n    processor = AutoProcessor.from_pretrained(\"microsoft/git-base-coco\")\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-coco\")\n\n    image = Image.open(image_path)\n\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_caption\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-large-handwritten", "python_environment_requirements": ["packages"], "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.", "function_name": "microsoft_trocr_large_handwritten", "function_arguments": {"image_path": {"type": "str", "description": "The path to the handwritten image to be processed", "default_value": ""}}, "function_code": "def microsoft_trocr_large_handwritten(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n\n    image = Image.open(image_path).convert(\"RGB\")\n    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-handwritten\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-handwritten\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image-to-Text", "api_name": "ydshieh/vit-gpt2-coco-en", "python_environment_requirements": ["torch", "requests", "PIL", "transformers"], "description": "A proof-of-concept model for the Hugging Face FlaxVisionEncoderDecoder Framework that produces reasonable image captioning results.", "function_name": "ydshieh_vit_gpt2_coco_en", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed", "default_value": ""}}, "function_code": "def ydshieh_vit_gpt2_coco_en(image_path):\n    import torch\n    from PIL import Image\n    from transformers import ViTFeatureExtractor, AutoTokenizer, VisionEncoderDecoderModel\n\n    loc = \"ydshieh/vit-gpt2-coco-en\"\n    feature_extractor = ViTFeatureExtractor.from_pretrained(loc)\n    tokenizer = AutoTokenizer.from_pretrained(loc)\n    model = VisionEncoderDecoderModel.from_pretrained(loc)\n    model.eval()\n\n    def predict(image):\n        pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values\n        with torch.no_grad():\n            output_ids = model.generate(pixel_values, max_length=16, num_beams=4,\n                                        return_dict_in_generate=True).sequences\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n        return preds\n\n    with Image.open(image_path) as image:\n        preds = predict(image)\n\n    return preds\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-base-handwritten", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "TrOCR model fine-tuned on the IAM dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.", "function_name": "microsoft_trocr_base_handwritten", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed.", "default_value": ""}}, "function_code": "def microsoft_trocr_base_handwritten(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n\n    image = Image.open(image_path).convert('RGB')\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-base-handwritten')\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-base-handwritten')\n    pixel_values = processor(images=image, return_tensors='pt').pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "donut-base-finetuned-cord-v2", "python_environment_requirements": ["transformers"], "description": "Donut consists of a vision encoder (Swin Transformer) and a text decoder (BART). Given an image, the encoder first encodes the image into a tensor of embeddings (of shape batch_size, seq_len, hidden_size), after which the decoder autoregressively generates text, conditioned on the encoding of the encoder. This model is fine-tuned on CORD, a document parsing dataset.", "function_name": "donut_base_finetuned_cord_v2", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed", "default_value": ""}}, "function_code": "def donut_base_finetuned_cord_v2(image_path):\n    import torch\n    import re\n    from PIL import Image\n    from transformers import DonutProcessor, VisionEncoderDecoderModel\n    # Load the model and tokenizer\n    processor = DonutProcessor.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"naver-clova-ix/donut-base-finetuned-cord-v2\")\n\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model.to(device)\n    image = Image.open(image_path)\n\n    # prepare decoder inputs\n    task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n    question = \"When is the coffee break?\"\n    prompt = task_prompt.replace(\"{user_input}\", prompt)\n    decoder_input_ids = processor.tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n\n    pixel_values = processor(image, return_tensors=\"pt\").pixel_values\n\n    outputs = model.generate(\n        pixel_values.to(device),\n        decoder_input_ids=decoder_input_ids.to(device),\n        max_length=model.decoder.config.max_position_embeddings,\n        pad_token_id=processor.tokenizer.pad_token_id,\n        eos_token_id=processor.tokenizer.eos_token_id,\n        use_cache=True,\n        bad_words_ids=[[processor.tokenizer.unk_token_id]],\n        return_dict_in_generate=True,\n    )\n\n    sequence = processor.batch_decode(outputs.sequences)[0]\n    sequence = sequence.replace(processor.tokenizer.eos_token, \"\").replace(processor.tokenizer.pad_token, \"\")\n    sequence = re.sub(r\"<.*?>\", \"\", sequence, count=1).strip()  # remove first task start token\n\n    return processor.token2json(sequence)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-coco", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on COCO. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository.", "function_name": "git_large_coco", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image that will be processed by the GIT model", "default_value": ""}, "question": {"type": "str", "description": "The question input to the Generative Image-to-text Transformer model", "default_value": ""}}, "function_code": "def git_large_coco(image_path, question):\n    from transformers import AutoProcessor, AutoModelForCausalLM\n    from PIL import Image\n    import torch\n    processor = AutoProcessor.from_pretrained(\"microsoft/git-base-textvqa\")\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base-textvqa\")\n\n    image = Image.open(image_path).convert(\"RGB\")\n\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n    input_ids = processor(text=question, add_special_tokens=False).input_ids\n    input_ids = [processor.tokenizer.cls_token_id] + input_ids\n    input_ids = torch.tensor(input_ids).unsqueeze(0)\n\n    generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n\n    return processor.batch_decode(generated_ids, skip_special_tokens=True)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/pix2struct-base", "python_environment_requirements": ["transformers", "torch"], "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. The model is pretrained by learning to parse masked screenshots of web pages into simplified HTML. It can achieve state-of-the-art results in six out of nine tasks across four domains: documents, illustrations, user interfaces, and natural images.", "function_name": "google_pix2struct_base", "function_arguments": {"PATH_TO_SAVE": {"type": "str", "description": "The path to save the model weights and other necessary files.", "default_value": ""}, "USERNAME": {"type": "str", "description": "The username to be used as input for the Pix2Struct image encoder - text decoder model.", "default_value": ""}, "MODEL_NAME": {"type": "str", "description": "The name of the Google Pix2Struct model to be used", "default_value": ""}}, "function_code": "def google_pix2struct_base(PATH_TO_SAVE, USERNAME, MODEL_NAME):\n    from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\n    model = Pix2StructForConditionalGeneration.from_pretrained(PATH_TO_SAVE)\n    processor = Pix2StructProcessor.from_pretrained(PATH_TO_SAVE)\n\n    model.push_to_hub(f\"{USERNAME}/{MODEL_NAME}\")\n    processor.push_to_hub(f\"{USERNAME}/{MODEL_NAME}\")\n", "executable": false}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/pix2struct-textcaps-base", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "Pix2Struct is an image encoder - text decoder model that is trained on image-text pairs for various tasks, including image captioning and visual question answering. It is pretrained by learning to parse masked screenshots of web pages into simplified HTML. The web, with its richness of visual elements cleanly reflected in the HTML structure, provides a large source of pretraining data well suited to the diversity of downstream tasks.", "function_name": "google_pix2struct_textcaps_base", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be encoded", "default_value": ""}}, "function_code": "def google_pix2struct_textcaps_base(image_path):\n    from PIL import Image\n    from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n\n    image = Image.open(image_path)\n    model = Pix2StructForConditionalGeneration.from_pretrained(\"google/pix2struct-textcaps-base\")\n    processor = Pix2StructProcessor.from_pretrained(\"google/pix2struct-textcaps-base\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    predictions = model.generate(**inputs)\n\n    return processor.decode(predictions[0], skip_special_tokens=True)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "microsoft/git-base", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).", "function_name": "git_base", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def git_base(image_path):\n    from transformers import AutoProcessor, AutoModelForCausalLM\n    from PIL import Image\n\n    processor = AutoProcessor.from_pretrained(\"microsoft/git-base\")\n    model = AutoModelForCausalLM.from_pretrained(\"microsoft/git-base\")\n\n    image = Image.open(image_path)\n\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_caption\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-large-printed", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of BEiT, while the text decoder was initialized from the weights of RoBERTa.", "function_name": "microsoft_trocr_large_printed", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the input image for OCR", "default_value": ""}}, "function_code": "def microsoft_trocr_large_printed(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n\n    image = Image.open(image_path).convert(\"RGB\")\n    processor = TrOCRProcessor.from_pretrained(\"microsoft/trocr-large-printed\")\n    model = VisionEncoderDecoderModel.from_pretrained(\"microsoft/trocr-large-printed\")\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-textcaps", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).", "function_name": "git_large_textcaps", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed.", "default_value": ""}}, "function_code": "def git_large_textcaps(image_path):\n    from transformers import AutoProcessor, AutoModelForCausalLM\n    import requests\n    from PIL import Image\n    model_name = \"microsoft/git-large-textcaps\"\n    processor = AutoProcessor.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    image = Image.open(image_path)\n\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_caption\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "git-large-r-textcaps", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, large-sized version, fine-tuned on TextCaps. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).", "function_name": "git_large_r_textcaps", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that will be processed by the GIT model", "default_value": ""}}, "function_code": "def git_large_r_textcaps(image_path):\n    from transformers import AutoProcessor, AutoModelForCausalLM\n    import requests\n    from PIL import Image\n    model_name = \"microsoft/git-large-r-textcaps\"\n    processor = AutoProcessor.from_pretrained(model_name)\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n\n    image = Image.open(image_path)\n\n    pixel_values = processor(images=image, return_tensors=\"pt\").pixel_values\n\n    generated_ids = model.generate(pixel_values=pixel_values, max_length=50)\n    generated_caption = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n    print(generated_caption)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-stage1", "python_environment_requirements": ["transformers", "PIL", "requests", "torch"], "description": "TrOCR pre-trained only model. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder. Next, the Transformer text decoder autoregressively generates tokens.", "function_name": "microsoft_trocr_small_stage1", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed by the TrOCR model.", "default_value": ""}}, "function_code": "def microsoft_trocr_small_stage1(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n    import torch\n\n    image = Image.open(image_path).convert('RGB')\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-stage1')\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-stage1')\n\n    pixel_values = processor(image, return_tensors='pt').pixel_values\n    decoder_input_ids = torch.tensor([[model.config.decoder.decoder_start_token_id]])\n    outputs = model(pixel_values=pixel_values, decoder_input_ids=decoder_input_ids)\n\n    return outputs\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/trocr-small-printed", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "TrOCR model fine-tuned on the SROIE dataset. It was introduced in the paper TrOCR: Transformer-based Optical Character Recognition with Pre-trained Models by Li et al. and first released in this repository. The TrOCR model is an encoder-decoder model, consisting of an image Transformer as encoder, and a text Transformer as decoder. The image encoder was initialized from the weights of DeiT, while the text decoder was initialized from the weights of UniLM.", "function_name": "microsoft_trocr_small_printed", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}}, "function_code": "def microsoft_trocr_small_printed(image_path):\n    from transformers import TrOCRProcessor, VisionEncoderDecoderModel\n    from PIL import Image\n\n    image = Image.open(image_path).convert('RGB')\n    processor = TrOCRProcessor.from_pretrained('microsoft/trocr-small-printed')\n    model = VisionEncoderDecoderModel.from_pretrained('microsoft/trocr-small-printed')\n    pixel_values = processor(images=image, return_tensors='pt').pixel_values\n    generated_ids = model.generate(pixel_values)\n    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n\n    return generated_text\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "dpt-large-redesign", "python_environment_requirements": ["torch", "transformers"], "description": "A depth estimation model based on the DPT architecture.", "function_name": "dpt_large_redesign", "function_arguments": {"prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}}, "function_code": "def dpt_large_redesign(prompt):\n    import torch\n    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n    # Load the model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained('nielsr/dpt-large-redesign')\n    tokenizer = AutoTokenizer.from_pretrained('nielsr/dpt-large-redesign')\n\n    # Tokenize the input text\n    inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors='pt')\n    inputs = {k: v.to('cpu') for k, v in inputs.items()}\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    # Get the predicted label\n    predicted_label = torch.argmax(outputs.logits).item()\n\n    return predicted_label\n", "executable": false}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-kitti", "python_environment_requirements": ["transformers"], "description": "Global-Local Path Networks (GLPN) model trained on KITTI for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.", "function_name": "glpn_kitti", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the output of the GLPN model", "default_value": ""}}, "function_code": "def glpn_kitti(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"vinvino02/glpn-kitti\")\n    model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-kitti\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Monocular Depth Estimation", "api_name": "Intel/dpt-large", "python_environment_requirements": ["transformers"], "description": "Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021). DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation.", "function_name": "Intel_dpt_large", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be used as input for monocular depth estimation", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the output of the DPT model.", "default_value": ""}}, "function_code": "def Intel_dpt_large(image_path, output_path):\n    from transformers import DPTImageProcessor, DPTForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large\")\n    model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-large\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n\n    depth.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu", "python_environment_requirements": ["transformers", "torch", "numpy", "PIL", "requests"], "description": "Global-Local Path Networks (GLPN) model trained on NYUv2 for monocular depth estimation. It was introduced in the paper Global-Local Path Networks for Monocular Depth Estimation with Vertical CutDepth by Kim et al. and first released in this repository.", "function_name": "glpn_nyu", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed by the GLPN model", "default_value": ""}, "output_path": {"type": "str", "description": "The output path where the depth estimation results will be saved.", "default_value": ""}}, "function_code": "def glpn_nyu(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path, output_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"vinvino02/glpn-nyu\")\n    model = GLPNForDepthEstimation.from_pretrained(\"vinvino02/glpn-nyu\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=\"bicubic\",\n                                                 align_corners=False)\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-nyu-finetuned-diode", "python_environment_requirements": ["transformers"], "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.", "function_name": "glpn_nyu_finetuned_diode", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the output files", "default_value": ""}}, "function_code": "def glpn_nyu_finetuned_diode(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode\")\n    model = GLPNForDepthEstimation.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(predicted_depth.unsqueeze(1), size=image.size[::-1], mode=\"bicubic\",\n                                                 align_corners=False)\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "Intel/dpt-hybrid-midas", "python_environment_requirements": ["torch", "transformers", "PIL", "numpy", "requests"], "description": "Dense Prediction Transformer (DPT) model trained on 1.4 million images for monocular depth estimation. Introduced in the paper Vision Transformers for Dense Prediction by Ranftl et al. (2021) and first released in this repository. DPT uses the Vision Transformer (ViT) as backbone and adds a neck + head on top for monocular depth estimation. This repository hosts the hybrid version of the model as stated in the paper. DPT-Hybrid diverges from DPT by using ViT-hybrid as a backbone and taking some activations from the backbone.", "function_name": "intel_dpt_hybrid_midas", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image that the Dense Prediction Transformer (DPT) model will be applied to for monocular depth estimation.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the directory where the output will be saved", "default_value": ""}}, "function_code": "def intel_dpt_hybrid_midas(image_path, output_path):\n    from PIL import Image\n    import numpy as np\n    import os\n    import torch\n    from transformers import DPTForDepthEstimation, DPTFeatureExtractor\n\n    model = DPTForDepthEstimation.from_pretrained(\"Intel/dpt-hybrid-midas\", low_cpu_mem_usage=True)\n    feature_extractor = DPTFeatureExtractor.from_pretrained(\"Intel/dpt-hybrid-midas\")\n\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-030603", "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1"], "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.", "function_name": "glpn_nyu_finetuned_diode_221122_030603", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "output_path": {"type": "str", "description": "The path where the output will be saved.", "default_value": ""}}, "function_code": "def glpn_nyu_finetuned_diode_221122_030603(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode-221122-030603\")\n    model = GLPNForDepthEstimation.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode-221122-030603\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Depth Estimation", "api_name": "glpn-kitti-finetuned-diode", "python_environment_requirements": ["transformers==4.24.0, torch==1.12.1+cu113, tokenizers==0.13.2"], "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset.", "function_name": "glpn_kitti_finetuned_diode", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "output_path": {"type": "str", "description": "The file path to output the results", "default_value": ""}}, "function_code": "def glpn_kitti_finetuned_diode(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"sayakpaul/glpn-kitti-finetuned-diode\")\n    model = GLPNForDepthEstimation.from_pretrained(\"sayakpaul/glpn-kitti-finetuned-diode\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-nyu-finetuned-diode-221122-044810", "python_environment_requirements": ["transformers==4.24.0, torch==1.12.1, tokenizers==0.13.2"], "description": "This model is a fine-tuned version of vinvino02/glpn-nyu on the diode-subset dataset.", "function_name": "glpn_nyu_finetuned_diode_221122_044810", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "output_path": {"type": "str", "description": "The file path where the output will be saved", "default_value": ""}}, "function_code": "def glpn_nyu_finetuned_diode_221122_044810(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode-221122-044810\")\n    model = GLPNForDepthEstimation.from_pretrained(\"sayakpaul/glpn-nyu-finetuned-diode-221122-044810\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Depth Estimation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "glpn-kitti-finetuned-diode-221214-123047", "python_environment_requirements": ["transformers==4.24.0", "torch==1.12.1+cu116", "tokenizers==0.13.2"], "description": "This model is a fine-tuned version of vinvino02/glpn-kitti on the diode-subset dataset. It is used for depth estimation in computer vision applications.", "function_name": "glpn_kitti_finetuned_diode_221214_123047", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the results will be saved", "default_value": ""}}, "function_code": "def glpn_kitti_finetuned_diode_221214_123047(image_path, output_path):\n    from transformers import GLPNFeatureExtractor, GLPNForDepthEstimation\n    import torch\n    import os\n    import numpy as np\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = GLPNFeatureExtractor.from_pretrained(\"sayakpaul/glpn-kitti-finetuned-diode-221214-123047\")\n    model = GLPNForDepthEstimation.from_pretrained(\"sayakpaul/glpn-kitti-finetuned-diode-221214-123047\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    predicted_depth = outputs.predicted_depth\n    prediction = torch.nn.functional.interpolate(\n        predicted_depth.unsqueeze(1),\n        size=image.size[::-1],\n        mode=\"bicubic\",\n        align_corners=False,\n    )\n\n    output = prediction.squeeze().cpu().numpy()\n    formatted = (output * 255 / np.max(output)).astype(np.uint8)\n    depth = Image.fromarray(formatted)\n    depth.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/resnet-50", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "ResNet-50 v1.5 is a pre-trained convolutional neural network for image classification on the ImageNet-1k dataset at resolution 224x224. It was introduced in the paper Deep Residual Learning for Image Recognition by He et al. ResNet (Residual Network) democratized the concepts of residual learning and skip connections, enabling the training of much deeper models. ResNet-50 v1.5 differs from the original model in the bottleneck blocks which require downsampling, v1 has stride = 2 in the first 1x1 convolution, whereas v1.5 has stride = 2 in the 3x3 convolution. This difference makes ResNet50 v1.5 slightly more accurate but comes with a small performance drawback.", "function_name": "microsoft_resnet_50", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for which the ResNet-50 v1.5 model should perform classification.", "default_value": ""}}, "function_code": "def microsoft_resnet_50(image_path):\n    from transformers import AutoImageProcessor, ResNetForImageClassification\n    import torch\n    from PIL import Image\n\n    image = Image.open(image_path)\n\n    processor = AutoImageProcessor.from_pretrained('microsoft/resnet-50')\n    model = ResNetForImageClassification.from_pretrained('microsoft/resnet-50')\n\n    inputs = processor(image, return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-large-224", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration.", "function_name": "facebook_convnext_large_224", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def facebook_convnext_large_224(image_path):\n    from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\n    import torch\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-large-224')\n    model = ConvNextForImageClassification.from_pretrained('facebook/convnext-large-224')\n\n    inputs = feature_extractor(image, return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/beit-base-patch16-224-pt22k-ft22k", "python_environment_requirements": ["transformers"], "description": "BEiT model pre-trained in a self-supervised fashion on ImageNet-22k - also called ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on the same dataset at resolution 224x224. It was introduced in the paper BEIT: BERT Pre-Training of Image Transformers by Hangbo Bao, Li Dong and Furu Wei and first released in this repository.", "function_name": "microsoft_beit_base_patch16_224_pt22k_ft22k", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed.", "default_value": ""}}, "function_code": "def microsoft_beit_base_patch16_224_pt22k_ft22k(image_path):\n    from transformers import BeitImageProcessor, BeitForImageClassification\n    from PIL import Image\n\n    image = Image.open(image_path)\n    processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n    model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224-pt22k-ft22k')\n    inputs = processor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/vit-base-patch16-224", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al.", "function_name": "google_vit_base_patch16_224", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}}, "function_code": "def google_vit_base_patch16_224(image_path):\n    from transformers import ViTImageProcessor, ViTForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n    inputs = processor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "martinezomg/vit-base-patch16-224-diabetic-retinopathy", "python_environment_requirements": ["transformers", "pytorch", "datasets", "tokenizers"], "description": "This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.", "function_name": "martinezomg_vit_base_patch16_224_diabetic_retinopathy", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be classified for diabetic retinopathy detection.", "default_value": ""}}, "function_code": "def martinezomg_vit_base_patch16_224_diabetic_retinopathy(image_path):\n    from transformers import pipeline\n\n    image_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\n    result = image_classifier(image_path)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Age Classification", "api_name": "nateraw/vit-age-classifier", "python_environment_requirements": ["requests", "PIL", "transformers"], "description": "A vision transformer finetuned to classify the age of a given person's face.", "function_name": "nateraw_vit_age_classifier", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be classified", "default_value": ""}}, "function_code": "def nateraw_vit_age_classifier(image_path):\n    from PIL import Image\n    from transformers import ViTFeatureExtractor, ViTForImageClassification\n\n    im = Image.open(image_path)\n\n    model = ViTForImageClassification.from_pretrained('nateraw/vit-age-classifier')\n    transforms = ViTFeatureExtractor.from_pretrained('nateraw/vit-age-classifier')\n\n    inputs = transforms(im, return_tensors='pt')\n    output = model(**inputs)\n\n    proba = output.logits.softmax(1)\n    preds = proba.argmax(1)\n\n    return preds\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/vit-base-patch16-384", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "Vision Transformer (ViT) model pre-trained on ImageNet-21k (14 million images, 21,843 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 384x384. It was introduced in the paper An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale by Dosovitskiy et al. Images are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds absolute position embeddings before feeding the sequence to the layers of the Transformer encoder.", "function_name": "google_vit_base_patch16_384", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def google_vit_base_patch16_384(image_path):\n    from transformers import ViTFeatureExtractor, ViTForImageClassification\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = ViTFeatureExtractor.from_pretrained('google/vit-base-patch16-384')\n    model = ViTForImageClassification.from_pretrained('google/vit-base-patch16-384')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/beit-base-patch16-224", "python_environment_requirements": ["transformers"], "description": "BEiT model pre-trained in a self-supervised fashion on ImageNet-21k (14 million images, 21,841 classes) at resolution 224x224, and fine-tuned on ImageNet 2012 (1 million images, 1,000 classes) at resolution 224x224.", "function_name": "microsoft_beit_base_patch16_224", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def microsoft_beit_base_patch16_224(image_path):\n    from transformers import BeitImageProcessor, BeitForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = BeitImageProcessor.from_pretrained('microsoft/beit-base-patch16-224')\n    model = BeitForImageClassification.from_pretrained('microsoft/beit-base-patch16-224')\n    inputs = processor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    predicted_class = model.config.id2label[predicted_class_idx]\n\n    return predicted_class\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "lysandre/tiny-vit-random", "python_environment_requirements": ["transformers"], "description": "A tiny-vit-random model for image classification using Hugging Face Transformers.", "function_name": "lysandre_tiny_vit_random", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be classified.", "default_value": ""}}, "function_code": "def lysandre_tiny_vit_random(image_path):\n    import torch\n    from PIL import Image\n    from torchvision.transforms import functional as F\n    from transformers import ViTFeatureExtractor, ViTForImageClassification\n\n    model_name_or_path = \"lysandre/tiny-vit-random\"\n    # Load the image\n    image = Image.open(image_path)\n\n    # Preprocess the image\n    image = F.to_tensor(image)\n\n    # Load the model and tokenizer\n    feature_extractor = ViTFeatureExtractor.from_pretrained(\"lysandre/tiny-vit-random\")\n    model = ViTForImageClassification.from_pretrained(model_name_or_path).to(\"cuda\")\n\n    # Tokenize the image\n    inputs = feature_extractor(images=image, return_tensors='pt')\n\n    # Make predictions\n    inputs = {k: v.to(\"cuda\") for k, v in inputs.items()}\n    outputs = model(**inputs)\n\n    # Get the predicted class label\n    predicted_class_idx = torch.argmax(outputs.logits, dim=1).item()\n\n    return predicted_class_idx\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "fxmarty/resnet-tiny-beans", "python_environment_requirements": ["transformers"], "description": "A model trained on the beans dataset, just for testing and having a really tiny model.", "function_name": "fxmarty_resnet_tiny_beans", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def fxmarty_resnet_tiny_beans(image_path):\n    from transformers import pipeline\n\n    classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\n    results = classifier(image_path)\n\n    return results\n", "executable": false}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/mobilenet_v1_0.75_192", "python_environment_requirements": ["transformers"], "description": "MobileNet V1 model pre-trained on ImageNet-1k at resolution 192x192. It was introduced in MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications by Howard et al, and first released in this repository. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.", "function_name": "google_mobilenet_v1_0_75_192", "function_arguments": {"image_path": {"type": "str", "description": "Path to the input image file.", "default_value": ""}}, "function_code": "def google_mobilenet_v1_0_75_192(image_path):\n    from transformers import AutoImageProcessor, AutoModelForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    preprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v1_0.75_192\")\n    model = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v1_0.75_192\")\n    inputs = preprocessor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "nvidia/mit-b0", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "SegFormer encoder fine-tuned on Imagenet-1k. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository. SegFormer consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on semantic segmentation benchmarks such as ADE20K and Cityscapes.", "function_name": "nvidia_mit_b0", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image", "default_value": ""}}, "function_code": "def nvidia_mit_b0(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    feature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\n    model = SegformerForImageClassification.from_pretrained('nvidia/mit-b0')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    predicted_class = model.config.id2label[predicted_class_idx]\n\n    return predicted_class\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "vit_base_patch16_224.augreg2_in21k_ft_in1k", "python_environment_requirements": ["transformers", "torch"], "description": "A Vision Transformer model for image classification, pretrained on ImageNet-21k and fine-tuned on ImageNet-1k.", "function_name": "vit_base_patch16_224_augreg2_in21k_ft_in1k", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed.", "default_value": ""}}, "function_code": "def vit_base_patch16_224_augreg2_in21k_ft_in1k(image_path):\n    import torch\n    from PIL import Image\n    from torchvision.transforms import functional as F\n    from transformers import ViTFeatureExtractor, ViTForImageClassification\n\n    model_path = \"timm/vit_base_patch16_224.augreg2_in21k_ft_in1k\"\n    # Load the pre-trained model\n    model = ViTForImageClassification.from_pretrained(model_path)\n\n    # Load and preprocess the image\n    image = Image.open(image_path)\n    image = F.resize(image, (224, 224))\n    image = F.to_tensor(image)\n    image = F.normalize(image, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n\n    # Generate features from the image\n    feature_extractor = ViTFeatureExtractor.from_pretrained(model_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    inputs.pop(\"pixel_values\")\n    inputs = {k: v.to(torch.device(\"cuda\")) for k, v in inputs.items()}\n\n    # Perform inference\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    # Get the predicted class label\n    predicted_class = torch.argmax(logits, dim=1).item()\n\n    return predicted_class\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "google/mobilenet_v2_1.0_224", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "MobileNet V2 model pre-trained on ImageNet-1k at resolution 224x224. It was introduced in MobileNetV2: Inverted Residuals and Linear Bottlenecks by Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, Liang-Chieh Chen. MobileNets are small, low-latency, low-power models parameterized to meet the resource constraints of a variety of use cases. They can be built upon for classification, detection, embeddings and segmentation similar to how other popular large scale models, such as Inception, are used. MobileNets can be run efficiently on mobile devices.", "function_name": "google_mobilenet_v2_1_0_224", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed by the MobileNet V2 model.", "default_value": ""}}, "function_code": "def google_mobilenet_v2_1_0_224(image_path):\n    from transformers import AutoImageProcessor, AutoModelForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    preprocessor = AutoImageProcessor.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n    model = AutoModelForImageClassification.from_pretrained(\"google/mobilenet_v2_1.0_224\")\n    inputs = preprocessor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/swin-tiny-patch4-window7-224", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "Swin Transformer model trained on ImageNet-1k at resolution 224x224. It was introduced in the paper Swin Transformer: Hierarchical Vision Transformer using Shifted Windows by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches (shown in gray) in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window (shown in red). It can thus serve as a general-purpose backbone for both image classification and dense recognition tasks.", "function_name": "microsoft_swin_tiny_patch4_window7_224", "function_arguments": {"image_path": {"type": "str", "description": "The path of the input image file.", "default_value": ""}}, "function_code": "def microsoft_swin_tiny_patch4_window7_224(image_path):\n    from transformers import AutoFeatureExtractor, SwinForImageClassification\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n    model = SwinForImageClassification.from_pretrained(\"microsoft/swin-tiny-patch4-window7-224\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/swinv2-tiny-patch4-window8-256", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "Swin Transformer v2 model pre-trained on ImageNet-1k at resolution 256x256. It was introduced in the paper Swin Transformer V2: Scaling Up Capacity and Resolution by Liu et al. and first released in this repository. The Swin Transformer is a type of Vision Transformer. It builds hierarchical feature maps by merging image patches in deeper layers and has linear computation complexity to input image size due to computation of self-attention only within each local window. Swin Transformer v2 adds 3 main improvements: 1) a residual-post-norm method combined with cosine attention to improve training stability; 2) a log-spaced continuous position bias method to effectively transfer models pre-trained using low-resolution images to downstream tasks with high-resolution inputs; 3) a self-supervised pre-training method, SimMIM, to reduce the needs of vast labeled images.", "function_name": "microsoft_swinv2_tiny_patch4_window8_256", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the input image.", "default_value": ""}}, "function_code": "def microsoft_swinv2_tiny_patch4_window8_256(image_path):\n    from transformers import AutoImageProcessor, AutoModelForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = AutoImageProcessor.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n    model = AutoModelForImageClassification.from_pretrained(\"microsoft/swinv2-tiny-patch4-window8-256\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "saltacc/anime-ai-detect", "python_environment_requirements": ["transformers"], "description": "A BEiT classifier to see if anime art was made by an AI or a human.", "function_name": "anime_ai_detect", "function_arguments": {"prompt": {"type": "str", "description": "The prompt to be used for AI vs. human classification", "default_value": ""}}, "function_code": "def anime_ai_detect(prompt):\n    from transformers import pipeline\n    model_name = \"saltacc/anime-ai-detect\"\n    classifier = pipeline(\"text-classification\", model=model_name)\n    result = classifier(prompt)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "swin-tiny-patch4-window7-224-bottom_cleaned_data", "python_environment_requirements": ["Transformers 4.28.1", "Pytorch 2.0.0+cu118", "Datasets 2.11.0", "Tokenizers 0.13.3"], "description": "This model is a fine-tuned version of microsoft/swin-tiny-patch4-window7-224 on the imagefolder dataset.", "function_name": "swin_tiny_patch4_window7_224_bottom_cleaned_data", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def swin_tiny_patch4_window7_224_bottom_cleaned_data(image_path):\n    from transformers import AutoFeatureExtractor, SwinForImageClassification\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = AutoFeatureExtractor.from_pretrained(\n        \"Soulaimen/swin-tiny-patch4-window7-224-bottom_cleaned_data\")\n    model = SwinForImageClassification.from_pretrained(\"Soulaimen/swin-tiny-patch4-window7-224-bottom_cleaned_data\")\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/table-transformer-structure-recognition", "python_environment_requirements": ["transformers"], "description": "Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.", "function_name": "microsoft_table_transformer_structure_recognition", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image", "default_value": ""}}, "function_code": "def microsoft_table_transformer_structure_recognition(image_path):\n    from transformers import AutoImageProcessor, TableTransformerModel\n    from huggingface_hub import hf_hub_download\n    from PIL import Image\n\n    file_path = hf_hub_download(repo_id=\"nielsr/example-pdf\", repo_type=\"dataset\", filename=\"example_pdf.png\")\n    image = Image.open(image_path).convert(\"RGB\")\n\n    image_processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\n    model = TableTransformerModel.from_pretrained(\"microsoft/table-transformer-structure-recognition\")\n\n    # prepare image for the model\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    # forward pass\n    outputs = model(**inputs)\n\n    # the last hidden states are the final query embeddings of the Transformer decoder\n    # these are of shape (batch_size, num_queries, hidden_size)\n    last_hidden_states = outputs.last_hidden_state\n\n    return last_hidden_states\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/regnet-y-008", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "RegNet model trained on imagenet-1k. It was introduced in the paper Designing Network Design Spaces and first released in this repository.", "function_name": "facebook_regnet_y_008", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}}, "function_code": "def facebook_regnet_y_008(image_path):\n    from transformers import AutoFeatureExtractor, RegNetForImageClassification\n    import torch\n    from PIL import Image\n\n    image = Image.open(image_path)\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\n    model = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040')\n\n    inputs = feature_extractor(image, return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/table-transformer-detection", "python_environment_requirements": ["transformers"], "description": "Table Transformer (DETR) model trained on PubTables1M for detecting tables in documents. Introduced in the paper PubTables-1M: Towards Comprehensive Table Extraction From Unstructured Documents by Smock et al.", "function_name": "microsoft_table_transformer_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image for table detection.", "default_value": ""}}, "function_code": "def microsoft_table_transformer_detection(image_path):\n    from transformers import AutoImageProcessor, TableTransformerModel\n    from PIL import Image\n    image = Image.open(image_path)\n    image_processor = AutoImageProcessor.from_pretrained(\"microsoft/table-transformer-detection\")\n    model = TableTransformerModel.from_pretrained(\"microsoft/table-transformer-detection\")\n\n    # prepare image for the model\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    # forward pass\n    outputs = model(**inputs)\n\n    # the last hidden states are the final query embeddings of the Transformer decoder\n    # these are of shape (batch_size, num_queries, hidden_size)\n    last_hidden_states = outputs.last_hidden_state\n\n    return last_hidden_states\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-50", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.", "function_name": "facebook_detr_resnet_50", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed by the Facebook DETR ResNet-50 model.", "default_value": ""}}, "function_code": "def facebook_detr_resnet_50(image_path):\n    from transformers import DetrImageProcessor, DetrForObjectDetection\n    import torch\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n    return outputs\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "hustvl/yolos-tiny", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN). The model is trained using a bipartite matching loss: one compares the predicted classes + bounding boxes of each of the N = 100 object queries to the ground truth annotations, padded up to the same length N (so if an image only contains 4 objects, 96 annotations will just have a no object as class and no bounding box as bounding box). The Hungarian matching algorithm is used to create an optimal one-to-one mapping between each of the N queries and each of the N annotations. Next, standard cross-entropy (for the classes) and a linear combination of the L1 and generalized IoU loss (for the bounding boxes) are used to optimize the parameters of the model.", "function_name": "hustvl_yolos_tiny", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be processed by the YOLOS model.", "default_value": ""}}, "function_code": "def hustvl_yolos_tiny(image_path):\n    from transformers import YolosFeatureExtractor, YolosForObjectDetection\n    from PIL import Image\n\n    image = Image.open(image_path)\n    feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-tiny')\n    model = YolosForObjectDetection.from_pretrained('hustvl/yolos-tiny')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    bboxes = outputs.pred_boxes\n\n    return logits, bboxes\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-101", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.", "function_name": "facebook_detr_resnet_101", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be processed by the DETR model", "default_value": ""}}, "function_code": "def facebook_detr_resnet_101(image_path):\n    from transformers import DetrImageProcessor, DetrForObjectDetection\n    from PIL import Image\n\n    image = Image.open(image_path)\n    processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-101\")\n    model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-101\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n    return outputs\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-base-patch32", "python_environment_requirements": ["transformers"], "description": "OWL-ViT is a zero-shot text-conditioned object detection model that uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. The model can be used to query an image with one or multiple text queries.", "function_name": "google_owlvit_base_patch32", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image that needs to be queried.", "default_value": ""}, "prompt": {"type": "str", "description": "The text query or prompt to query an image", "default_value": ""}}, "function_code": "def google_owlvit_base_patch32(image_path, prompt):\n    from PIL import Image\n    import torch\n    from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8m-table-extraction", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "A YOLOv8 model for table extraction in images, capable of detecting both bordered and borderless tables. Trained using the keremberke/table-extraction dataset.", "function_name": "keremberke_yolov8m_table_extraction", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed", "default_value": ""}, "output_path": {"type": "str", "description": "The file path to save the extracted tables.", "default_value": ""}}, "function_code": "def keremberke_yolov8m_table_extraction(image_path, output_path):\n    from ultralyticsplus import YOLO, render_result\n    import os\n\n    model = YOLO('keremberke/yolov8m-table-extraction')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Detect Bordered and Borderless tables in documents", "api_name": "TahaDouaji/detr-doc-table-detection", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "detr-doc-table-detection is a model trained to detect both Bordered and Borderless tables in documents, based on facebook/detr-resnet-50.", "function_name": "detr_doc_table_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL of the image to be processed", "default_value": ""}}, "function_code": "def detr_doc_table_detection(image_path):\n    from transformers import DetrImageProcessor, DetrForObjectDetection\n    import torch\n    from PIL import Image\n\n    image = Image.open(image_path)\n    processor = DetrImageProcessor.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")\n    model = DetrForObjectDetection.from_pretrained(\"TahaDouaji/detr-doc-table-detection\")\n    inputs = processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    target_sizes = torch.tensor([image.size[::-1]])\n    results = processor.post_process_object_detection(outputs, target_sizes=target_sizes, threshold=0.9)[0]\n\n    for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n        box = [round(i, 2) for i in box.tolist()]\n        print(\n            f\"Detected {model.config.id2label[label.item()]} with confidence {round(score.item(), 3)} at location {box}\")\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "hustvl/yolos-small", "python_environment_requirements": ["packages"], "description": "YOLOS model fine-tuned on COCO 2017 object detection (118k annotated images). It was introduced in the paper You Only Look at One Sequence: Rethinking Transformer in Vision through Object Detection by Fang et al. and first released in this repository. YOLOS is a Vision Transformer (ViT) trained using the DETR loss. Despite its simplicity, a base-sized YOLOS model is able to achieve 42 AP on COCO validation 2017 (similar to DETR and more complex frameworks such as Faster R-CNN).", "function_name": "hustvl_yolos_small", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image to be processed", "default_value": ""}}, "function_code": "def hustvl_yolos_small(image_path):\n    from transformers import YolosFeatureExtractor, YolosForObjectDetection\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    feature_extractor = YolosFeatureExtractor.from_pretrained('hustvl/yolos-small')\n    model = YolosForObjectDetection.from_pretrained('hustvl/yolos-small')\n\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    logits = outputs.logits\n    bboxes = outputs.pred_boxes\n\n    return logits, bboxes\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "facebook/detr-resnet-101-dc5", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "DETR (End-to-End Object Detection) model with ResNet-101 backbone (dilated C5 stage). The model is trained on COCO 2017 object detection dataset and achieves an average precision (AP) of 44.9 on the COCO 2017 validation set.", "function_name": "facebook_detr_resnet_101_dc5", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that will be passed to the DETR (End-to-End Object Detection) model.", "default_value": ""}}, "function_code": "def facebook_detr_resnet_101_dc5(image_path):\n    from transformers import DetrFeatureExtractor, DetrForObjectDetection\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-101-dc5')\n    model = DetrForObjectDetection.from_pretrained('facebook/detr-resnet-101-dc5')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    bboxes = outputs.pred_boxes\n\n    return logits, bboxes\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "deformable-detr", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "Deformable DETR model with ResNet-50 backbone trained end-to-end on COCO 2017 object detection (118k annotated images). It was introduced in the paper Deformable DETR: Deformable Transformers for End-to-End Object Detection by Zhu et al. and first released in this repository.", "function_name": "deformable_detr", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be used as input for object detection", "default_value": ""}}, "function_code": "def deformable_detr(image_path):\n    from transformers import AutoImageProcessor, DeformableDetrForObjectDetection\n    import torch\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = AutoImageProcessor.from_pretrained('SenseTime/deformable-detr')\n    model = DeformableDetrForObjectDetection.from_pretrained('SenseTime/deformable-detr')\n    inputs = processor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    return outputs\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-hard-hat-detection", "python_environment_requirements": ["ultralyticsplus==0.0.24", "ultralytics==8.0.23"], "description": "A YOLOv8 model for detecting hard hats in images. The model can distinguish between 'Hardhat' and 'NO-Hardhat' classes. It can be used to ensure safety compliance in construction sites or other industrial environments where hard hats are required.", "function_name": "keremberke_yolov8m_hard_hat_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image", "default_value": ""}, "output_path": {"type": "str", "description": "Specifies the output path where the detected images or results will be saved.", "default_value": ""}, "conf": {"type": "str", "description": "The configuration file or string", "default_value": "0.25"}, "iou": {"type": "float", "description": "The intersection over union (IOU) threshold for non-maximum suppression. It determines the overlap required for two bounding boxes to be considered as the same object.", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "Boolean flag indicating whether to use agnostic NMS or not in the YOLOv8 model for detecting hard hats in images.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to return.", "default_value": "1000"}}, "function_code": "def keremberke_yolov8m_hard_hat_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                          max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8m-hard-hat-detection')\n\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n\n    print(results[0].boxes)\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "License Plate Detection", "api_name": "keremberke/yolov5m-license-plate", "python_environment_requirements": ["pip install -U yolov5"], "description": "A YOLOv5 model for license plate detection trained on a custom dataset. The model can detect license plates in images with high accuracy.", "function_name": "keremberke_yolov5m_license_plate", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the detected license plates", "default_value": ""}, "size": {"type": "int", "description": "The size of the input images in pixels", "default_value": "640"}, "augment": {"type": "bool", "description": "Flag indicating whether or not to apply augmentation during license plate detection.", "default_value": "False"}, "conf": {"type": "str", "description": "The configuration for the YOLOv5m license plate detection model", "default_value": "0.25"}, "iou": {"type": "str", "description": "The IoU (Intersection over Union) threshold used for non-maximum suppression during license plate detection.", "default_value": "0.45"}, "agnostic": {"type": "str", "description": "This argument determines whether the model should be agnostic to any specific license plate type or design.", "default_value": "False,"}, "max_det": {"type": "int", "description": "The maximum number of license plates to detect in an image", "default_value": "1000"}}, "function_code": "def keremberke_yolov5m_license_plate(image_path, output_path, size=640, augment=False, conf=0.25, iou=0.45,\n                                     agnostic=False,\n                                     multi_label=False, max_det=1000):\n    import yolov5\n    import os\n\n    model = yolov5.load('keremberke/yolov5m-license-plate')\n    model.conf = conf\n    model.iou = iou\n    model.agnostic = agnostic\n    model.multi_label = multi_label\n    model.max_det = max_det\n\n    results = model(image_path, size=size, augment=augment)\n    predictions = results.pred[0]\n    boxes = predictions[:, :4]\n    scores = predictions[:, 4]\n    categories = predictions[:, 5]\n\n    results.save(output_path)\n\n    return boxes, scores, categories, os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-valorant-detection", "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "A YOLOv8 model for object detection in Valorant game, trained on a custom dataset. It detects dropped spike, enemy, planted spike, and teammate objects.", "function_name": "keremberke_yolov8m_valorant_detection", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to perform object detection on in the Valorant game", "default_value": ""}, "output_path": {"type": "str", "description": "The file path where the detection results will be saved", "default_value": ""}}, "function_code": "def keremberke_yolov8m_valorant_detection(image_path, output_path):\n    from ultralyticsplus import YOLO, render_result\n    import os\n\n    model = YOLO('keremberke/yolov8m-valorant-detection')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-csgo-player-detection", "python_environment_requirements": ["ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "An object detection model trained to detect Counter-Strike: Global Offensive (CS:GO) players. The model is based on the YOLOv8 architecture and can identify 'ct', 'cthead', 't', and 'thead' labels.", "function_name": "keremberke_yolov8m_csgo_player_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image that needs to be processed for player detection.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output file where the detection results will be saved.", "default_value": ""}}, "function_code": "def keremberke_yolov8m_csgo_player_detection(image_path, output_path):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8m-csgo-player-detection')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n\n    render.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8s-table-extraction", "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "A YOLOv8 model for table extraction in documents, capable of detecting bordered and borderless tables. Trained on the table-extraction dataset, the model achieves a mAP@0.5 of 0.984 on the validation set.", "function_name": "keremberke_yolov8s_table_extraction", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that contains the table to be extracted.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the extracted tables will be saved.", "default_value": ""}}, "function_code": "def keremberke_yolov8s_table_extraction(image_path, output_path):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8s-table-extraction')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-large-patch14", "python_environment_requirements": ["torch", "transformers", "PIL", "requests"], "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. It uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features. OWL-ViT is trained on publicly available image-caption data and fine-tuned on publicly available object detection datasets such as COCO and OpenImages.", "function_name": "google_owlvit_large_patch14", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be used for object detection", "default_value": ""}, "prompt": {"type": "str", "description": "The text prompt for the object detection query", "default_value": ""}}, "function_code": "def google_owlvit_large_patch14(image_path, prompt):\n    import requests\n    from PIL import Image\n    import torch\n    from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-large-patch14\")\n    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-large-patch14\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    i = 0\n    text = prompt[i]\n    boxes, scores, labels = results[i][\"boxes\"], results[i][\"scores\"], results[i][\"labels\"]\n\n    score_threshold = 0.1\n    for box, score, label in zip(boxes, scores, labels):\n        box = [round(i, 2) for i in box.tolist()]\n        if score >= score_threshold:\n            print(f\"Detected {text[label]} with confidence {round(score.item(), 3)} at location {box}\")\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-nlf-head-detection", "python_environment_requirements": ["pip install ultralyticsplus==0.0.24 ultralytics==8.0.23"], "description": "A YOLOv8 model trained for head detection in American football. The model is capable of detecting helmets, blurred helmets, difficult helmets, partial helmets, and sideline helmets.", "function_name": "keremberke_yolov8m_nlf_head_detection", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed by the YOLOv8 model for head detection.", "default_value": ""}, "conf": {"type": "str", "description": "The configuration for the YOLOv8 model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The Intersection over Union (IoU) threshold used for object detection confidence", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "Indicates whether to apply agnostic NMS during head detection.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to be made by the model", "default_value": "1000"}}, "function_code": "def keremberke_yolov8m_nlf_head_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                          max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8m-nlf-head-detection')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-forklift-detection", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "A YOLOv8 model for detecting forklifts and persons in images.", "function_name": "keremberke_yolov8m_forklift_detection", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed", "default_value": ""}, "conf": {"type": "str", "description": "The configuration file for the YOLOv8 model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The Intersection over Union (IoU) threshold for object detection", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "A boolean flag indicating whether to use agnostic NMS or not for post-processing", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to return", "default_value": "1000"}}, "function_code": "def keremberke_yolov8m_forklift_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                          max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8m-forklift-detection')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "zero-shot-object-detection", "api_name": "google/owlvit-base-patch16", "python_environment_requirements": ["requests", "PIL", "torch", "transformers"], "description": "OWL-ViT is a zero-shot text-conditioned object detection model that can be used to query an image with one or multiple text queries. OWL-ViT uses CLIP as its multi-modal backbone, with a ViT-like Transformer to get visual features and a causal language model to get the text features.", "function_name": "google_owlvit_base_patch16", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image", "default_value": ""}, "prompt": {"type": "str", "description": "The text query or prompt to query the image with", "default_value": ""}}, "function_code": "def google_owlvit_base_patch16(image_path, prompt):\n    import torch\n    import requests\n    from PIL import Image\n    from transformers import OwlViTProcessor, OwlViTForObjectDetection\n\n    processor = OwlViTProcessor.from_pretrained(\"google/owlvit-base-patch16\")\n    model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch16\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n\n    target_sizes = torch.Tensor([image.size[::-1]])\n    results = processor.post_process(outputs=outputs, target_sizes=target_sizes)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-plane-detection", "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "A YOLOv8 model for plane detection trained on the keremberke/plane-detection dataset. The model is capable of detecting planes in images with high accuracy.", "function_name": "keremberke_yolov8m_plane_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image that needs plane detection", "default_value": ""}, "output_path": {"type": "str", "description": "The output path where the resulting plane detection images will be saved.", "default_value": ""}, "conf": {"type": "str", "description": "The configuration for the YOLOv8m plane detection model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The Intersection over Union (IoU) threshold for plane detection. This parameter controls the minimum overlap between the predicted bounding box and the ground truth bounding box required for a detection to be considered as valid.", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "A boolean flag indicating whether to use agnostic NMS or not. If set to True, agnostic NMS will be used, otherwise, the default NMS will be used.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to be outputted by the model", "default_value": "1000"}}, "function_code": "def keremberke_yolov8m_plane_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False, max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8m-plane-detection')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8s-csgo-player-detection", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players. Supports the labels ['ct', 'cthead', 't', 'thead'].", "function_name": "keremberke_yolov8s_csgo_player_detection", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image to be processed", "default_value": ""}, "output_path": {"type": "str", "description": "The path where the detection output will be saved", "default_value": ""}}, "function_code": "def keremberke_yolov8s_csgo_player_detection(image_path, output_path):\n    from PIL import Image\n    from io import BytesIO\n    from urllib.request import urlopen\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8s-csgo-player-detection')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    image = Image.open(image_path)\n    results = model.predict(image)\n\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8m-blood-cell-detection", "python_environment_requirements": ["ultralyticsplus==0.0.24", "ultralytics==8.0.23"], "description": "A YOLOv8 model for blood cell detection, including Platelets, RBC, and WBC. Trained on the blood-cell-object-detection dataset.", "function_name": "keremberke_yolov8m_blood_cell_detection", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed for blood cell detection", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the blood cell detection results will be saved", "default_value": ""}, "conf": {"type": "str", "description": "The configuration for the YOLOv8 model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The intersection over union (IOU) threshold for bounding box detection. It determines the minimum overlap required between a predicted bounding box and a ground truth bounding box to be considered a correct detection.", "default_value": "0.45"}, "agnostic_nms": {"type": "str", "description": "A flag indicating whether to use agnostic NMS (Non-Maximum Suppression), which is a technique used in object detection algorithms that helps eliminate duplicate object detections. Setting this flag to 'True' will use agnostic NMS and 'False' will not use agnostic NMS.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to return", "default_value": "1000"}}, "function_code": "def keremberke_yolov8m_blood_cell_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                            max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n\n    model = YOLO('keremberke/yolov8m-blood-cell-detection')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8s-hard-hat-detection", "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "An object detection model trained to detect hard hats and no-hard hats in images. The model is based on YOLOv8 architecture and can be used for safety applications.", "function_name": "keremberke_yolov8s_hard_hat_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed for hard hat detection.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the output or prediction results.", "default_value": ""}}, "function_code": "def keremberke_yolov8s_hard_hat_detection(image_path, output_path):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8s-hard-hat-detection')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Transformers", "functionality": "Object Detection", "api_name": "fcakyon/yolov5s-v7.0", "python_environment_requirements": ["pip install -U yolov5"], "description": "Yolov5s-v7.0 is an object detection model trained on the COCO dataset. It can detect objects in images and return their bounding boxes, scores, and categories.", "function_name": "fcakyon_yolov5s_v7_0", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "output_path": {"type": "str", "description": "The file path or directory path where the output should be saved", "default_value": ""}, "conf": {"type": "str", "description": "A configuration file or object", "default_value": "0.25"}, "iou": {"type": "float", "description": "The intersection over union threshold for non-maximum suppression", "default_value": "0.45"}, "agnostic": {"type": "boolean", "description": "Flag indicating whether to use agnostic or not", "default_value": "False"}, "multi_label": {"type": "str", "description": "A string containing the description of the argument", "default_value": "False,max_det=1000"}}, "function_code": "def fcakyon_yolov5s_v7_0(image_path, output_path, conf=0.25, iou=0.45, agnostic=False, multi_label=False, max_det=1000):\n    import yolov5\n    import os\n    model = yolov5.load('fcakyon/yolov5s-v7.0')\n    model.conf = conf\n    model.iou = iou\n    model.agnostic = agnostic\n    model.multi_label = multi_label\n    model.max_det = max_det\n\n    results = model(image_path)\n\n    predictions = results.pred[0]\n    boxes = predictions[:, :4]\n    scores = predictions[:, 4]\n    categories = predictions[:, 5]\n\n    results.show()\n\n    results.save(save_dir=output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Table Extraction", "api_name": "keremberke/yolov8n-table-extraction", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "An object detection model for extracting tables from documents. Supports two label types: 'bordered' and 'borderless'.", "function_name": "keremberke_yolov8n_table_extraction", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the extracted tables", "default_value": ""}, "conf": {"type": "str", "description": "The configuration for the object detection model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The intersection over union (IoU) threshold for object detection", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "A boolean value indicating whether or not to use agnostic NMS during table extraction.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections", "default_value": "1000"}}, "function_code": "def keremberke_yolov8n_table_extraction(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False, max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8n-table-extraction')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "clipseg-rd64-refined", "python_environment_requirements": ["transformers"], "description": "CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.", "function_name": "clipseg_rd64_refined", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for segmentation", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}}, "function_code": "def clipseg_rd64_refined(image_path, prompt):\n    from PIL import Image\n    from transformers import AutoProcessor, CLIPSegModel\n\n    processor = AutoProcessor.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n    model = CLIPSegModel.from_pretrained(\"CIDAS/clipseg-rd64-refined\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(\n        text=[prompt], images=image, return_tensors=\"pt\", padding=True\n    )\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Object Detection", "api_name": "keremberke/yolov8n-csgo-player-detection", "python_environment_requirements": ["pip install ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "A YOLOv8 model for detecting Counter-Strike: Global Offensive (CS:GO) players with supported labels: ['ct', 'cthead', 't', 'thead'].", "function_name": "keremberke_yolov8n_csgo_player_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL of the image to perform CS:GO player detection on.", "default_value": ""}, "output_path": {"type": "str", "description": "The output path where the detection results will be saved.", "default_value": ""}, "conf": {"type": "str", "description": "The configuration for the YOLOv8n CS:GO player detection model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The intersection over union (IOU) threshold for bounding box overlap calculation.", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "A boolean flag indicating whether to use agnostic non-maximum suppression during the CS:GO player detection.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to return", "default_value": "1000"}}, "function_code": "def keremberke_yolov8n_csgo_player_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                             max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8n-csgo-player-detection')\n\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n\n    print(results[0].boxes)\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "License Plate Detection", "api_name": "keremberke/yolov5s-license-plate", "python_environment_requirements": ["pip install -U yolov5"], "description": "A YOLOv5 based license plate detection model trained on a custom dataset.", "function_name": "keremberke_yolov5s_license_plate", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed by the YOLOv5 license plate detection model.", "default_value": ""}, "output_path": {"type": "str", "description": "The file path where the output will be saved.", "default_value": ""}, "size": {"type": "int", "description": "The size of the input image for license plate detection", "default_value": "640"}, "augment": {"type": "bool", "description": "A flag indicating if data augmentation should be applied during detection", "default_value": "False"}, "conf": {"type": "str", "description": "The description of the configuration argument", "default_value": "0.25"}, "iou": {"type": "float", "description": "The IOU threshold for object detection", "default_value": "0.45"}, "agnostic": {"type": "str", "description": "Whether to use agnostic labels or not. If set to True, the model will predict only the class and not the corresponding license plate number.", "default_value": "False,"}, "max_det": {"type": "int", "description": "The maximum number of detections to output", "default_value": "1000"}}, "function_code": "def keremberke_yolov5s_license_plate(image_path, output_path, size=640, augment=False, conf=0.25, iou=0.45,\n                                     agnostic=False,\n                                     multi_label=False, max_det=1000):\n    import yolov5\n    import os\n\n    model = yolov5.load('keremberke/yolov5s-license-plate')\n    model.conf = conf\n    model.iou = iou\n    model.agnostic = agnostic\n    model.multi_label = multi_label\n    model.max_det = max_det\n\n    results = model(image_path, size=size, augment=augment)\n    predictions = results.pred[0]\n    boxes = predictions[:, :4]\n    scores = predictions[:, 4]\n    categories = predictions[:, 5]\n\n    results.show()\n\n    results.save(output_path)\n\n    return boxes, scores, categories, os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "openmmlab/upernet-convnext-small", "python_environment_requirements": ["transformers"], "description": "UperNet framework for semantic segmentation, leveraging a ConvNeXt backbone. UperNet was introduced in the paper Unified Perceptual Parsing for Scene Understanding by Xiao et al. Combining UperNet with a ConvNeXt backbone was introduced in the paper A ConvNet for the 2020s.", "function_name": "openmmlab_upernet_convnext_small", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image", "default_value": ""}}, "function_code": "def openmmlab_upernet_convnext_small(image_path):\n    from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n    from PIL import Image\n\n    image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n    model = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n\n    image = Image.open(image_path).convert(\"RGB\")\n\n    inputs = image_processor(images=image, return_tensors=\"pt\")\n\n    outputs = model(**inputs)\n\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Blood Cell Detection", "api_name": "keremberke/yolov8n-blood-cell-detection", "python_environment_requirements": ["ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "This model detects blood cells in images, specifically Platelets, RBC, and WBC. It is based on the YOLOv8 architecture and trained on the blood-cell-object-detection dataset.", "function_name": "keremberke_yolov8n_blood_cell_detection", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be analyzed", "default_value": ""}, "output_path": {"type": "str", "description": "The path for saving the detection results", "default_value": ""}, "conf": {"type": "str", "description": "The configuration file path for the YOLOv8n blood cell detection model", "default_value": "0.25"}, "iou": {"type": "float", "description": "The Intersection over Union threshold for object detection. It is used to determine the overlap between the predicted bounding box and the ground truth bounding box.", "default_value": "0.45"}, "agnostic_nms": {"type": "bool", "description": "Whether or not to use agnostic NMS during the blood cell detection process.", "default_value": "False"}, "max_det": {"type": "int", "description": "The maximum number of detections to be returned.", "default_value": "1000"}}, "function_code": "def keremberke_yolov8n_blood_cell_detection(image_path, output_path, conf=0.25, iou=0.45, agnostic_nms=False,\n                                            max_det=1000):\n    from ultralyticsplus import YOLO, render_result\n    import os\n    model = YOLO('keremberke/yolov8n-blood-cell-detection')\n    model.overrides['conf'] = conf\n    model.overrides['iou'] = iou\n    model.overrides['agnostic_nms'] = agnostic_nms\n    model.overrides['max_det'] = max_det\n\n    results = model.predict(image_path)\n    print(results[0].boxes)\n\n    render = render_result(model=model, image=image_path, result=results[0])\n    render.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Semantic Segmentation", "api_name": "nvidia/segformer-b0-finetuned-ade-512-512", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "SegFormer model fine-tuned on ADE20k at resolution 512x512. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.", "function_name": "nvidia_segformer_b0_finetuned_ade_512_512", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image file.", "default_value": ""}}, "function_code": "def nvidia_segformer_b0_finetuned_ade_512_512(image_path):\n    from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n    from PIL import Image\n\n    processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "nvidia/segformer-b5-finetuned-ade-640-640", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "SegFormer model fine-tuned on ADE20k at resolution 640x640. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.", "function_name": "nvidia_segformer_b5_finetuned_ade_640_640", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image to be processed.", "default_value": ""}}, "function_code": "def nvidia_segformer_b5_finetuned_ade_640_640(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n    from PIL import Image\n\n    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b5-finetuned-ade-512-512\")\n    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-ade-512-512\")\n\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Semantic Segmentation", "api_name": "nvidia/segformer-b2-finetuned-cityscapes-1024-1024", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.", "function_name": "nvidia_segformer_b2_finetuned_cityscapes_1024_1024", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def nvidia_segformer_b2_finetuned_cityscapes_1024_1024(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n    from PIL import Image\n\n    feature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\n    model = SegformerForSemanticSegmentation.from_pretrained('nvidia/segformer-b2-finetuned-cityscapes-1024-1024')\n\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "nvidia/segformer-b0-finetuned-cityscapes-1024-1024", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.", "function_name": "nvidia_segformer_b0_finetuned_cityscapes_1024_1024", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be segmented", "default_value": ""}}, "function_code": "def nvidia_segformer_b0_finetuned_cityscapes_1024_1024(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n    from PIL import Image\n    import requests\n\n    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\")\n    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-cityscapes-1024-1024\")\n\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "facebook/detr-resnet-50-panoptic", "python_environment_requirements": ["torch", "numpy", "transformers", "PIL", "requests", "io"], "description": "DEtection TRansformer (DETR) model trained end-to-end on COCO 2017 panoptic (118k annotated images). It was introduced in the paper End-to-End Object Detection with Transformers by Carion et al. and first released in this repository.", "function_name": "facebook_detr_resnet_50_panoptic", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for which object detection is to be performed.", "default_value": ""}}, "function_code": "def facebook_detr_resnet_50_panoptic(image_path):\n    import io\n    import requests\n    from PIL import Image\n    import torch\n    import numpy as np\n    from transformers import DetrFeatureExtractor, DetrForSegmentation\n    from transformers.models.detr.feature_extraction_detr import rgb_to_id\n\n    image = Image.open(image_path)\n    feature_extractor = DetrFeatureExtractor.from_pretrained('facebook/detr-resnet-50-panoptic')\n    model = DetrForSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic')\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n    processed_sizes = torch.as_tensor(inputs['pixel_values'].shape[-2:]).unsqueeze(0)\n    result = feature_extractor.post_process_panoptic(outputs, processed_sizes)[0]\n    panoptic_seg = Image.open(io.BytesIO(result['png_string']))\n    panoptic_seg = np.array(panoptic_seg, dtype=np.uint8)\n    panoptic_seg_id = rgb_to_id(panoptic_seg)\n\n    return panoptic_seg_id\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/maskformer-swin-base-coco", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "MaskFormer model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.", "function_name": "facebook_maskformer_swin_base_coco", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def facebook_maskformer_swin_base_coco(image_path):\n    from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n    from PIL import Image\n    import requests\n\n    feature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-coco')\n    model = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-coco')\n\n    image = Image.open(image_path)\n\n    inputs = feature_extractor(images=image, return_tensors='pt')\n    outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result['segmentation']\n\n    return predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "mattmdjaga/segformer_b2_clothes", "python_environment_requirements": ["transformers", "PIL", "requests", "matplotlib", "torch"], "description": "SegFormer model fine-tuned on ATR dataset for clothes segmentation.", "function_name": "mattmdjaga_segformer_b2_clothes", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed.", "default_value": ""}}, "function_code": "def mattmdjaga_segformer_b2_clothes(image_path):\n    from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n    from PIL import Image\n    import matplotlib.pyplot as plt\n    import torch.nn as nn\n    import os\n    processor = SegformerImageProcessor.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n    model = AutoModelForSemanticSegmentation.from_pretrained(\"mattmdjaga/segformer_b2_clothes\")\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors=\"pt\")\n\n    outputs = model(**inputs)\n    logits = outputs.logits.cpu()\n\n    upsampled_logits = nn.functional.interpolate(\n        logits,\n        size=image.size[::-1],\n        mode=\"bilinear\",\n        align_corners=False,\n    )\n\n    pred_seg = upsampled_logits.argmax(dim=1)[0]\n    plt.imshow(pred_seg)\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/mask2former-swin-base-coco-panoptic", "python_environment_requirements": ["packages"], "description": "Mask2Former model trained on COCO panoptic segmentation (base-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.", "function_name": "facebook_mask2former_swin_base_coco_panoptic", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed by the Mask2Former model.", "default_value": ""}}, "function_code": "def facebook_mask2former_swin_base_coco_panoptic(image_path):\n    import requests\n    import torch\n    from PIL import Image\n    from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n    processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\n    model = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-base-coco-panoptic')\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors='pt')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result['segmentation']\n\n    return predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/mask2former-swin-large-cityscapes-semantic", "python_environment_requirements": ["torch", "transformers"], "description": "Mask2Former model trained on Cityscapes semantic segmentation (large-sized version, Swin backbone). It addresses instance, semantic and panoptic segmentation by predicting a set of masks and corresponding labels. The model outperforms the previous SOTA, MaskFormer, in terms of performance and efficiency.", "function_name": "facebook_mask2former_swin_large_cityscapes_semantic", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL of the input image.", "default_value": ""}}, "function_code": "def facebook_mask2former_swin_large_cityscapes_semantic(image_path):\n    import torch\n    from PIL import Image\n    from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n    processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\n    model = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-large-cityscapes-semantic')\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors='pt')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n\n    return class_queries_logits, masks_queries_logits, predicted_semantic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/maskformer-swin-large-ade", "python_environment_requirements": ["packages"], "description": "MaskFormer model trained on ADE20k semantic segmentation (large-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.", "function_name": "facebook_maskformer_swin_large_ade", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file", "default_value": ""}}, "function_code": "def facebook_maskformer_swin_large_ade(image_path):\n    from transformers import MaskFormerImageProcessor, MaskFormerForInstanceSegmentation\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    processor = MaskFormerImageProcessor.from_pretrained('facebook/maskformer-swin-large-ade')\n    inputs = processor(images=image, return_tensors='pt')\n    model = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-large-ade')\n    outputs = model(**inputs)\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n    predicted_semantic_map = processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n\n    return class_queries_logits, masks_queries_logits, predicted_semantic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "shi-labs/oneformer_ade20k_swin_large", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "OneFormer model trained on the ADE20k dataset (large-sized version, Swin backbone). It was introduced in the paper OneFormer: One Transformer to Rule Universal Image Segmentation by Jain et al. and first released in this repository. OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.", "function_name": "shi_labs_oneformer_ade20k_swin_large", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def shi_labs_oneformer_ade20k_swin_large(image_path):\n    from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n    from PIL import Image\n\n    image = Image.open(image_path)\n\n    processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n    model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_large\")\n\n    semantic_inputs = processor(images=image, task_inputs=[\"semantic\"], return_tensors=\"pt\")\n    semantic_outputs = model(**semantic_inputs)\n\n    predicted_semantic_map = \\\n    processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\n\n    return predicted_semantic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/mask2former-swin-large-coco-panoptic", "python_environment_requirements": ["requests", "torch", "PIL", "transformers"], "description": "Mask2Former model trained on COCO panoptic segmentation (large-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.", "function_name": "facebook_mask2former_swin_large_coco_panoptic", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image file.", "default_value": ""}}, "function_code": "def facebook_mask2former_swin_large_coco_panoptic(image_path):\n    import torch\n    import requests\n    from PIL import Image\n    from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n    processor = AutoImageProcessor.from_pretrained(\"facebook/mask2former-swin-large-coco-panoptic\")\n    model = Mask2FormerForUniversalSegmentation.from_pretrained(\"facebook/mask2former-swin-large-coco-panoptic\")\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = processor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result[\"segmentation\"]\n\n    return predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/mask2former-swin-small-coco-instance", "python_environment_requirements": ["requests", "torch", "PIL", "transformers"], "description": "Mask2Former model trained on COCO instance segmentation (small-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. Mask2Former addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation. Mask2Former outperforms the previous SOTA, MaskFormer both in terms of performance an efficiency.", "function_name": "facebook_mask2former_swin_small_coco_instance", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image", "default_value": ""}}, "function_code": "def facebook_mask2former_swin_small_coco_instance(image_path):\n    import torch\n    import requests\n    from PIL import Image\n    from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation\n\n    processor = AutoImageProcessor.from_pretrained('facebook/mask2former-swin-small-coco-instance')\n    model = Mask2FormerForUniversalSegmentation.from_pretrained('facebook/mask2former-swin-small-coco-instance')\n\n    image = Image.open(image_path)\n    inputs = processor(images=image, return_tensors='pt')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_instance_map = result['segmentation']\n\n    return predicted_instance_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "shi-labs/oneformer_ade20k_swin_tiny", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "OneFormer is the first multi-task universal image segmentation framework. It needs to be trained only once with a single universal architecture, a single model, and on a single dataset, to outperform existing specialized models across semantic, instance, and panoptic segmentation tasks. OneFormer uses a task token to condition the model on the task in focus, making the architecture task-guided for training, and task-dynamic for inference, all with a single model.", "function_name": "shi_labs_oneformer_ade20k_swin_tiny", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed", "default_value": ""}, "semantic": {"type": "str", "description": "The task in focus for image segmentation", "default_value": "True"}, "instance": {"type": "str", "description": "The description of the argument", "default_value": "True"}, "panoptic": {"type": "str", "description": "The panoptic segmentation map of an image", "default_value": "True"}}, "function_code": "def shi_labs_oneformer_ade20k_swin_tiny(image_path, semantic=True, instance=True, panoptic=True):\n    from transformers import OneFormerProcessor, OneFormerForUniversalSegmentation\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n\n    processor = OneFormerProcessor.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n    model = OneFormerForUniversalSegmentation.from_pretrained(\"shi-labs/oneformer_ade20k_swin_tiny\")\n\n    pt = \"pt\"\n\n    semantic_inputs = processor(images=image, task_inputs=[semantic], return_tensors=pt)\n    semantic_outputs = model(**semantic_inputs)\n\n    predicted_semantic_map = \\\n    processor.post_process_semantic_segmentation(semantic_outputs, target_sizes=[image.size[::-1]])[0]\n\n    instance_inputs = processor(images=image, task_inputs=[instance], return_tensors=pt)\n    instance_outputs = model(**instance_inputs)\n\n    predicted_instance_map = \\\n    processor.post_process_instance_segmentation(instance_outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n\n    panoptic_inputs = processor(images=image, task_inputs=[panoptic], return_tensors=pt)\n    panoptic_outputs = model(**panoptic_inputs)\n\n    predicted_panoptic_map = \\\n    processor.post_process_panoptic_segmentation(panoptic_outputs, target_sizes=[image.size[::-1]])[0][\"segmentation\"]\n\n    return predicted_semantic_map, predicted_instance_map, predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8m-building-segmentation", "python_environment_requirements": ["pip install ultralyticsplus==0.0.21"], "description": "A YOLOv8 model for building segmentation in satellite images. It can detect and segment buildings in the input images.", "function_name": "keremberke_yolov8m_building_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image that needs to be segmented for building detection and segmentation.", "default_value": ""}}, "function_code": "def keremberke_yolov8m_building_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8m-building-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Semantic Segmentation", "api_name": "nvidia/segformer-b5-finetuned-cityscapes-1024-1024", "python_environment_requirements": ["packages"], "description": "SegFormer model fine-tuned on CityScapes at resolution 1024x1024. It was introduced in the paper SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Xie et al. and first released in this repository.", "function_name": "nvidia_segformer_b5_finetuned_cityscapes_1024_1024", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL to the input image that needs to be segmented.", "default_value": ""}}, "function_code": "def nvidia_segformer_b5_finetuned_cityscapes_1024_1024(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForSemanticSegmentation\n    from PIL import Image\n    import requests\n\n    feature_extractor = SegformerFeatureExtractor.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\")\n    model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b5-finetuned-cityscapes-1024-1024\").to(\"cuda\")\n\n    image = Image.open(image_path)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\").to(\"cuda\")\n    outputs = model(**inputs)\n    logits = outputs.logits\n\n    return logits\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/mask2former-swin-tiny-coco-instance", "python_environment_requirements": ["torch", "transformers", "PIL", "requests"], "description": "Mask2Former model trained on COCO instance segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Masked-attention Mask Transformer for Universal Image Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. You can use this particular checkpoint for instance segmentation.", "function_name": "facebook_mask2former_swin_tiny_coco_instance", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for which instance segmentation needs to be performed.", "default_value": ""}}, "function_code": "def facebook_mask2former_swin_tiny_coco_instance(image_path):\n    from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n    from PIL import Image\n    import requests\n    from diffusers.utils import load_image\n\n    feature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/mask2former-swin-tiny-coco-instance')\n    model = MaskFormerForInstanceSegmentation.from_pretrained('facebook/mask2former-swin-tiny-coco-instance').to('cuda')\n\n    image = load_image(image_path)\n    inputs = feature_extractor(images=image, return_tensors='pt').to('cuda')\n    outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result['segmentation']\n\n    return class_queries_logits, masks_queries_logits, predicted_panoptic_map\n", "executable": false}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/maskformer-swin-base-ade", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "MaskFormer model trained on ADE20k semantic segmentation (base-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository. This model addresses instance, semantic and panoptic segmentation with the same paradigm: by predicting a set of masks and corresponding labels. Hence, all 3 tasks are treated as if they were instance segmentation.", "function_name": "facebook_maskformer_swin_base_ade", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def facebook_maskformer_swin_base_ade(image_path):\n    from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n    from PIL import Image\n    import requests\n    from diffusers.utils import load_image\n\n    feature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-base-ade')\n    model = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-base-ade', ignore_mismatched_sizes=True).to('cuda')\n\n    image = load_image(image_path)\n    inputs = feature_extractor(images=image, return_tensors='pt').to('cuda')\n    outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result['segmentation']\n\n    return class_queries_logits, masks_queries_logits, predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8m-pcb-defect-segmentation", "python_environment_requirements": ["ultralyticsplus==0.0.24", "ultralytics==8.0.23"], "description": "A YOLOv8 model for PCB defect segmentation trained on the pcb-defect-segmentation dataset. The model can detect and segment defects in PCB images, such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.", "function_name": "keremberke_yolov8m_pcb_defect_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the PCB image for defect segmentation", "default_value": ""}}, "function_code": "def keremberke_yolov8m_pcb_defect_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8m-pcb-defect-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "facebook/maskformer-swin-tiny-coco", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "MaskFormer model trained on COCO panoptic segmentation (tiny-sized version, Swin backbone). It was introduced in the paper Per-Pixel Classification is Not All You Need for Semantic Segmentation and first released in this repository.", "function_name": "facebook_maskformer_swin_tiny_coco", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed.", "default_value": ""}}, "function_code": "def facebook_maskformer_swin_tiny_coco(image_path):\n    from transformers import MaskFormerFeatureExtractor, MaskFormerForInstanceSegmentation\n    from PIL import Image\n    import requests\n    from diffusers.utils import load_image\n\n    feature_extractor = MaskFormerFeatureExtractor.from_pretrained('facebook/maskformer-swin-tiny-coco')\n    model = MaskFormerForInstanceSegmentation.from_pretrained('facebook/maskformer-swin-tiny-coco').to('cuda')\n\n    image = load_image(image_path)\n    inputs = feature_extractor(images=image, return_tensors='pt').to('cuda')\n    outputs = model(**inputs)\n\n    class_queries_logits = outputs.class_queries_logits\n    masks_queries_logits = outputs.masks_queries_logits\n\n    result = feature_extractor.post_process_panoptic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    predicted_panoptic_map = result['segmentation']\n\n    return class_queries_logits, masks_queries_logits, predicted_panoptic_map\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8m-pothole-segmentation", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "A YOLOv8 model for pothole segmentation trained on keremberke/pothole-segmentation dataset. It can detect potholes in images and provide segmentation masks for the detected potholes.", "function_name": "yolov8m_pothole_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for pothole segmentation", "default_value": ""}}, "function_code": "def yolov8m_pothole_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8m-pothole-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8s-building-segmentation", "python_environment_requirements": ["ultralyticsplus==0.0.21"], "description": "A YOLOv8 model for building segmentation in satellite images. Trained on the satellite-building-segmentation dataset, it can detect and segment buildings with high accuracy.", "function_name": "keremberke_yolov8s_building_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The path to the satellite image that the model will perform building segmentation on.", "default_value": ""}}, "function_code": "def keremberke_yolov8s_building_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8s-building-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8s-pothole-segmentation", "python_environment_requirements": ["ultralyticsplus", "ultralytics"], "description": "A YOLOv8 model for pothole segmentation. This model detects potholes in images and outputs bounding boxes and masks for the detected potholes.", "function_name": "yolov8s_pothole_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image to be processed", "default_value": ""}}, "function_code": "def yolov8s_pothole_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8s-pothole-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8n-pothole-segmentation", "python_environment_requirements": ["ultralyticsplus", "ultralytics"], "description": "A YOLOv8 model for pothole segmentation in images. The model is trained on the pothole-segmentation dataset and achieves high accuracy in detecting potholes.", "function_name": "yolov8n_pothole_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed by the YOLOv8 model", "default_value": ""}}, "function_code": "def yolov8n_pothole_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8n-pothole-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8n-pcb-defect-segmentation", "python_environment_requirements": ["ultralyticsplus==0.0.23 ultralytics==8.0.21"], "description": "A YOLOv8 model for detecting and segmenting PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.", "function_name": "keremberke_yolov8n_pcb_defect_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image for PCB defect segmentation.", "default_value": ""}}, "function_code": "def keremberke_yolov8n_pcb_defect_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n\n    model = YOLO('keremberke/yolov8n-pcb-defect-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    results = model.predict(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image Variations", "api_name": "lambdalabs/sd-image-variations-diffusers", "python_environment_requirements": ["Diffusers >=0.8.0"], "description": "This version of Stable Diffusion has been fine tuned from CompVis/stable-diffusion-v1-4-original to accept CLIP image embedding rather than text embeddings. This allows the creation of image variations similar to DALLE-2 using Stable Diffusion.", "function_name": "lambdalabs_sd_image_variations_diffusers", "function_arguments": {"original_image_path": {"type": "str", "description": "The path to the original image file", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the directory where the output variations will be saved.", "default_value": ""}, "guidance_scale": {"type": "str", "description": "The scale of diffusion guidance for image variations", "default_value": "3"}}, "function_code": "def lambdalabs_sd_image_variations_diffusers(original_image_path, output_path, guidance_scale=3):\n    from diffusers import StableDiffusionImageVariationPipeline\n    from PIL import Image\n    import torchvision.transforms as transforms\n    import os\n\n    sd_pipe = StableDiffusionImageVariationPipeline.from_pretrained(\n        'lambdalabs/sd-image-variations-diffusers',\n        revision='v2.0',\n    ).to('cuda')\n\n    im = Image.open(original_image_path)\n    tform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Resize(\n            (224, 224),\n            interpolation=transforms.InterpolationMode.BICUBIC,\n            antialias=False,\n        ),\n        transforms.Normalize(\n            [0.48145466, 0.4578275, 0.40821073],\n            [0.26862954, 0.26130258, 0.27577711]\n        ),\n    ])\n    inp = tform(im).to(\"cuda\").unsqueeze(0)\n    out = sd_pipe(inp, guidance_scale=guidance_scale)\n    out['images'][0].save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Human Pose Estimation", "api_name": "lllyasviel/sd-controlnet-openpose", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Human Pose Estimation. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_openpose", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_openpose(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector, LineartDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/sd-controlnet-openpose\"\n\n    image = load_image(control_image_path)\n    # image = image.resize((512, 512))\n\n    processor =  OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n    # processor = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n    control_image = processor(image)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image-to-Image", "api_name": "lllyasviel/sd-controlnet-hed", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on HED Boundary. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_hed", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be processed.", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path where the output will be saved", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_hed(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector, LineartDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/sd-controlnet-hed\"\n\n    image = load_image(control_image_path)\n    # image = image.resize((512, 512))\n\n    processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    # processor = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n    control_image = processor(image)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image Segmentation", "api_name": "lllyasviel/sd-controlnet-seg", "python_environment_requirements": ["diffusers", "transformers", "accelerate"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Image Segmentation. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_seg", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be used for image segmentation", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output file or directory", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_seg(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector, LineartDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n    palette = np.asarray([\n        [0, 0, 0],\n        [120, 120, 120],\n        [180, 120, 120],\n        [6, 230, 230],\n        [80, 50, 50],\n        [4, 200, 3],\n        [120, 120, 80],\n        [140, 140, 140],\n        [204, 5, 255],\n        [230, 230, 230],\n        [4, 250, 7],\n        [224, 5, 255],\n        [235, 255, 7],\n        [150, 5, 61],\n        [120, 120, 70],\n        [8, 255, 51],\n        [255, 6, 82],\n        [143, 255, 140],\n        [204, 255, 4],\n        [255, 51, 7],\n        [204, 70, 3],\n        [0, 102, 200],\n        [61, 230, 250],\n        [255, 6, 51],\n        [11, 102, 255],\n        [255, 7, 71],\n        [255, 9, 224],\n        [9, 7, 230],\n        [220, 220, 220],\n        [255, 9, 92],\n        [112, 9, 255],\n        [8, 255, 214],\n        [7, 255, 224],\n        [255, 184, 6],\n        [10, 255, 71],\n        [255, 41, 10],\n        [7, 255, 255],\n        [224, 255, 8],\n        [102, 8, 255],\n        [255, 61, 6],\n        [255, 194, 7],\n        [255, 122, 8],\n        [0, 255, 20],\n        [255, 8, 41],\n        [255, 5, 153],\n        [6, 51, 255],\n        [235, 12, 255],\n        [160, 150, 20],\n        [0, 163, 255],\n        [140, 140, 140],\n        [250, 10, 15],\n        [20, 255, 0],\n        [31, 255, 0],\n        [255, 31, 0],\n        [255, 224, 0],\n        [153, 255, 0],\n        [0, 0, 255],\n        [255, 71, 0],\n        [0, 235, 255],\n        [0, 173, 255],\n        [31, 0, 255],\n        [11, 200, 200],\n        [255, 82, 0],\n        [0, 255, 245],\n        [0, 61, 255],\n        [0, 255, 112],\n        [0, 255, 133],\n        [255, 0, 0],\n        [255, 163, 0],\n        [255, 102, 0],\n        [194, 255, 0],\n        [0, 143, 255],\n        [51, 255, 0],\n        [0, 82, 255],\n        [0, 255, 41],\n        [0, 255, 173],\n        [10, 0, 255],\n        [173, 255, 0],\n        [0, 255, 153],\n        [255, 92, 0],\n        [255, 0, 255],\n        [255, 0, 245],\n        [255, 0, 102],\n        [255, 173, 0],\n        [255, 0, 20],\n        [255, 184, 184],\n        [0, 31, 255],\n        [0, 255, 61],\n        [0, 71, 255],\n        [255, 0, 204],\n        [0, 255, 194],\n        [0, 255, 82],\n        [0, 10, 255],\n        [0, 112, 255],\n        [51, 0, 255],\n        [0, 194, 255],\n        [0, 122, 255],\n        [0, 255, 163],\n        [255, 153, 0],\n        [0, 255, 10],\n        [255, 112, 0],\n        [143, 255, 0],\n        [82, 0, 255],\n        [163, 255, 0],\n        [255, 235, 0],\n        [8, 184, 170],\n        [133, 0, 255],\n        [0, 255, 92],\n        [184, 0, 255],\n        [255, 0, 31],\n        [0, 184, 255],\n        [0, 214, 255],\n        [255, 0, 112],\n        [92, 255, 0],\n        [0, 224, 255],\n        [112, 224, 255],\n        [70, 184, 160],\n        [163, 0, 255],\n        [153, 0, 255],\n        [71, 255, 0],\n        [255, 0, 163],\n        [255, 204, 0],\n        [255, 0, 143],\n        [0, 255, 235],\n        [133, 255, 0],\n        [255, 0, 235],\n        [245, 0, 255],\n        [255, 0, 122],\n        [255, 245, 0],\n        [10, 190, 212],\n        [214, 255, 0],\n        [0, 204, 255],\n        [20, 0, 255],\n        [255, 255, 0],\n        [0, 153, 255],\n        [0, 41, 255],\n        [0, 255, 204],\n        [41, 0, 255],\n        [41, 255, 0],\n        [173, 0, 255],\n        [0, 245, 255],\n        [71, 0, 255],\n        [122, 0, 255],\n        [0, 255, 184],\n        [0, 92, 255],\n        [184, 255, 0],\n        [0, 133, 255],\n        [255, 214, 0],\n        [25, 194, 194],\n        [102, 255, 0],\n        [92, 0, 255],\n    ])\n\n    checkpoint = \"lllyasviel/sd-controlnet-seg\"\n\n    image = load_image(control_image_path)\n    image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n    image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n\n    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n\n    with torch.no_grad():\n        outputs = image_segmentor(pixel_values)\n\n    seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n\n    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)  # height, width, 3\n\n    for label, color in enumerate(palette):\n        color_seg[seg == label, :] = color\n\n    color_seg = color_seg.astype(np.uint8)\n\n    control_image = Image.fromarray(color_seg)\n\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Depth Estimation", "api_name": "lllyasviel/sd-controlnet-depth", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "PIL", "numpy", "torch"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Depth estimation. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_depth", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the results will be saved", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_depth(image_path, prompt,  output_path):\n    from transformers import pipeline\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n    from PIL import Image\n    import numpy as np\n    import torch\n    from diffusers.utils import load_image\n    import os\n\n    depth_estimator = pipeline('depth-estimation')\n\n    image = load_image(image_path)\n    image = depth_estimator(image)['depth']\n    image = np.array(image)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    image = Image.fromarray(image)\n\n    controlnet = ControlNetModel.from_pretrained('lllyasviel/sd-controlnet-depth', torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained('runwayml/stable-diffusion-v1-5', controlnet=controlnet,\n                                                             safety_checker=None, torch_dtype=torch.float16)\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_model_cpu_offload()\n\n    image = pipe(prompt, image, num_inference_steps=20).images[0]\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Diffusion Models", "api_name": "lllyasviel/sd-controlnet-scribble", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_scribble", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image used as a condition for the ControlNet", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file where the result will be saved.", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_scribble(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector, LineartDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/sd-controlnet-scribble\"\n\n    image = load_image(control_image_path)\n    # image = image.resize((512, 512))\n\n    processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    # processor = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n    control_image = processor(image)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image-to-Image", "api_name": "lllyasviel/control_v11p_sd15_canny", "python_environment_requirements": ["pip install opencv-contrib-python", "pip install diffusers transformers accelerate"], "description": "Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.", "function_name": "lllyasviel_control_v11p_sd15_canny", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the image used as input for the ControlNet model.", "default_value": ""}, "prompt": {"type": "str", "description": "The prompt for the neural network to generate control conditions based on Canny edges", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file", "default_value": ""}, "high_threshold": {"type": "float", "description": "The high threshold for the Canny edge detection algorithm", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_canny(control_image_path, prompt, output_image_path ,low_threshold=100, high_threshold=200):\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    import numpy as np\n    import cv2\n    from PIL import Image\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_canny\"\n    image = load_image(control_image_path)\n    image = np.array(image)\n    image = cv2.Canny(image, low_threshold, high_threshold)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    control_image = Image.fromarray(image)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n    )\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n    generator = torch.manual_seed(33)\n    image = pipe(\n        prompt,\n        num_inference_steps=20,\n        generator=generator,\n        image=control_image,\n    ).images[0]\n    image.save(output_image_path)\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "ControlNet", "api_name": "lllyasviel/control_v11p_sd15_lineart", "python_environment_requirements": ["pip install diffusers transformers accelerate controlnet_aux==0.3.0"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart images.", "function_name": "lllyasviel_control_v11p_sd15_lineart", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image file.", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_lineart(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector, LineartDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_lineart\"\n\n    image = load_image(control_image_path)\n    image = image.resize((512, 512))\n\n    # processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    processor = LineartDetector.from_pretrained(\"lllyasviel/Annotators\")\n\n    control_image = processor(image)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Normal Map Estimation", "api_name": "lllyasviel/sd-controlnet-normal", "python_environment_requirements": ["diffusers", "transformers", "accelerate"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Normal Map Estimation. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_normal", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL to the input image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the results will be saved.", "default_value": ""}}, "function_code": "def lllyasviel_sd_controlnet_normal(image_path, prompt,  output_path):\n    from PIL import Image\n    from transformers import pipeline\n    import numpy as np\n    import os\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n    import torch\n    import cv2\n    from diffusers.utils import load_image\n\n    # Load image\n    image = load_image(image_path).convert(\"RGB\")\n\n    # Depth estimation\n    depth_estimator = pipeline(\"depth-estimation\", model='Intel/dpt-hybrid-midas')\n    image = depth_estimator(image)[\"predicted_depth\"][0]\n    image = image.numpy()\n\n    # Normalize depth image\n    image_depth = image.copy()\n    image_depth -= np.min(image_depth)\n    image_depth /= np.max(image_depth)\n\n    # Threshold for background\n    bg_threshold = 0.4\n\n    # Apply Sobel filter\n    x = cv2.Sobel(image, cv2.CV_32F, 1, 0, ksize=3)\n    x[image_depth < bg_threshold] = 0\n    y = cv2.Sobel(image, cv2.CV_32F, 0, 1, ksize=3)\n    y[image_depth < bg_threshold] = 0\n    z = np.ones_like(x) * np.pi * 2.0\n    image = np.stack([x, y, z], axis=2)\n    image /= np.sum(image ** 2.0, axis=2, keepdims=True) ** 0.5\n    image = (image * 127.5 + 127.5).clip(0, 255).astype(np.uint8)\n    image = Image.fromarray(image)\n\n    # Load ControlNet model\n    controlnet = ControlNetModel.from_pretrained(\"fusing/stable-diffusion-v1-5-controlnet-normal\",\n                                                 torch_dtype=torch.float16)\n\n    # Load StableDiffusionControlNetPipeline\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet,\n                                                             safety_checker=None, torch_dtype=torch.float16)\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_model_cpu_offload()\n\n    # Apply controlnet to image\n    image = pipe(prompt, image, num_inference_steps=20).images[0]\n\n    # Save output image\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Diffusers", "functionality": "Text-to-Image", "api_name": "lllyasviel/control_v11p_sd15_scribble", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux==0.3.0"], "description": "Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Scribble images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.", "function_name": "llllyasviel_control_v11p_sd15_scribble", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image for the ControlNet conditioned on Scribble images.", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The output path for the generated image", "default_value": ""}}, "function_code": "def llllyasviel_control_v11p_sd15_scribble(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_scribble\"\n\n    image = load_image(control_image_path)\n\n    # processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n\n    control_image = processor(image, scribble=True)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Diffusers", "functionality": "Text-to-Image Diffusion Models", "api_name": "lllyasviel/control_v11p_sd15_openpose", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux==0.3.0"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on openpose images.", "function_name": "lllyasviel_control_v11p_sd15_openpose", "function_arguments": {"control_image_path": {"type": "str", "description": "The file path to the control image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_openpose(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector, OpenposeDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_openpose\"\n\n    image = load_image(control_image_path)\n\n    # processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    processor = OpenposeDetector.from_pretrained('lllyasviel/ControlNet')\n\n    control_image = processor(image, safe=True)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Diffusion-based text-to-image generation model", "api_name": "lllyasviel/control_v11e_sd15_ip2p", "python_environment_requirements": ["diffusers", "transformers", "accelerate"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on instruct pix2pix images.", "function_name": "lllyasviel_control_v11e_sd15_ip2p", "function_arguments": {"control_image_path": {"type": "str", "description": "Path to the control image for the ControlNet", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to the output image file", "default_value": ""}}, "function_code": "def lllyasviel_control_v11e_sd15_ip2p(control_image_path, prompt, output_image_path) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11e_sd15_ip2p\"\n\n    image = load_image(control_image_path)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Diffusion-based text-to-image generation", "api_name": "lllyasviel/control_v11p_sd15_softedge", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux==0.3.0"], "description": "Controlnet v1.1 is a diffusion-based text-to-image generation model that controls pretrained large diffusion models to support additional input conditions. This checkpoint corresponds to the ControlNet conditioned on Soft edges. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.", "function_name": "lllyasviel_control_v11p_sd15_softedge", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image used for conditioning the text-to-image generation model.", "default_value": ""}, "prompt": {"type": "str", "description": "The prompt for generating text-to-image results with ControlNet v1.1 conditioned on Soft edges.", "default_value": ""}, "output_image_path": {"type": "str", "description": "The path to save the generated output image", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_softedge(control_image_path: str, prompt: str, output_image_path: str) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import PidiNetDetector, HEDdetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_softedge\"\n\n    image = load_image(control_image_path)\n\n    # processor = HEDdetector.from_pretrained('lllyasviel/Annotators')\n    processor = PidiNetDetector.from_pretrained('lllyasviel/Annotators')\n\n    control_image = processor(image, safe=True)\n    control_image.save(control_image_path)\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "swin2SR-lightweight-x2-64", "python_environment_requirements": ["transformers, torch"], "description": "Swin2SR model that upscales images x2. It was introduced in the paper Swin2SR: SwinV2 Transformer for Compressed Image Super-Resolution and Restoration by Conde et al. and first released in this repository. This model is intended for lightweight image super resolution.", "function_name": "swin2SR_lightweight_x2_64", "function_arguments": {"input_image": {"type": "str", "description": "The input image for super-resolution upscaling.", "default_value": ""}, "output_path": {"type": "str", "description": "The path to save the output image", "default_value": ""}}, "function_code": "def swin2SR_lightweight_x2_64(input_image, output_path):\n    import torch\n    from PIL import Image\n    from torchvision.transforms import ToTensor\n    from transformers import Swin2SRForImageSuperResolution, AutoImageProcessor\n    import os\n    import numpy as np\n\n    # Load the pre-trained model and processor\n    model = Swin2SRForImageSuperResolution.from_pretrained(\"caidas/swin2SR-lightweight-x2-64\").to(\"cuda\")\n    processor = AutoImageProcessor.from_pretrained(\"caidas/swin2SR-lightweight-x2-64\")\n\n    # Preprocess the input image\n    image = Image.open(input_image)\n    image_tensor = ToTensor()(image).unsqueeze(0)\n\n    # Run the model to generate the super-resolution image\n    inputs = processor(image_tensor, return_tensors=\"pt\").to(\"cuda\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    # Postprocess the output image\n    output = outputs.reconstruction.data.squeeze().float().cpu().clamp_(0, 1).numpy()\n    output = np.moveaxis(output, source=0, destination=-1)\n    output = (output * 255.0).round().astype(np.uint8)\n    sr_image = Image.fromarray(output)\n    sr_image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Diffusion Models", "api_name": "lllyasviel/control_v11p_sd15_mlsd", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux"], "description": "Controlnet v1.1 is a neural network structure to control diffusion models by adding extra conditions. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5. This checkpoint corresponds to the ControlNet conditioned on MLSD images.", "function_name": "lllyasviel_control_v11p_sd15_mlsd", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output location where the results will be stored.", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_mlsd(image_path: str, prompt: str, output_path: str) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import MLSDdetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_mlsd\"\n    image = load_image(image_path)\n    prompt = prompt\n    processor = MLSDdetector.from_pretrained('lllyasviel/ControlNet')\n    control_image = processor(image)\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        'runwayml/stable-diffusion-v1-5', controlnet=controlnet, torch_dtype=torch.float16\n    ).to('cuda')\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n    generator = torch.manual_seed(0)\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Diffusion-based text-to-image generation model", "api_name": "lllyasviel/control_v11p_sd15_normalbae", "python_environment_requirements": ["diffusers", "transformers", "accelerate", "controlnet_aux"], "description": "ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on normalbae images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.", "function_name": "lllyasviel_control_v11p_sd15_normalbae", "function_arguments": {"control_image_path": {"type": "str", "description": "The path to the control image that will be used for conditioning the ControlNet v1.1", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_image_path": {"type": "str", "description": "The file path to save the output image", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_normalbae(control_image_path: str, prompt: str,\n                                           output_image_path: str) -> None:\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import NormalBaeDetector\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    image = load_image(control_image_path)\n    processor = NormalBaeDetector.from_pretrained(\"lllyasviel/Annotators\")\n    control_image = processor(image)\n    control_image.save(control_image_path)\n    controlnet = ControlNetModel.from_pretrained('lllyasviel/control_v11p_sd15_normalbae', torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\",\n        controlnet=controlnet,\n        torch_dtype=torch.float16,\n    ).to(\"cuda\")\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n    generator = torch.manual_seed(33)\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_image_path)\n\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image-to-Image", "api_name": "GreeneryScenery/SheepsControlV3", "python_environment_requirements": ["transformers", "torch"], "description": "GreeneryScenery/SheepsControlV3 is a model for image-to-image tasks. It can be used to generate images based on the input image and optional text guidance. The model has some limitations, such as the conditioning image not affecting the output image much. Improvements can be made by training for more epochs, using better prompts, and preprocessing the data.", "function_name": "GreeneryScenery_SheepsControlV3", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image file.", "default_value": ""}, "text_guidance": {"type": "str", "description": "The optional text guidance for generating images based on the input image", "default_value": ""}}, "function_code": "def GreeneryScenery_SheepsControlV3(image_path, text_guidance=None):\n    from transformers import pipeline\n\n    model = pipeline('image-to-image', model='GreeneryScenery/SheepsControlV3')\n    result = model({'image': image_path, 'text_guidance': text_guidance})\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image-to-Image", "api_name": "GreeneryScenery/SheepsControlV5", "python_environment_requirements": ["huggingface_hub", "transformers", "torch"], "description": "SheepsControlV5 is an image-to-image model trained on the poloclub/diffusiondb dataset. It is designed for transforming input images into a different style or representation.", "function_name": "GreeneryScenery_SheepsControlV5", "function_arguments": {"input_text": {"type": "str", "description": "The input text to be processed by the SheepsControlV5 model", "default_value": ""}, "model_name": {"type": "str", "description": "The name of the model to be used for image transformation", "default_value": ""}, "tokenizer_name": {"type": "str", "description": "The name of the tokenizer to be used in the GreeneryScenery_SheepsControlV5 function.", "default_value": ""}}, "function_code": "def GreeneryScenery_SheepsControlV5(input_text, model_name, tokenizer_name):\n    from transformers import AutoModelForSequenceClassification, AutoTokenizer\n\n    # Load the model and tokenizer\n    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n\n    # Tokenize the input text\n    inputs = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\")\n\n    # Make predictions\n    outputs = model(**inputs)\n    predictions = outputs.logits.argmax(dim=1)\n\n    return predictions\n", "executable": false}
{"domain": "Computer Vision Image-to-Image", "framework": "Keras", "functionality": "Image Deblurring", "api_name": "google/maxim-s3-deblurring-gopro", "python_environment_requirements": ["huggingface_hub", "PIL", "tensorflow", "numpy", "requests"], "description": "MAXIM model pre-trained for image deblurring. It was introduced in the paper MAXIM: Multi-Axis MLP for Image Processing by Zhengzhong Tu, Hossein Talebi, Han Zhang, Feng Yang, Peyman Milanfar, Alan Bovik, Yinxiao Li and first released in this repository.", "function_name": "google_maxim_s3_deblurring_gopro", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be deblurred.", "default_value": ""}}, "function_code": "def google_maxim_s3_deblurring_gopro(image_path):\n    from huggingface_hub import from_pretrained_keras\n    from PIL import Image\n    from diffusers.utils import load_image\n    import tensorflow as tf\n    import numpy as np\n    import requests\n\n    image = load_image(image_path)\n    image = np.array(image)\n    image = tf.convert_to_tensor(image)\n    image = tf.image.resize(image, (256, 256))\n\n    model = from_pretrained_keras(\"google/maxim-s3-deblurring-gopro\").to(\"cuda\")\n\n    # put image on GPU\n    image = tf.expand_dims(image, 0).to(\"cuda\")\n    predictions = model.predict(image)\n\n    return predictions\n", "executable": false}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Text-to-Image Diffusion Models", "api_name": "lllyasviel/control_v11p_sd15s2_lineart_anime", "python_environment_requirements": ["pip install diffusers transformers accelerate", "pip install controlnet_aux==0.3.0"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on lineart_anime images.", "function_name": "lllyasviel_control_v11p_sd15s2_lineart_anime", "function_arguments": {"control_image_path": {"type": "str", "description": "The file path of the control image.", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The description of the argument", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15s2_lineart_anime(control_image_path, prompt, output_path):\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from controlnet_aux import LineartAnimeDetector\n    from transformers import CLIPTextModel\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    checkpoint = \"lllyasviel/control_v11p_sd15s2_lineart_anime\"\n    image = load_image(control_image_path)\n    image = image.resize((512, 512))\n    processor = LineartAnimeDetector.from_pretrained(\"lllyasviel/Annotators\")\n    control_image = processor(image)\n    text_encoder = CLIPTextModel.from_pretrained(\"runwayml/stable-diffusion-v1-5\", subfolder=\"text_encoder\",\n                                                 num_hidden_layers=11, torch_dtype=torch.float16)\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\",\n                                                             text_encoder=text_encoder, controlnet=controlnet,\n                                                             torch_dtype=torch.float16).to(\"cuda\")\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n    generator = torch.manual_seed(0)\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image Inpainting", "api_name": "lllyasviel/control_v11p_sd15_inpaint", "python_environment_requirements": ["pip install diffusers transformers accelerate"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on inpaint images.", "function_name": "lllyasviel_control_v11p_sd15_inpaint", "function_arguments": {"original_image_path": {"type": "str", "description": "The path of the original image to be inpainted", "default_value": ""}, "mask_image_path": {"type": "str", "description": "The path to the mask image", "default_value": ""}, "prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "output_path": {"type": "str", "description": "The path to the output directory where the result files will be saved.", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_inpaint(original_image_path, mask_image_path, prompt, output_path):\n    from diffusers import ControlNetModel, StableDiffusionControlNetInpaintPipeline, DDIMScheduler\n    from diffusers.utils import load_image\n    import numpy as np\n    import torch\n    import os\n\n    '''\n    description: This function uses the pretrained model from diffusers to inpaint the image\n\n    original_image_path: path to the original image\n    mask_image_path: path to the mask image\n    prompt: prompt for the image\n    output_path: path to the output image\n    '''\n\n    init_image = load_image(original_image_path)\n    init_image = init_image.resize((512, 512))\n\n    generator = torch.Generator(device=\"cuda\").manual_seed(1)\n\n    mask_image = load_image(mask_image_path)\n    mask_image = mask_image.resize((512, 512))\n\n    def make_inpaint_condition(image, image_mask):\n        image = np.array(image.convert(\"RGB\")).astype(np.float32) / 255.0\n        image_mask = np.array(image_mask.convert(\"L\")).astype(np.float32) / 255.0\n\n        assert image.shape[0:1] == image_mask.shape[0:1], \"image and image_mask must have the same image size\"\n        image[image_mask > 0.5] = -1.0  # set as masked pixel\n        image = np.expand_dims(image, 0).transpose(0, 3, 1, 2)\n        image = torch.from_numpy(image)\n        return image\n\n    control_image = make_inpaint_condition(init_image, mask_image).to(\"cuda\")\n\n    controlnet = ControlNetModel.from_pretrained(\n        \"lllyasviel/control_v11p_sd15_inpaint\", torch_dtype=torch.float16\n    )\n    pipe = StableDiffusionControlNetInpaintPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = DDIMScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    # generate image\n    image = pipe(\n        prompt,\n        num_inference_steps=20,\n        generator=generator,\n        eta=1.0,\n        image=init_image,\n        mask_image=mask_image,\n        control_image=control_image,\n    ).images[0]\n\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "google/ddpm-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN, obtaining state-of-the-art FID score of 3.17 and Inception score of 9.46.", "function_name": "google_ddpm_celebahq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def google_ddpm_celebahq_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-celebahq-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Denoising Diffusion Probabilistic Models (DDPM)", "api_name": "google/ddpm-ema-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "High quality image synthesis using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics.", "function_name": "google_ddpm_ema_celebahq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path where the output images will be saved", "default_value": ""}}, "function_code": "def google_ddpm_ema_celebahq_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-ema-celebahq-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "google/ddpm-ema-church-256", "python_environment_requirements": ["diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. DDPM models can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. The model can be used with different pipelines for faster inference and better trade-off between quality and speed.", "function_name": "google_ddpm_ema_church_256", "function_arguments": {"output_path": {"type": "str", "description": "The path where the output will be saved", "default_value": ""}}, "function_code": "def google_ddpm_ema_church_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-ema-church-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "CompVis/ldm-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.", "function_name": "CompVis_ldm_celebahq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path where the synthesized image will be saved", "default_value": ""}}, "function_code": "def CompVis_ldm_celebahq_256(output_path):\n    from diffusers import DiffusionPipeline\n    import os\n    sde_ve = DiffusionPipeline.from_pretrained(\"CompVis/ldm-celebahq-256\").to(\"cuda\")\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ddpm-church-256", "python_environment_requirements": ["diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis. Trained on the unconditional CIFAR10 dataset and 256x256 LSUN. Supports different noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference.", "function_name": "google_ddpm_church_256", "function_arguments": {"output_path": {"type": "str", "description": "The path where the synthesized images will be saved.", "default_value": ""}}, "function_code": "def google_ddpm_church_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-church-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ncsnpp-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.", "function_name": "google_ncsnpp_celebahq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def google_ncsnpp_celebahq_256(output_path):\n    from diffusers import DiffusionPipeline\n    import os\n    sde_ve = DiffusionPipeline.from_pretrained(\"google/ncsnpp-celebahq-256\").to(\"cuda\")\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "johnowhitaker/sd-class-wikiart-from-bedrooms", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model initialized from https://huggingface.co/google/ddpm-bedroom-256 and trained for 5000 steps on https://huggingface.co/datasets/huggan/wikiart.", "function_name": "johnowhitaker_sd_class_wikiart_from_bedrooms", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output file where the result will be saved.", "default_value": ""}}, "function_code": "def johnowhitaker_sd_class_wikiart_from_bedrooms(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('johnowhitaker/sd-class-wikiart-from-bedrooms').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "ddpm-cifar10-32", "python_environment_requirements": ["!pip install diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) for high quality image synthesis. Trained on the unconditional CIFAR10 dataset. Supports various discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.", "function_name": "ddpm_cifar10_32", "function_arguments": {"output_path": {"type": "str", "description": "The path to the directory where the output will be saved.", "default_value": ""}}, "function_code": "def ddpm_cifar10_32(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Denoising Diffusion Probabilistic Models (DDPM)", "api_name": "google/ddpm-ema-bedroom-256", "python_environment_requirements": ["diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics, capable of producing high-quality image synthesis results. The model can use discrete noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm for inference. It obtains an Inception score of 9.46 and a state-of-the-art FID score of 3.17 on the unconditional CIFAR10 dataset.", "function_name": "google_ddpm_ema_bedroom_256", "function_arguments": {"output_path": {"type": "str", "description": "The output path where the images will be saved", "default_value": ""}}, "function_code": "def google_ddpm_ema_bedroom_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-ema-bedroom-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Inference", "api_name": "google/ncsnpp-ffhq-1024", "python_environment_requirements": ["diffusers"], "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.", "function_name": "google_ncsnpp_ffhq_1024", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated images will be saved", "default_value": ""}}, "function_code": "def google_ncsnpp_ffhq_1024(output_path):\n    from diffusers import DiffusionPipeline\n    import os\n    sde_ve = DiffusionPipeline.from_pretrained(\"google/ncsnpp-ffhq-1024\").to(\"cuda\")\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "ocariz/universe_1400", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.", "function_name": "ocariz_universe_1400", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated images will be saved", "default_value": ""}}, "function_code": "def ocariz_universe_1400(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "WiNE-iNEFF/Minecraft-Skin-Diffusion-V2", "python_environment_requirements": ["diffusers"], "description": "An unconditional image generation model for generating Minecraft skin images using the diffusion model.", "function_name": "minecraft_skin_diffusion_v2", "function_arguments": {"output_path": {"type": "str", "description": "The output path where the generated Minecraft skin images will be saved.", "default_value": ""}}, "function_code": "def minecraft_skin_diffusion_v2(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion-V2').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "Minecraft-Skin-Diffusion", "python_environment_requirements": ["diffusers"], "description": "Unconditional Image Generation model for generating Minecraft skins using diffusion-based methods.", "function_name": "minecraft_skin_diffusion", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated Minecraft skin will be saved as an image file", "default_value": ""}}, "function_code": "def minecraft_skin_diffusion(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('WiNE-iNEFF/Minecraft-Skin-Diffusion').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "sd-class-butterflies-32", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of cute butterflies.", "function_name": "sd_class_butterflies_32", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated butterfly images will be saved.", "default_value": ""}}, "function_code": "def sd_class_butterflies_32(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('clp/sd-class-butterflies-32').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "MFawad/sd-class-butterflies-32", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b.", "function_name": "MFawad_sd_class_butterflies_32", "function_arguments": {"output_path": {"type": "str", "description": "The output path to save the generated images", "default_value": ""}}, "function_code": "def MFawad_sd_class_butterflies_32(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('MFawad/sd-class-butterflies-32').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ncsnpp-ffhq-256", "python_environment_requirements": ["diffusers"], "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. Achieves record-breaking performance on CIFAR-10 and demonstrates high fidelity generation of 1024 x 1024 images for the first time from a score-based generative model.", "function_name": "google_ncsnpp_ffhq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path where the output images will be saved.", "default_value": ""}}, "function_code": "def google_ncsnpp_ffhq_256(output_path):\n    from diffusers import DiffusionPipeline\n    import os\n    sde_ve = DiffusionPipeline.from_pretrained(\"google/ncsnpp-ffhq-256\").to(\"cuda\")\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Denoising Diffusion Probabilistic Models (DDPM)", "api_name": "google/ddpm-ema-cat-256", "python_environment_requirements": ["!pip install diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. It can generate high-quality images, and supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm. On the unconditional CIFAR10 dataset, it achieves an Inception score of 9.46 and a state-of-the-art FID score of 3.17.", "function_name": "google_ddpm_ema_cat_256", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the generated images will be saved", "default_value": ""}}, "function_code": "def google_ddpm_ema_cat_256(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('google/ddpm-ema-cat-256').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "ocariz/butterfly_200", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of cute butterflies trained for 200 epochs.", "function_name": "ocariz_butterfly_200", "function_arguments": {"output_path": {"type": "str", "description": "The path to save the generated butterflies", "default_value": ""}}, "function_code": "def ocariz_butterfly_200(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('ocariz/butterfly_200').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "ntrant7/sd-class-butterflies-32", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of cute butterflies.", "function_name": "ntrant7_sd_class_butterflies_32", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated images of cute butterflies will be saved.", "default_value": ""}}, "function_code": "def ntrant7_sd_class_butterflies_32(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('ntrant7/sd-class-butterflies-32').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "Apocalypse-19/shoe-generator", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of shoes trained on a custom dataset at 128x128 resolution.", "function_name": "apocalypse_19_shoe_generator", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def apocalypse_19_shoe_generator(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('Apocalypse-19/shoe-generator').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs", "python_environment_requirements": ["diffusers"], "description": "Example Fine-Tuned Model for Unit 2 of the Diffusion Models Class", "function_name": "pravsels_ddpm_ffhq_vintage_finetuned_vintage_3epochs", "function_arguments": {"output_path": {"type": "str", "description": "The path where the output will be saved.", "default_value": ""}}, "function_code": "def pravsels_ddpm_ffhq_vintage_finetuned_vintage_3epochs(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('pravsels/ddpm-ffhq-vintage-finetuned-vintage-3epochs').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/xclip-base-patch32", "python_environment_requirements": ["transformers"], "description": "X-CLIP is a minimal extension of CLIP for general video-language understanding. The model is trained in a contrastive way on (video, text) pairs. This allows the model to be used for tasks like zero-shot, few-shot or fully supervised video classification and video-text retrieval.", "function_name": "microsoft_xclip_base_patch32", "function_arguments": {"video_path": {"type": "str", "description": "The file path to the video that needs to be processed", "default_value": ""}, "labels": {"type": "list[str]", "description": "A list of labels for video classification and retrieval", "default_value": ""}}, "function_code": "def microsoft_xclip_base_patch32(video_path, labels):\n    import av\n    import torch\n    import numpy as np\n\n    from transformers import AutoProcessor, AutoModel\n    from huggingface_hub import hf_hub_download\n    from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n    def read_video_pyav(container, indices):\n        '''\n        Decode the video with PyAV decoder.\n        Args:\n            container (`av.container.input.InputContainer`): PyAV container.\n            indices (`List[int]`): List of frame indices to decode.\n        Returns:\n            result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n        '''\n        frames = []\n        container.seek(0)\n        start_index = indices[0]\n        end_index = indices[-1]\n        for i, frame in enumerate(container.decode(video=0)):\n            if i > end_index:\n                break\n            if i >= start_index and i in indices:\n                frames.append(frame)\n        return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        '''\n        Sample a given number of frame indices from the video.\n        Args:\n            clip_len (`int`): Total number of frames to sample.\n            frame_sample_rate (`int`): Sample every n-th frame.\n            seg_len (`int`): Maximum allowed index of sample's last frame.\n        Returns:\n            indices (`List[int]`): List of sampled frame indices\n        '''\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n    container = av.open(video_path)\n\n    # sample 8 frames\n    frame_sample_rate = container.streams.video[0].frames // 8\n    indices = sample_frame_indices(clip_len=8, frame_sample_rate=frame_sample_rate, seg_len=container.streams.video[0].frames)\n    video = read_video_pyav(container, indices)\n\n    processor = AutoProcessor.from_pretrained(\"microsoft/xclip-base-patch32\")\n    model = AutoModel.from_pretrained(\"microsoft/xclip-base-patch32\").to(\"cuda\")\n\n    inputs = processor(\n        text=labels,\n        videos=list(video),\n        return_tensors=\"pt\",\n        padding=True,\n    ).to(\"cuda\")\n\n    # forward pass\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits_per_video = outputs.logits_per_video  # this is the video-text similarity score\n    probs = logits_per_video.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "myunus1/diffmodels_galaxies_scratchbook", "python_environment_requirements": ["package", "import"], "description": "This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b.", "function_name": "myunus1_diffmodels_galaxies_scratchbook", "function_arguments": {"output_path": {"type": "str", "description": "The path to the directory where the generated images will be saved.", "default_value": ""}}, "function_code": "def myunus1_diffmodels_galaxies_scratchbook(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('myunus1/diffmodels_galaxies_scratchbook').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "utyug1/sd-class-butterflies-32", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of cute butterflies.", "function_name": "utyug1_sd_class_butterflies_32", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the generated images will be saved", "default_value": ""}}, "function_code": "def utyug1_sd_class_butterflies_32(output_path):\n    import torch\n    import os\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('utyug1/sd-class-butterflies-32').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "sd-class-pandas-32", "python_environment_requirements": ["package", "import"], "description": "This model is a diffusion model for unconditional image generation of cute \ud83e\udd8b.", "function_name": "sd_class_pandas_32", "function_arguments": {"output_path": {"type": "str", "description": "The file path where the generated images will be saved.", "default_value": ""}}, "function_code": "def sd_class_pandas_32():\n    import torch\n    from diffusers import DDPMPipeline\n\n    pipeline = DDPMPipeline.from_pretrained('schdoel/sd-class-AFHQ-32')\n    image = pipeline().images[0]\n    return image\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-base-finetuned-k400", "python_environment_requirements": ["transformers"], "description": "TimeSformer is a video classification model pre-trained on Kinetics-400. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 400 possible Kinetics-400 labels.", "function_name": "facebook_timesformer_base_finetuned_k400", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file to be classified.", "default_value": ""}}, "function_code": "def facebook_timesformer_base_finetuned_k400(video):\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n\n    video = list(np.random.randn(8, 3, 224, 224))\n    processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n    model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n    inputs = processor(video, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-base", "python_environment_requirements": ["transformers"], "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches.", "function_name": "MCG_NJU_videomae_base", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that will be used as input for the MCG_NJU_videomae_base function.", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_base(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    # video = list(np.random.randn(num_frames, 3, 224, 224))\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base-ssv2\")\n    model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base-ssv2\")\n\n    pixel_values = feature_extractor(list(frames), return_tensors=\"pt\").pixel_values\n\n    # feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base-short-ssv2\")\n    # model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base-short-ssv2\")\n    #\n    # pixel_values = feature_extractor(video, return_tensors=\"pt\").pixel_values\n\n    num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n    seq_length = (16 // model.config.tubelet_size) * num_patches_per_frame\n\n    bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n\n    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n    loss = outputs.loss\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-base-finetuned-k600", "python_environment_requirements": ["transformers"], "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.", "function_name": "facebook_timesformer_base_finetuned_k600", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file.", "default_value": ""}}, "function_code": "def facebook_timesformer_base_finetuned_k600(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((448, 448)),\n        CenterCrop((448, 448)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-k600')\n    model = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-k600').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-base-finetuned-kinetics", "python_environment_requirements": ["transformers"], "description": "VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.", "function_name": "MCG_NJU_videomae_base_finetuned_kinetics", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file to be processed.", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_base_finetuned_kinetics(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-finetuned-kinetics')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-kinetics').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-hr-finetuned-k400", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al.", "function_name": "facebook_timesformer_hr_finetuned_k400", "function_arguments": {"video_path": {"type": "str", "description": "Path to the video file for classification", "default_value": ""}}, "function_code": "def facebook_timesformer_hr_finetuned_k400(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((448, 448)),\n        CenterCrop((448, 448)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-k400')\n    model = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-k400').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-base-finetuned-ssv2", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.", "function_name": "facebook_timesformer_base_finetuned_ssv2", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file.", "default_value": ""}}, "function_code": "def facebook_timesformer_base_finetuned_ssv2(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((448, 448)),\n        CenterCrop((448, 448)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = AutoImageProcessor.from_pretrained('facebook/timesformer-base-finetuned-ssv2')\n    model = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-base-finetuned-ssv2').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-hr-finetuned-ssv2", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "TimeSformer model pre-trained on Something Something v2. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository.", "function_name": "facebook_timesformer_hr_finetuned_ssv2", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file.", "default_value": ""}}, "function_code": "def facebook_timesformer_hr_finetuned_ssv2(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((448, 448)),\n        CenterCrop((448, 448)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = AutoImageProcessor.from_pretrained('facebook/timesformer-hr-finetuned-ssv2')\n    model = TimesformerForVideoClassification.from_pretrained('facebook/timesformer-hr-finetuned-ssv2').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "videomae-large", "python_environment_requirements": ["transformers"], "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.", "function_name": "videomae_large", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that will be processed by the VideoMAE model.", "default_value": ""}}, "function_code": "def videomae_large(video_path):\n    from transformers import VideoMAEFeatureExtractor, VideoMAEForPreTraining\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    # video = list(np.random.randn(num_frames, 3, 224, 224))\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-large\")\n    model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-large\").to('cuda')\n\n    pixel_values = feature_extractor(list(frames), return_tensors=\"pt\").pixel_values.to('cuda')\n\n    # feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-base-short-ssv2\")\n    # model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base-short-ssv2\")\n    #\n    # pixel_values = feature_extractor(video, return_tensors=\"pt\").pixel_values\n\n    num_patches_per_frame = (model.config.image_size // model.config.patch_size) ** 2\n    seq_length = (16 // model.config.tubelet_size) * num_patches_per_frame\n\n    bool_masked_pos = torch.randint(0, 2, (1, seq_length)).bool()\n\n    outputs = model(pixel_values, bool_masked_pos=bool_masked_pos)\n    loss = outputs.loss\n\n    return loss\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-base-finetuned-ssv2", "python_environment_requirements": ["transformers"], "description": "VideoMAE model pre-trained for 2400 epochs in a self-supervised way and fine-tuned in a supervised way on Something-Something-v2. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.", "function_name": "MCG_NJU_videomae_base_finetuned_ssv2", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file.", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_base_finetuned_ssv2(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-finetuned-ssv2').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-base-short", "python_environment_requirements": ["packages"], "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks.", "function_name": "MCG_NJU_videomae_base_short", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file.", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_base_short(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-large-finetuned-kinetics", "python_environment_requirements": ["transformers"], "description": "VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.", "function_name": "MCG_NJU_videomae_large_finetuned_kinetics", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that the function will process.", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_large_finetuned_kinetics(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-large-finetuned-kinetics').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "MCG-NJU/videomae-base-short-finetuned-kinetics", "python_environment_requirements": ["transformers"], "description": "VideoMAE model pre-trained for 800 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.", "function_name": "MCG_NJU_videomae_base_short_finetuned_kinetics", "function_arguments": {"video_path": {"type": "str", "description": "The file path to the video", "default_value": ""}}, "function_code": "def MCG_NJU_videomae_base_short_finetuned_kinetics(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-base-short-finetuned-kinetics').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "videomae-base-finetuned-RealLifeViolenceSituations-subset", "python_environment_requirements": ["transformers", "pytorch", "datasets", "tokenizers"], "description": "This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is trained for video classification task, specifically for RealLifeViolenceSituations.", "function_name": "videomae_base_finetuned_RealLifeViolenceSituations_subset", "function_arguments": {"video_path": {"type": "str", "description": "The file path to the video file that needs to be classified", "default_value": ""}}, "function_code": "def videomae_base_finetuned_RealLifeViolenceSituations_subset(video_path):\n    from torchvision.transforms import Resize, CenterCrop, ToTensor, Normalize, Compose\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('videomae/base-finetuned-RealLifeViolenceSituations-subset')\n    model = VideoMAEForVideoClassification.from_pretrained('videomae/base-finetuned-RealLifeViolenceSituations-subset').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": false}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "fcakyon/timesformer-large-finetuned-k400", "python_environment_requirements": ["transformers"], "description": "TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.", "function_name": "fcakyon_timesformer_large_finetuned_k400", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that needs to be classified", "default_value": ""}}, "function_code": "def fcakyon_timesformer_large_finetuned_k400(video_path):\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    import torch\n\n    video = list(np.random.randn(96, 3, 224, 224))\n    processor = AutoImageProcessor.from_pretrained(\"fcakyon/timesformer-large-finetuned-k400\")\n    model = TimesformerForVideoClassification.from_pretrained(\"fcakyon/timesformer-large-finetuned-k400\")\n    inputs = processor(video, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n", "executable": false}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "fcakyon/timesformer-hr-finetuned-k400", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "TimeSformer model pre-trained on Kinetics-400 for video classification into one of the 400 possible Kinetics-400 labels. Introduced in the paper 'TimeSformer: Is Space-Time Attention All You Need for Video Understanding?' by Tong et al.", "function_name": "fcakyon_timesformer_hr_finetuned_k400", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that needs to be classified", "default_value": ""}}, "function_code": "def fcakyon_timesformer_hr_finetuned_k400(video_path):\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    video = list(np.ones((16, 3, 224, 224)))\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((448, 448)),\n        CenterCrop((448, 448)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('fcakyon/timesformer-hr-finetuned-k400')\n    model = VideoMAEForVideoClassification.from_pretrained('fcakyon/timesformer-hr-finetuned-k400').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": false}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "videomae-small-finetuned-kinetics", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "VideoMAE model pre-trained for 1600 epochs in a self-supervised way and fine-tuned in a supervised way on Kinetics-400. It was introduced in the paper VideoMAE: Masked Autoencoders are Data-Efficient Learners for Self-Supervised Video Pre-Training by Tong et al. and first released in this repository.", "function_name": "videomae_small_finetuned_kinetics", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that needs to be processed", "default_value": ""}}, "function_code": "def videomae_small_finetuned_kinetics(video_path):\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    video = list(np.ones((16, 3, 224, 224)))\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics')\n    model = VideoMAEForVideoClassification.from_pretrained('MCG-NJU/videomae-small-finetuned-kinetics').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "videomae-base-finetuned-ucf101-subset", "python_environment_requirements": ["transformers==4.25.1, torch==1.10.0, datasets==2.7.1, tokenizers==0.12.1"], "description": "This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It is used for video classification tasks.", "function_name": "videomae_base_finetuned_ucf101_subset", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that needs to be classified", "default_value": ""}}, "function_code": "def videomae_base_finetuned_ucf101_subset(video_path: str):\n    from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n    import numpy as np\n    import torch\n    from decord import VideoReader, cpu\n\n    video = list(np.ones((16, 3, 224, 224)))\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    video = list(video.transpose(0, 3, 1, 2))\n\n    preprocessing_pipeline = Compose([\n        Resize((224, 224)),\n        CenterCrop((224, 224)),\n    ])\n\n    frames = [preprocessing_pipeline(torch.tensor(frame, dtype=torch.float16)) for frame in video]\n    frames = np.stack([frame / 255 for frame in frames])\n\n    # sample 16 frames from the video\n    frames = frames[np.linspace(0, len(frames) - 1, 16, dtype=int)]\n\n    processor = VideoMAEImageProcessor.from_pretrained('zahrav/videomae-base-finetuned-ucf101-subset')\n    model = VideoMAEForVideoClassification.from_pretrained('zahrav/videomae-base-finetuned-ucf101-subset').to('cuda')\n    inputs = processor(list(frames), return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "sayakpaul/videomae-base-finetuned-ucf101-subset", "python_environment_requirements": ["Transformers", "Pytorch", "Datasets", "Tokenizers"], "description": "This model is a fine-tuned version of MCG-NJU/videomae-base on an unknown dataset. It achieves the following results on the evaluation set: Loss: 0.3992, Accuracy: 0.8645.", "function_name": "sayakpaul_videomae_base_finetuned_ucf101_subset", "function_arguments": {"video_file_path": {"type": "str", "description": "The file path of the video to be processed.", "default_value": ""}}, "function_code": "def sayakpaul_videomae_base_finetuned_ucf101_subset(video_file_path: str):\n    \"\"\"\n       Classifies the input video using the specified model.\n\n       :param video_file_path: str, path to the input video file.\n       :param model_name: str, name of the model to be used for classification. Default is \"sayakpaul/videomae-base-finetuned-ucf101-subset\".\n       :return: str, name of the predicted class.\n       \"\"\"\n    import torch\n    import imageio\n    from torchvision.transforms import Compose, Lambda\n    from pytorchvideo.transforms import Normalize, ShortSideScale, UniformTemporalSubsample\n    from transformers import VideoMAEForVideoClassification, VideoMAEImageProcessor\n\n    # Load the fine-tuned model\n    model = VideoMAEForVideoClassification.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subse')\n\n    # Load the processor\n    image_processor = VideoMAEImageProcessor.from_pretrained('sayakpaul/videomae-base-finetuned-ucf101-subse')\n\n    # Load video\n    video_reader = imageio.get_reader(video_file_path, \"ffmpeg\")\n    video_frames = [frame for frame in video_reader]\n    video_tensor = torch.tensor(video_frames).permute(3, 0, 1, 2)  # Change to (C, T, H, W)\n\n    # Preprocess the video\n    mean = image_processor.image_mean\n    std = image_processor.image_std\n    resize_to = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n\n    transform = Compose([\n        UniformTemporalSubsample(model.config.num_frames),\n        Lambda(lambda x: x / 255.0),\n        Normalize(mean, std),\n        ShortSideScale(resize_to),\n    ])\n\n    preprocessed_video = transform(video_tensor)\n\n    # Make predictions\n    with torch.no_grad():\n        inputs = {\"pixel_values\": preprocessed_video.unsqueeze(0)}\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class_idx = logits.argmax(-1).item()\n\n    # Map the predicted index to the corresponding label\n    predicted_class_label = model.config.id2label[predicted_class_idx]\n\n    return predicted_class_label\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "openai/clip-vit-base-patch32", "python_environment_requirements": ["PIL", "requests", "transformers"], "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.", "function_name": "openai_clip_vit_base_patch32", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image", "default_value": ""}, "labels": {"type": "str", "description": "The labels for image classification tasks", "default_value": ""}}, "function_code": "def openai_clip_vit_base_patch32(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import CLIPProcessor, CLIPModel\n\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "openai/clip-vit-large-patch14", "python_environment_requirements": ["packages"], "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.", "function_name": "openai_clip_vit_large_patch14", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "labels": {"type": "str", "description": "The type of the argument is a string. It is likely that the argument 'labels' is used to pass a list of labels to the function. The function may use these labels to classify images or perform some other task related to image classification.", "default_value": ""}}, "function_code": "def openai_clip_vit_large_patch14(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import CLIPProcessor, CLIPModel\n\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(\"cuda\")\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-large-patch14\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-ViT-bigG-14-laion2B-39B-b160k", "python_environment_requirements": ["transformers"], "description": "A CLIP ViT-bigG/14 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. The model is intended for research purposes and enables researchers to better understand and explore zero-shot, arbitrary image classification. It can be used for interdisciplinary studies of the potential impact of such models. The model achieves a 80.1 zero-shot top-1 accuracy on ImageNet-1k.", "function_name": "laion_clip_vit_bigG_14_laion2B_39B_b160k", "function_arguments": {"image": {"type": "image", "description": "The image to be classified", "default_value": ""}, "possible_class_names": {"type": "list", "description": "A list of possible class names that the model can classify an image into.", "default_value": ""}}, "function_code": "def laion_clip_vit_bigG_14_laion2B_39B_b160k(image, possible_class_names):\n    from transformers import pipeline\n\n    classifier = pipeline('image-classification', model='laion/CLIP-ViT-bigG-14-laion2B-39B-b160k')\n    result = classifier(image, possible_class_names=possible_class_names)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "openai/clip-vit-base-patch16", "python_environment_requirements": ["PIL", "requests", "transformers"], "description": "The CLIP model was developed by researchers at OpenAI to learn about what contributes to robustness in computer vision tasks. The model was also developed to test the ability of models to generalize to arbitrary image classification tasks in a zero-shot manner.", "function_name": "openai_clip_vit_base_patch16", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed by the CLIP model.", "default_value": ""}, "labels": {"type": "str", "description": "The labels for the image classification tasks", "default_value": ""}}, "function_code": "def openai_clip_vit_base_patch16(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import CLIPProcessor, CLIPModel\n\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch16\").to(\"cuda\")\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch16\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-ViT-B-16-laion2B-s34B-b88K", "python_environment_requirements": ["transformers"], "description": "A CLIP ViT-B/16 model trained with the LAION-2B English subset of LAION-5B using OpenCLIP. This model is intended for research purposes and can be used for zero-shot image classification, image and text retrieval, and other related tasks.", "function_name": "laion_clip_vit_b_16_laion2b_s34b_b88k", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be processed", "default_value": ""}, "labels": {"type": "str", "description": "A list of labels that correspond to the image or text input", "default_value": ""}}, "function_code": "def laion_clip_vit_b_16_laion2b_s34b_b88k(image_path, labels):\n    from transformers import pipeline\n\n    classify = pipeline('image-classification', model='laion/CLIP-ViT-B-16-laion2B-s34B-b88K')\n    result = classify(image_path, labels)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "patrickjohncyh/fashion-clip", "python_environment_requirements": ["transformers"], "description": "FashionCLIP is a CLIP-based model developed to produce general product representations for fashion concepts. Leveraging the pre-trained checkpoint (ViT-B/32) released by OpenAI, it is trained on a large, high-quality novel fashion dataset to study whether domain specific fine-tuning of CLIP-like models is sufficient to produce product representations that are zero-shot transferable to entirely new datasets and tasks.", "function_name": "patrickjohncyh_fashion_clip", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "labels": {"type": "str", "description": "The labels associated with the fashion concepts", "default_value": ""}}, "function_code": "def patrickjohncyh_fashion_clip(image_path, labels):\n    from transformers import CLIPProcessor, CLIPModel\n    import PIL\n\n    model = CLIPModel.from_pretrained('patrickjohncyh/fashion-clip').to('cuda')\n    processor = CLIPProcessor.from_pretrained('patrickjohncyh/fashion-clip')\n\n    image = PIL.Image.open(image_path)\n\n    inputs = processor(text=labels, images=[image], return_tensors='pt', padding=True).to('cuda')\n    logits_per_image = model(**inputs).logits_per_image\n    probs = logits_per_image.softmax(dim=-1).tolist()[0]\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Large model (convnext_large) as the image tower, a MLP (fc - gelu - drop - fc) head in vision tower instead of the single projection of other CLIP models, and a text tower with same width but 4 layers more depth than ViT-L / RN50x16 models (depth 16, embed dim 768).", "function_name": "laion_clip_convnext_large_d_320_laion2B_s29B_b131K_ft_soup", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image file to be processed by the function.", "default_value": ""}, "labels": {"type": "str", "description": "The labels for classification tasks", "default_value": ""}}, "function_code": "def laion_clip_convnext_large_d_320_laion2B_s29B_b131K_ft_soup(image_path, labels):\n    from transformers import CLIPProcessor, CLIPModel\n\n    processor = CLIPProcessor.from_pretrained(\"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\")\n    model = CLIPModel.from_pretrained(\"laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft-soup\")\n\n    inputs = processor(text=labels, images=image_path, return_tensors=\"pt\", padding=True)\n    outputs = model(**inputs)\n\n    return outputs\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. These models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. They can be used for zero-shot image classification, image and text retrieval, and other tasks.", "function_name": "laion_clip_convnext_base_w_laion_aesthetic_s13B_b82K", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "labels": {"type": "str", "description": "The labels of the images for zero-shot image classification.", "default_value": ""}}, "function_code": "def laion_clip_convnext_base_w_laion_aesthetic_s13B_b82K(image_path, labels):\n    from transformers import pipeline\n\n    model = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion_aesthetic-s13B-b82K')\n    result = model(image_path, labels)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup", "python_environment_requirements": ["transformers", "torch"], "description": "A series of CLIP ConvNeXt-XXLarge models trained on LAION-2B (English), a subset of LAION-5B, using OpenCLIP. These models achieve between 79.1 and 79.4 top-1 zero-shot accuracy on ImageNet-1k.", "function_name": "laion_clip_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be processed.", "default_value": ""}, "labels": {"type": "str", "description": "The labels for the CLIP ConvNeXt-XXLarge models.", "default_value": ""}}, "function_code": "def laion_clip_convnext_xxlarge_laion2B_s34B_b82K_augreg_soup(image_path, labels):\n    from transformers import CLIPProcessor, CLIPModel\n    import PIL\n\n    # Load the CLIP model\n    model_name = \"laion/CLIP-convnext_xxlarge-laion2B-s34B-b82K-augreg-soup\"\n    model = CLIPModel.from_pretrained(model_name)\n\n    # Load the CLIP processor\n    processor = CLIPProcessor.from_pretrained(model_name)\n\n    image = PIL.Image.open(image_path)\n    # Preprocess the image\n    inputs = processor(images=image, return_tensors=\"pt\")\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    # Get the predicted class probabilities\n    logits = outputs.logits\n    probabilities = logits.softmax(dim=-1)\n\n    # Get the predicted class labels\n    if labels is not None:\n        predicted_labels = [labels[i] for i in probabilities.argmax(dim=-1).tolist()]\n    else:\n        predicted_labels = probabilities.argmax(dim=-1).tolist()\n\n    return predicted_labels\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models utilize the timm ConvNeXt-Base model (convnext_base) as the image tower, and the same text tower as the RN50x4 (depth 12, embed dim 640) model from OpenAI CLIP.", "function_name": "laion_clip_image_classification", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be classified.", "default_value": ""}, "class_names": {"type": "List[str]", "description": "A list of class names corresponding to the image categories for classification.", "default_value": ""}}, "function_code": "def laion_clip_image_classification(image_path, class_names):\n    from transformers import pipeline\n\n    image_classification = pipeline('image-classification',\n                                    model='laion/CLIP-convnext_base_w_320-laion_aesthetic-s13B-b82K-augreg')\n    result = image_classification(image_path, class_names)\n    return result\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "flax-community/clip-rsicd-v2", "python_environment_requirements": ["PIL", "requests", "transformers"], "description": "This model is a fine-tuned CLIP by OpenAI. It is designed with an aim to improve zero-shot image classification, text-to-image and image-to-image retrieval specifically on remote sensing images.", "function_name": "clip_rsicd_v2", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "labels": {"type": "str", "description": "Labels used for image classification and retrieval", "default_value": ""}}, "function_code": "def clip_rsicd_v2(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import CLIPProcessor, CLIPModel\n\n    model = CLIPModel.from_pretrained(\"flax-community/clip-rsicd-v2\")\n    processor = CLIPProcessor.from_pretrained(\"flax-community/clip-rsicd-v2\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=[f\"a photo of a {l}\" for l in labels], images=image, return_tensors=\"pt\", padding=True)\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "tiny-random-CLIPSegModel", "python_environment_requirements": ["transformers"], "description": "A tiny random CLIPSegModel for zero-shot image classification.", "function_name": "tiny_random_CLIPSegModel", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL of the image to be classified.", "default_value": ""}, "labels": {"type": "str", "description": "The labels for zero-shot image classification.", "default_value": ""}}, "function_code": "def tiny_random_CLIPSegModel(image_path, labels):\n\n    from transformers import  AutoProcessor, CLIPSegModel\n    import PIL\n\n    # Load the model and tokenizer\n    model = CLIPSegModel.from_pretrained('hf-tiny-model-private/tiny-random-CLIPSegModel').to(\"cuda\")\n    processor = AutoProcessor.from_pretrained('hf-tiny-model-private/tiny-random-CLIPSegModel')\n\n    image = PIL.Image.open(image_path)\n\n    # Tokenize the texts\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k", "python_environment_requirements": ["huggingface_hub, openai, transformers"], "description": "This model is a zero-shot image classification model based on OpenCLIP. It can be used for classifying images into various categories without any additional training.", "function_name": "timm_eva02_enormous_patch14_plus_clip_224_laion2b_s9b_b144k", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be classified", "default_value": ""}, "labels": {"type": "str", "description": "The labels for the image categories", "default_value": ""}}, "function_code": "def timm_eva02_enormous_patch14_plus_clip_224_laion2b_s9b_b144k(image_path, labels):\n\n    from transformers import AutoProcessor, CLIPModel\n    import PIL\n\n    # Load the model and tokenizer\n    model = CLIPModel.from_pretrained('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k').to(\"cuda\")\n    processor = AutoProcessor.from_pretrained('timm/eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k')\n\n    image = PIL.Image.open(image_path)\n\n    # Tokenize the texts\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Large (w/ extra text depth, vision MLP head) models trained on LAION-2B (english), a subset of LAION-5B, using OpenCLIP. The models are trained at 256x256 image resolution and achieve a 75.9 top-1 zero-shot accuracy on ImageNet-1k.", "function_name": "laion_clip_convnext_large_d_laion2B_s26B_b102K_augreg", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file to be processed.", "default_value": ""}, "classes": {"type": "str", "description": "The classes argument specifies the list of classes or labels for which the model has been trained on. It is used to classify the inputs into one of these classes.", "default_value": ""}}, "function_code": "def laion_clip_convnext_large_d_laion2B_s26B_b102K_augreg(image_path, classes):\n    from transformers import pipeline\n\n    clip = pipeline('image-classification', model='laion/CLIP-convnext_large_d.laion2B-s26B-b102K-augreg')\n    clip(image_path, classes)\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Large models trained on the LAION-2B (english) subset of LAION-5B using OpenCLIP. The models achieve between 75.9 and 76.9 top-1 zero-shot accuracy on ImageNet-1k.", "function_name": "laion_clip_convnext_large_d_320_laion2B_s29B_b131K_ft", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "labels": {"type": "str", "description": "The labels for the images.", "default_value": ""}}, "function_code": "def laion_clip_convnext_large_d_320_laion2B_s29B_b131K_ft(image_path, labels):\n    from transformers import AutoProcessor, CLIPModel\n    import PIL\n\n    # Load the model and tokenizer\n    model = CLIPModel.from_pretrained('laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft').to(\"cuda\")\n    processor = AutoProcessor.from_pretrained('laion/CLIP-convnext_large_d_320.laion2B-s29B-b131K-ft')\n\n    image = PIL.Image.open(image_path)\n\n    # Tokenize the texts\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": false}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "OFA-Sys/chinese-clip-vit-base-patch16", "python_environment_requirements": ["transformers"], "description": "Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It uses ViT-B/16 as the image encoder and RoBERTa-wwm-base as the text encoder.", "function_name": "OFA_Sys_chinese_clip_vit_base_patch16", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "labels": {"type": "str", "description": "The labels for the Chinese image-text pairs", "default_value": ""}}, "function_code": "def OFA_Sys_chinese_clip_vit_base_patch16(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n\n    model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\").to(\"cuda\")\n    processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True).to(\"cuda\")\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "clip-vit-base-patch32-ko", "python_environment_requirements": ["transformers", "torch", "PIL", "requests"], "description": "Korean CLIP model trained by Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. It is a zero-shot image classification model that can be used to classify images without any training data.", "function_name": "clip_vit_base_patch32_ko", "function_arguments": {"image_path": {"type": "str", "description": "The file path or URL of the image to classify", "default_value": ""}, "labels": {"type": "str", "description": "A list of labels for the images to be classified", "default_value": ""}}, "function_code": "def clip_vit_base_patch32_ko(image_path, labels):\n    import requests\n    import torch\n    from PIL import Image\n    from transformers import AutoModel, AutoProcessor\n\n    repo = \"Bingsu/clip-vit-base-patch32-ko\"\n    model = AutoModel.from_pretrained(repo).to(\"cuda\")\n    processor = AutoProcessor.from_pretrained(repo)\n\n    image = Image.open(image_path)\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n    with torch.inference_mode():\n        outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n    return probs\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face Transformers", "functionality": "Zero-Shot Image Classification", "api_name": "chinese-clip-vit-large-patch14", "python_environment_requirements": ["libraries"], "description": "Chinese-CLIP-ViT-Large-Patch14 is a large version of the Chinese CLIP model, with ViT-L/14 as the image encoder and RoBERTa-wwm-base as the text encoder. Chinese CLIP is a simple implementation of CLIP on a large-scale dataset of around 200 million Chinese image-text pairs. It is designed for zero-shot image classification tasks.", "function_name": "chinese_clip_vit_large_patch14", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for classification.", "default_value": ""}, "labels": {"type": "str", "description": "The labels associated with the images", "default_value": ""}}, "function_code": "def chinese_clip_vit_large_patch14(image_path, labels):\n    from PIL import Image\n    import requests\n    from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n\n    model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch14\").to(\"cuda\")\n    processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-large-patch14\")\n\n    image = Image.open(image_path)\n\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)  # probs: [[0.0066, 0.0211, 0.0031, 0.9692]]\n\n    return probs\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "clipseg-rd64-refined", "python_environment_requirements": ["transformers"], "description": "CLIPSeg model with reduce dimension 64, refined (using a more complex convolution). It was introduced in the paper Image Segmentation Using Text and Image Prompts by L\u00fcddecke et al. and first released in this repository. This model is intended for zero-shot and one-shot image segmentation.", "function_name": "clipseg_rd64_refined", "function_arguments": {"labels": {"type": "str", "description": "The labels for image segmentation", "default_value": ""}, "image_path": {"type": "str", "description": "The path to the image file that needs to be segmented.", "default_value": ""}}, "function_code": "def clipseg_rd64_refined(labels, image_path):\n    from transformers import  AutoProcessor, CLIPSegModel\n    import PIL\n\n    # Load the model and tokenizer\n    model = CLIPSegModel.from_pretrained('CIDAS/clipseg-rd64-refined').to(\"cuda\")\n    processor = AutoProcessor.from_pretrained('CIDAS/clipseg-rd64-refined')\n\n    image = PIL.Image.open(image_path)\n\n    # Tokenize the texts\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(\"cuda\")\n\n    # Forward pass through the model\n    outputs = model(**inputs)\n\n    logits_per_image = outputs.logits_per_image\n    probs = logits_per_image.softmax(dim=1)\n\n    return probs\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "microsoft/git-base", "python_environment_requirements": ["transformers"], "description": "GIT (short for GenerativeImage2Text) model, base-sized version. It was introduced in the paper GIT: A Generative Image-to-text Transformer for Vision and Language by Wang et al. and first released in this repository. The model is trained using 'teacher forcing' on a lot of (image, text) pairs. The goal for the model is simply to predict the next text token, giving the image tokens and previous text tokens. This allows the model to be used for tasks like image and video captioning, visual question answering (VQA) on images and videos, and even image classification (by simply conditioning the model on the image and asking it to generate a class for it in text).", "function_name": "git_base", "function_arguments": {"image": {"type": "str", "description": "The image tokens.", "default_value": ""}}, "function_code": "def git_base(image):\n    from transformers import AutoModelForCausalLM, AutoTokenizer\n\n    # Load the model and tokenizer\n    model_name = \"microsoft/git-base\"\n    model = AutoModelForCausalLM.from_pretrained(model_name)\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n\n    # Perform the desired operations using the model and tokenizer\n    # ...\n\n    # Return the result\n    return result\n", "executable": false}
{"domain": "Computer Vision Object Detection", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "microsoft/table-transformer-structure-recognition", "python_environment_requirements": ["transformers"], "description": "Table Transformer (DETR) model trained on PubTables1M for detecting the structure (like rows, columns) in tables.", "function_name": "microsoft_table_transformer_structure_recognition", "function_arguments": {"api_key": {"type": "str", "description": "The API key for accessing the Table Transformer (DETR) model", "default_value": ""}, "table_image_path": {"type": "str", "description": "The path to the image that contains the table.", "default_value": ""}}, "function_code": "def microsoft_table_transformer_structure_recognition(api_key, table_image_path):\n    import requests\n    from PIL import Image\n    import io\n\n    # Read the image file\n    with open(table_image_path, 'rb') as image_file:\n        image_data = image_file.read()\n\n    # Send the image data to the API\n    response = requests.post(\n        url=\"https://api.cognitive.microsoft.com/vision/v3.2/read/analyze\",\n        headers={\n            \"Content-Type\": \"application/octet-stream\",\n            \"Ocp-Apim-Subscription-Key\": api_key\n        },\n        data=image_data\n    )\n\n    # Get the response JSON\n    response_json = response.json()\n\n    # Extract the table structure from the response\n    table_structure = response_json[\"analyzeResult\"][\"readResults\"][0][\"tables\"][0][\"rows\"]\n\n    return table_structure\n", "executable": false}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "CompVis/ldm-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "Latent Diffusion Models (LDMs) achieve state-of-the-art synthesis results on image data and beyond by decomposing the image formation process into a sequential application of denoising autoencoders. LDMs enable high-resolution synthesis, semantic scene synthesis, super-resolution, and image inpainting while significantly reducing computational requirements compared to pixel-based DMs.", "function_name": "CompVis_ldm_celebahq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory where the synthesized images will be saved.", "default_value": ""}, "num_inference_steps": {"type": "int", "description": "The number of inference steps to be performed in the Latent Diffusion Model (LDM)", "default_value": "200"}}, "function_code": "def CompVis_ldm_celebahq_256(output_path, num_inference_steps=200):\n    import torch\n    from PIL import Image\n    from diffusers import DiffusionPipeline\n    import os\n\n    model_id = \"CompVis/ldm-celebahq-256\"\n    pipeline = DiffusionPipeline.from_pretrained(model_id)\n    image = pipeline(num_inference_steps=num_inference_steps)[0]\n    image[0].save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "ocariz/universe_1400", "python_environment_requirements": ["diffusers"], "description": "This model is a diffusion model for unconditional image generation of the universe trained for 1400 epochs.", "function_name": "ocariz_universe_1400", "function_arguments": {"output_path": {"type": "str", "description": "The path where the generated universe images will be saved", "default_value": ""}}, "function_code": "def ocariz_universe_1400(output_path):\n    from diffusers import DDPMPipeline\n    import os\n\n    pipeline = DDPMPipeline.from_pretrained('ocariz/universe_1400').to('cuda')\n    image = pipeline().images[0]\n    image.save(output_path)\n\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Diffusers", "api_name": "google/ddpm-celebahq-256", "python_environment_requirements": ["diffusers"], "description": "Leveraging Denoising Diffusion Probabilistic Models (DDPM) for high-quality image synthesis, our model is trained on the unconditional CIFAR10 dataset and 256x256 LSUN, achieving outstanding results with a state-of-the-art FID score of 3.17 and an Inception score of 9.46. Remarkably, our model harnesses this technique to generate striking celebrity images.", "function_name": "google_ddpm_celebahq_256", "function_arguments": {"api_key": {"type": "str", "description": "The API key used for accessing the DDPM model", "default_value": ""}, "output_path": {"type": "str", "description": "The file path where the generated images will be saved", "default_value": ""}}, "function_code": "def google_ddpm_celebahq_256(api_key, output_path):\n    import diffusers\n    from diffusers import DDPMPipeline\n    import os\n\n    model_id = \"google/ddpm-celebahq-256\"\n    sample = 0\n    ddpm = DDPMPipeline.from_pretrained(model_id, api_key=api_key)\n    image = ddpm()[sample]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": false}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ncsnpp-ffhq-256", "python_environment_requirements": ["diffusers"], "description": "Utilizing Score-Based Generative Modeling via Stochastic Differential Equations (SDE) for unconditional image generation, our model attains unprecedented performance on CIFAR-10. It also marks a significant milestone by producing high-fidelity 1024 x 1024 images, a first for score-based generative models. Our model effectively employs this technique to generate human facial images.", "function_name": "google_ncsnpp_ffhq_256", "function_arguments": {"output_path": {"type": "str", "description": "The path to save the generated images", "default_value": ""}}, "function_code": "def google_ncsnpp_ffhq_256(output_path):\n    import os\n    from diffusers import DiffusionPipeline\n    model_id = \"google/ncsnpp-ffhq-256\"\n    sde_ve = DiffusionPipeline.from_pretrained(model_id)\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "martinezomg/vit-base-patch16-224-diabetic-retinopathy", "python_environment_requirements": ["transformers", "pytorch", "datasets", "tokenizers"], "description": "This model is a fine-tuned version of google/vit-base-patch16-224 on the None dataset. It is designed for image classification tasks, specifically for diabetic retinopathy detection.", "function_name": "martinezomg_vit_base_patch16_224_diabetic_retinopathy", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that needs to be classified", "default_value": ""}}, "function_code": "def martinezomg_vit_base_patch16_224_diabetic_retinopathy(image_path):\n    from transformers import pipeline\n\n    image_classifier = pipeline('image-classification', 'martinezomg/vit-base-patch16-224-diabetic-retinopathy')\n    result = image_classifier(image_path)\n\n    return result\n", "executable": false}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "fxmarty/resnet-tiny-beans", "python_environment_requirements": ["transformers"], "description": "A model trained on the beans dataset, just for testing and having a really tiny model.", "function_name": "fxmarty_resnet_tiny_beans", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}}, "function_code": "def fxmarty_resnet_tiny_beans(image_path):\n    from transformers import pipeline\n\n    classifier = pipeline('image-classification', model='fxmarty/resnet-tiny-beans')\n    results = classifier(image_path)\n\n    return results\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "nvidia/mit-b0", "python_environment_requirements": ["transformers", "PIL", "requests"], "description": "The SegFormer encoder, fine-tuned on Imagenet-1k, is a key component introduced in the paper \"SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers\" by Xie et al. Initially unveiled in this repository, SegFormer comprises a hierarchical Transformer encoder paired with a lightweight all-MLP decode head. This architecture yields impressive results on semantic segmentation benchmarks, notably ADE20K and Cityscapes. Furthermore, it exhibits remarkable performance in classifying cityscape images.", "function_name": "nvidia_mit_b0", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file for processing", "default_value": ""}}, "function_code": "def nvidia_mit_b0(image_path):\n    from transformers import SegformerFeatureExtractor, SegformerForImageClassification\n    from PIL import Image\n    import requests\n\n    image = Image.open(image_path)\n    feature_extractor = SegformerFeatureExtractor.from_pretrained('nvidia/mit-b0')\n    model = SegformerForImageClassification.from_pretrained('nvidia/mit-b0').to('cuda')\n    inputs = feature_extractor(images=image, return_tensors='pt').to('cuda')\n    outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_idx = logits.argmax(-1).item()\n    predicted_class = model.config.id2label[predicted_class_idx]\n\n    return predicted_class\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/regnet-y-008", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "The RegNet model, trained on Imagenet-1k, is a significant component featured in the paper \"Designing Network Design Spaces.\" It made its debut in this repository and serves as a versatile tool for image classification, providing relevant class outputs.", "function_name": "facebook_regnet_y_008", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be classified.", "default_value": ""}}, "function_code": "def facebook_regnet_y_008(image_path):\n    from transformers import AutoFeatureExtractor, RegNetForImageClassification\n    import torch\n    from datasets import load_dataset\n    from PIL import Image\n\n    image = Image.open(image_path)\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained('zuppif/regnet-y-040')\n    model = RegNetForImageClassification.from_pretrained('zuppif/regnet-y-040').to('cuda')\n\n    inputs = feature_extractor(image, return_tensors='pt').to('cuda')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n    print(model.config.id2label[predicted_label])\n", "executable": false}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Image Captioning", "api_name": "nlpconnect/vit-gpt2-image-captioning", "python_environment_requirements": ["transformers", "torch", "PIL"], "description": "An image captioning model that uses transformers to generate captions for input images. The model is based on the Illustrated Image Captioning using transformers approach.", "function_name": "nlpconnect_vit_gpt2_image_captioning", "function_arguments": {"image_path": {"type": "str", "description": "The path to the input image file", "default_value": ""}}, "function_code": "def nlpconnect_vit_gpt2_image_captioning(image_paths):\n    from transformers import VisionEncoderDecoderModel, ViTImageProcessor, AutoTokenizer\n    import torch\n    from PIL import Image\n\n    model = VisionEncoderDecoderModel.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    feature_extractor = ViTImageProcessor.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    tokenizer = AutoTokenizer.from_pretrained(\"nlpconnect/vit-gpt2-image-captioning\")\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    max_length = 16\n    num_beams = 4\n    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n\n    def predict_step(image_paths):\n        images = []\n        for image_path in image_paths:\n            i_image = Image.open(image_path)\n            if i_image.mode != \"RGB\":\n                i_image = i_image.convert(mode=\"RGB\")\n            images.append(i_image)\n\n        pixel_values = feature_extractor(images=images, return_tensors=\"pt\").pixel_values\n        pixel_values = pixel_values.to(device)\n\n        output_ids = model.generate(pixel_values, **gen_kwargs)\n        preds = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n        preds = [pred.strip() for pred in preds]\n        return preds\n\n    return predict_step(image_paths)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "promptcap-coco-vqa", "python_environment_requirements": ["pip install promptcap"], "description": "PromptCap is a captioning model that can be controlled by natural language instruction. The instruction may contain a question that the user is interested in. It achieves SOTA performance on COCO captioning (150 CIDEr) and knowledge-based VQA tasks when paired with GPT-3 (60.4% on OK-VQA and 59.6% on A-OKVQA).", "function_name": "promptcap_coco_vqa", "function_arguments": {"question": {"type": "str", "description": "The question or instruction provided to the PromptCap captioning model", "default_value": ""}, "image_path": {"type": "str", "description": "The path to the image file for captioning", "default_value": ""}}, "function_code": "def promptcap_coco_vqa(question, image_path):\n    import torch\n    from promptcap import PromptCap\n    import PIL\n\n    model = PromptCap(\"vqascore/promptcap-coco-vqa\")\n\n    if torch.cuda.is_available():\n        model.cuda()\n\n    # image = PIL.Image.open(image_path)\n\n    return model.caption(question, image_path)\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "AICVTG_What_if_a_machine_could_create_captions_automatically", "python_environment_requirements": ["transformers", "torch", "Image"], "description": "This is an image captioning model training by Zayn", "function_name": "openai_clip_vit_base_patch32", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}, "labels": {"type": "str", "description": "Labels are the captions corresponding to the images used for training the image captioning model.", "default_value": ""}}, "function_code": "def openai_clip_vit_base_patch32(image_path, labels):\n    import torch\n    from PIL import Image\n    from torchvision.transforms import functional as F\n    from transformers import CLIPProcessor, CLIPModel\n\n    # Load the CLIP model and processor\n    model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(torch.device(\"cuda\"))\n    processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n\n    # Prepare the images\n    image = Image.open(image_path)\n    image = F.resize(image, (224, 224))\n    image = F.to_tensor(image)\n\n    # Encode the images\n    inputs = processor(text=labels, images=[image], return_tensors=\"pt\", padding=True).to(torch.device(\"cuda\"))\n    # image_features = model.get_image_features(**inputs)\n\n    outputs = model(**inputs)\n    logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n    probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n    return probs\n", "executable": true}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-flan-t5-xl", "python_environment_requirements": ["transformers", "requests", "PIL"], "description": "BLIP-2 model, leveraging Flan T5-xl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The goal for the model is to predict the next text token, giving the query embeddings and the previous text. This allows the model to be used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.", "function_name": "blip2_flan_t5_xl", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file that will be used as input to the BLIP-2 model", "default_value": ""}, "question": {"type": "str", "description": "The previous text or conversation as prompt to the model.", "default_value": ""}}, "function_code": "def blip2_flan_t5_xl(image_path, question):\n    import requests\n    from PIL import Image\n    from transformers import BlipProcessor, Blip2ForConditionalGeneration\n    import torch\n\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xl\").to(torch.device('cuda'))\n\n    raw_image = Image.open(image_path).convert('RGB')\n\n    inputs = processor(raw_image, question, return_tensors=\"pt\").to(torch.device('cuda'))\n    out = model.generate(**inputs)\n\n    return processor.decode(out[0], skip_special_tokens=True)\n", "executable": false}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "blip2-flan-t5-xxl", "python_environment_requirements": ["requests", "PIL", "transformers"], "description": "BLIP-2 model, leveraging Flan T5-xxl (a large language model). It was introduced in the paper BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models by Li et al. and first released in this repository. The model is used for tasks like image captioning, visual question answering (VQA), and chat-like conversations by feeding the image and the previous conversation as prompt to the model.", "function_name": "blip2_flan_t5_xxl", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}, "question": {"type": "str", "description": "The question to be fed as the prompt to the BLIP-2 model", "default_value": ""}}, "function_code": "def blip2_flan_t5_xxl(image_path, question):\n    import requests\n    from PIL import Image\n    from transformers import BlipProcessor, Blip2ForConditionalGeneration\n    import torch\n\n    processor = BlipProcessor.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\")\n    model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-flan-t5-xxl\").to(torch.device('cuda'))\n\n    raw_image = Image.open(image_path).convert('RGB')\n\n    inputs = processor(raw_image, question, return_tensors=\"pt\").to(torch.device('cuda'))\n    out = model.generate(**inputs)\n\n    return processor.decode(out[0], skip_special_tokens=True)\n", "executable": false}
{"domain": "Multimodal Image-to-Text", "framework": "Hugging Face Transformers", "functionality": "Transformers", "api_name": "google/deplot", "python_environment_requirements": ["transformers", "requests", "PIL"], "description": "DePlot is a model that translates the image of a plot or chart to a linearized table. It decomposes the challenge of visual language reasoning into two steps: (1) plot-to-text translation, and (2) reasoning over the translated text. The output of DePlot can then be directly used to prompt a pretrained large language model (LLM), exploiting the few-shot reasoning capabilities of LLMs.", "function_name": "google_deplot", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image of the plot or chart", "default_value": ""}, "text": {"type": "str", "description": "The text to be translated from an image of a plot or chart to a linearized table", "default_value": ""}}, "function_code": "def google_deplot(image_path: str, text: str) -> str:\n    from transformers import Pix2StructForConditionalGeneration, Pix2StructProcessor\n    import requests\n    from PIL import Image\n    import torch\n\n    model = Pix2StructForConditionalGeneration.from_pretrained('google/deplot').to(torch.device('cuda:0'))\n    processor = Pix2StructProcessor.from_pretrained('google/deplot')\n\n    image = Image.open(image_path)\n\n    inputs = processor(images=image, text=text, return_tensors='pt').to(torch.device('cuda:0'))\n\n    predictions = model.generate(**inputs, max_new_tokens=512)\n\n    return processor.decode(predictions[0], skip_special_tokens=True)\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "microsoft/resnet-18", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "ResNet model trained on imagenet-1k. It was introduced in the paper Deep Residual Learning for Image Recognition and first released in this repository. ResNet introduced residual connections, they allow to train networks with an unseen number of layers (up to 1000). ResNet won the 2015 ILSVRC & COCO competition, one important milestone in deep computer vision.", "function_name": "microsoft_resnet_18", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def microsoft_resnet_18(image_path):\n    from transformers import AutoFeatureExtractor, ResNetForImageClassification\n    import torch\n    import PIL\n\n    feature_extractor = AutoFeatureExtractor.from_pretrained('microsoft/resnet-18')\n    model = ResNetForImageClassification.from_pretrained('microsoft/resnet-18')\n    image = PIL.Image.open(image_path)\n    inputs = feature_extractor(image, return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-base-224", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. The authors started from a ResNet and 'modernized' its design by taking the Swin Transformer as inspiration. You can use the raw model for image classification.", "function_name": "facebook_convnext_base_224", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the input image.", "default_value": ""}}, "function_code": "def facebook_convnext_base_224(image_path):\n    from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\n    import torch\n    from datasets import load_dataset\n    import PIL\n\n    dataset = load_dataset('huggingface/cats-image')\n    feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-base-224')\n    model = ConvNextForImageClassification.from_pretrained('facebook/convnext-base-224')\n    image = PIL.Image.open(image_path)\n    inputs = feature_extractor(image, return_tensors='pt')\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "facebook/convnext-tiny-224", "python_environment_requirements": ["transformers", "torch", "datasets"], "description": "ConvNeXT is a pure convolutional model (ConvNet), inspired by the design of Vision Transformers, that claims to outperform them. It is trained on ImageNet-1k at resolution 224x224 and can be used for image classification.", "function_name": "facebook_convnext_tiny_224", "function_arguments": {"image_Path": {"type": "str", "description": "The path to the image file.", "default_value": ""}}, "function_code": "def facebook_convnext_tiny_224(image_Path):\n    from transformers import ConvNextFeatureExtractor, ConvNextForImageClassification\n    import torch\n    from datasets import load_dataset\n    import PIL\n\n    dataset = load_dataset('huggingface/cats-image')\n\n    feature_extractor = ConvNextFeatureExtractor.from_pretrained('facebook/convnext-tiny-224')\n    model = ConvNextForImageClassification.from_pretrained('facebook/convnext-tiny-224').to(torch.device('cuda'))\n\n    image = PIL.Image.open(image_Path)\n    inputs = feature_extractor(image, return_tensors='pt').to(torch.device('cuda'))\n\n    with torch.no_grad():\n        logits = model(**inputs).logits\n\n    predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Image Segmentation", "framework": "Hugging Face Transformers", "functionality": "Image Segmentation", "api_name": "keremberke/yolov8s-pcb-defect-segmentation", "python_environment_requirements": ["ultralyticsplus==0.0.23", "ultralytics==8.0.21"], "description": "YOLOv8s model for PCB defect segmentation. The model is trained to detect and segment PCB defects such as Dry_joint, Incorrect_installation, PCB_damage, and Short_circuit.", "function_name": "keremberke_yolov8s_pcb_defect_segmentation", "function_arguments": {"image_path": {"type": "str", "description": "The file path of the image to be segmented for PCB defects", "default_value": ""}}, "function_code": "def keremberke_yolov8s_pcb_defect_segmentation(image_path):\n    from ultralyticsplus import YOLO, render_result\n    import PIL\n\n    model = YOLO('keremberke/yolov8s-pcb-defect-segmentation')\n    model.overrides['conf'] = 0.25\n    model.overrides['iou'] = 0.45\n    model.overrides['agnostic_nms'] = False\n    model.overrides['max_det'] = 1000\n\n    image = PIL.Image.open(image_path)\n\n    results = model.predict(image)\n    boxes = results[0].boxes\n    masks = results[0].masks\n\n    render = render_result(model=model, image=image, result=results[0])\n    render.show()\n\n    return boxes, masks\n", "executable": false}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Image-to-Image", "api_name": "lllyasviel/sd-controlnet-canny", "python_environment_requirements": ["opencv", "diffusers"], "description": "ControlNet is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on Canny edges. It can be used in combination with Stable Diffusion.", "function_name": "lllyasviel_sd_controlnet_canny", "function_arguments": {"image_path": {"type": "str", "description": "The path of the image to be used for the ControlNet with Canny edges", "default_value": ""}, "output_path": {"type": "str", "description": "The path where the output will be saved", "default_value": ""}, "low_threshold": {"type": "str", "description": "The description of the argument", "default_value": "100"}, "high_threshold": {"type": "str", "description": "The high threshold value for the Canny edge detection algorithm", "default_value": "200"}}, "function_code": "def lllyasviel_sd_controlnet_canny(image_path: str, output_path: str, low_threshold: int=100, high_threshold: int=200) -> None:\n    import cv2\n    from PIL import Image\n    from diffusers import StableDiffusionControlNetPipeline, ControlNetModel, UniPCMultistepScheduler\n    import torch\n    import numpy as np\n    from diffusers.utils import load_image\n    import os\n\n    image = Image.open(image_path)\n    image = np.array(image)\n    image = cv2.Canny(image, low_threshold, high_threshold)\n    image = image[:, :, None]\n    image = np.concatenate([image, image, image], axis=2)\n    image = Image.fromarray(image)\n\n    controlnet = ControlNetModel.from_pretrained(\"lllyasviel/sd-controlnet-canny\", torch_dtype=torch.float16)\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", controlnet=controlnet,\n                                                             safety_checker=None, torch_dtype=torch.float16)\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_xformers_memory_efficient_attention()\n    pipe.enable_model_cpu_offload()\n\n    image = pipe(\"bird\", image, num_inference_steps=5).images[0]\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": false}
{"domain": "Computer Vision Image-to-Image", "framework": "Hugging Face", "functionality": "Diffusion-based text-to-image generation", "api_name": "lllyasviel/control_v11p_sd15_seg", "python_environment_requirements": ["diffusers", "transformers", "accelerate"], "description": "ControlNet v1.1 is a neural network structure to control diffusion models by adding extra conditions. This checkpoint corresponds to the ControlNet conditioned on seg images. It can be used in combination with Stable Diffusion, such as runwayml/stable-diffusion-v1-5.", "function_name": "lllyasviel_control_v11p_sd15_seg", "function_arguments": {"prompt": {"type": "str", "description": "The description of the argument", "default_value": ""}, "control_image_path": {"type": "str", "description": "The path to the control image", "default_value": ""}, "output_path": {"type": "str", "description": "The path where the output will be saved", "default_value": ""}}, "function_code": "def lllyasviel_control_v11p_sd15_seg(prompt, control_image_path, output_path):\n    import torch\n    import os\n    from huggingface_hub import HfApi\n    from pathlib import Path\n    from diffusers.utils import load_image\n    from PIL import Image\n    import numpy as np\n    from transformers import AutoImageProcessor, UperNetForSemanticSegmentation\n\n    from diffusers import (\n        ControlNetModel,\n        StableDiffusionControlNetPipeline,\n        UniPCMultistepScheduler,\n    )\n\n    ada_palette = np.asarray([\n        [0, 0, 0],\n        [120, 120, 120],\n        [180, 120, 120],\n        [6, 230, 230],\n        [80, 50, 50],\n        [4, 200, 3],\n        [120, 120, 80],\n        [140, 140, 140],\n        [204, 5, 255],\n        [230, 230, 230],\n        [4, 250, 7],\n        [224, 5, 255],\n        [235, 255, 7],\n        [150, 5, 61],\n        [120, 120, 70],\n        [8, 255, 51],\n        [255, 6, 82],\n        [143, 255, 140],\n        [204, 255, 4],\n        [255, 51, 7],\n        [204, 70, 3],\n        [0, 102, 200],\n        [61, 230, 250],\n        [255, 6, 51],\n        [11, 102, 255],\n        [255, 7, 71],\n        [255, 9, 224],\n        [9, 7, 230],\n        [220, 220, 220],\n        [255, 9, 92],\n        [112, 9, 255],\n        [8, 255, 214],\n        [7, 255, 224],\n        [255, 184, 6],\n        [10, 255, 71],\n        [255, 41, 10],\n        [7, 255, 255],\n        [224, 255, 8],\n        [102, 8, 255],\n        [255, 61, 6],\n        [255, 194, 7],\n        [255, 122, 8],\n        [0, 255, 20],\n        [255, 8, 41],\n        [255, 5, 153],\n        [6, 51, 255],\n        [235, 12, 255],\n        [160, 150, 20],\n        [0, 163, 255],\n        [140, 140, 140],\n        [250, 10, 15],\n        [20, 255, 0],\n        [31, 255, 0],\n        [255, 31, 0],\n        [255, 224, 0],\n        [153, 255, 0],\n        [0, 0, 255],\n        [255, 71, 0],\n        [0, 235, 255],\n        [0, 173, 255],\n        [31, 0, 255],\n        [11, 200, 200],\n        [255, 82, 0],\n        [0, 255, 245],\n        [0, 61, 255],\n        [0, 255, 112],\n        [0, 255, 133],\n        [255, 0, 0],\n        [255, 163, 0],\n        [255, 102, 0],\n        [194, 255, 0],\n        [0, 143, 255],\n        [51, 255, 0],\n        [0, 82, 255],\n        [0, 255, 41],\n        [0, 255, 173],\n        [10, 0, 255],\n        [173, 255, 0],\n        [0, 255, 153],\n        [255, 92, 0],\n        [255, 0, 255],\n        [255, 0, 245],\n        [255, 0, 102],\n        [255, 173, 0],\n        [255, 0, 20],\n        [255, 184, 184],\n        [0, 31, 255],\n        [0, 255, 61],\n        [0, 71, 255],\n        [255, 0, 204],\n        [0, 255, 194],\n        [0, 255, 82],\n        [0, 10, 255],\n        [0, 112, 255],\n        [51, 0, 255],\n        [0, 194, 255],\n        [0, 122, 255],\n        [0, 255, 163],\n        [255, 153, 0],\n        [0, 255, 10],\n        [255, 112, 0],\n        [143, 255, 0],\n        [82, 0, 255],\n        [163, 255, 0],\n        [255, 235, 0],\n        [8, 184, 170],\n        [133, 0, 255],\n        [0, 255, 92],\n        [184, 0, 255],\n        [255, 0, 31],\n        [0, 184, 255],\n        [0, 214, 255],\n        [255, 0, 112],\n        [92, 255, 0],\n        [0, 224, 255],\n        [112, 224, 255],\n        [70, 184, 160],\n        [163, 0, 255],\n        [153, 0, 255],\n        [71, 255, 0],\n        [255, 0, 163],\n        [255, 204, 0],\n        [255, 0, 143],\n        [0, 255, 235],\n        [133, 255, 0],\n        [255, 0, 235],\n        [245, 0, 255],\n        [255, 0, 122],\n        [255, 245, 0],\n        [10, 190, 212],\n        [214, 255, 0],\n        [0, 204, 255],\n        [20, 0, 255],\n        [255, 255, 0],\n        [0, 153, 255],\n        [0, 41, 255],\n        [0, 255, 204],\n        [41, 0, 255],\n        [41, 255, 0],\n        [173, 0, 255],\n        [0, 245, 255],\n        [71, 0, 255],\n        [122, 0, 255],\n        [0, 255, 184],\n        [0, 92, 255],\n        [184, 255, 0],\n        [0, 133, 255],\n        [255, 214, 0],\n        [25, 194, 194],\n        [102, 255, 0],\n        [92, 0, 255],\n    ])\n\n    image_processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-small\")\n    image_segmentor = UperNetForSemanticSegmentation.from_pretrained(\"openmmlab/upernet-convnext-small\")\n\n    checkpoint = \"lllyasviel/control_v11p_sd15_seg\"\n\n    image = Image.open(control_image_path)\n    # prompt = \"old house in stormy weather with rain and wind\"\n\n    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n    with torch.no_grad():\n        outputs = image_segmentor(pixel_values)\n    seg = image_processor.post_process_semantic_segmentation(outputs, target_sizes=[image.size[::-1]])[0]\n    color_seg = np.zeros((seg.shape[0], seg.shape[1], 3), dtype=np.uint8)  # height, width, 3\n    for label, color in enumerate(ada_palette):\n        color_seg[seg == label, :] = color\n    color_seg = color_seg.astype(np.uint8)\n    control_image = Image.fromarray(color_seg)\n\n    control_image.save(\"./data/control.png\")\n\n    controlnet = ControlNetModel.from_pretrained(checkpoint, torch_dtype=torch.float16).to(\"cuda\")\n    pipe = StableDiffusionControlNetPipeline.from_pretrained(\n        \"runwayml/stable-diffusion-v1-5\", controlnet=controlnet, torch_dtype=torch.float16\n    ).to(\"cuda\")\n\n    pipe.scheduler = UniPCMultistepScheduler.from_config(pipe.scheduler.config)\n    pipe.enable_model_cpu_offload()\n\n    generator = torch.manual_seed(0)\n    image = pipe(prompt, num_inference_steps=30, generator=generator, image=control_image).images[0]\n\n    image.save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Image Synthesis", "api_name": "google/ddpm-cifar10-32", "python_environment_requirements": ["diffusers"], "description": "Denoising Diffusion Probabilistic Models (DDPM) is a class of latent variable models inspired by nonequilibrium thermodynamics. It is used for high-quality image synthesis. The model supports different noise schedulers such as scheduling_ddpm, scheduling_ddim, and scheduling_pndm.", "function_name": "google_ddpm_cifar10_32", "function_arguments": {"output_path": {"type": "str", "description": "The path to the output directory for the generated images.", "default_value": ""}}, "function_code": "def google_ddpm_cifar10_32(output_path):\n    import os\n    import torch\n    from PIL import Image\n    from diffusers import DDPMPipeline\n\n    # Install diffusers if not already installed\n    try:\n        import diffusers\n    except ImportError:\n        #        !pip install diffusers\n        import diffusers\n\n    # Load the DDPMPipeline model\n    ddpm = DDPMPipeline.from_pretrained('google/ddpm-cifar10-32').to(torch.device('cuda'))\n\n    # Generate an image using the DDPMPipeline model\n    image = ddpm().images[0]\n\n    # Save the generated image to the specified output path\n    image.save(output_path)\n\n    # Return the absolute path of the saved image\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Transformers", "functionality": "Unconditional Image Generation", "api_name": "ceyda/butterfly_cropped_uniq1K_512", "python_environment_requirements": ["torch", "huggan.pytorch.lightweight_gan.lightweight_gan"], "description": "Butterfly GAN model based on the paper 'Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis'. The model is intended for fun and learning purposes. It was trained on 1000 images from the huggan/smithsonian_butterflies_subset dataset, with a focus on low data training as mentioned in the paper. The model generates high-quality butterfly images.", "function_name": "ceyda_butterfly_cropped_uniq1K_512", "function_arguments": {"output_path": {"type": "str", "description": "The path to the directory where the generated butterfly images will be saved.", "default_value": ""}}, "function_code": "def ceyda_butterfly_cropped_uniq1K_512(output_path):\n    import torch\n    import numpy as np\n    from PIL import Image\n    from huggan.pytorch.lightweight_gan.lightweight_gan import LightweightGAN\n    import os\n\n    gan = LightweightGAN.from_pretrained(\"ceyda/butterfly_cropped_uniq1K_512\")\n    gan.eval()\n\n    batch_size = 1\n    with torch.no_grad():\n        ims = gan.G(torch.randn(batch_size, gan.latent_dim)).clamp_(0., 1.) * 255\n        ims = ims.permute(0, 2, 3, 1).detach().cpu().numpy().astype(np.uint8)\n\n    Image.fromarray(ims[0]).save(output_path)\n    return os.path.abspath(output_path)\n", "executable": false}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Denoising Diffusion Probabilistic Models (DDPM)", "api_name": "google/ddpm-bedroom-256", "python_environment_requirements": ["diffusers"], "description": "We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.", "function_name": "google_ddpm_bedroom_256", "function_arguments": {"output_path": {"type": "str", "description": "The output path where the generated images will be saved.", "default_value": ""}}, "function_code": "def google_ddpm_bedroom_256(output_path):\n    import os\n    import torch\n    from PIL import Image\n    from diffusers import DDPMPipeline\n\n    # Install diffusers if not already installed\n    try:\n        import diffusers\n    except ImportError:\n        #        !pip install diffusers\n        import diffusers\n\n    model_id = \"google/ddpm-bedroom-256\"\n\n    # Load the DDPMPipeline model\n    ddpm = DDPMPipeline.from_pretrained(model_id).to(torch.device(\"cuda\"))\n\n    # Generate an image using the DDPMPipeline model\n    image = ddpm().images[0]\n\n    # Save the generated image to the specified output path\n    image.save(output_path)\n\n    # Return the path to the saved image\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ncsnpp-church-256", "python_environment_requirements": ["diffusers"], "description": "Score-Based Generative Modeling through Stochastic Differential Equations (SDE) for unconditional image generation. This model achieves record-breaking performance on CIFAR-10 and can generate high fidelity images of size 1024 x 1024.", "function_name": "google_ncsnpp_church_256", "function_arguments": {"output_path": {"type": "str", "description": "The output path to save the generated images", "default_value": ""}}, "function_code": "def google_ncsnpp_church_256(output_path):\n    import diffusers\n    from diffusers import DiffusionPipeline\n    import torch\n    import os\n\n    sde_ve = DiffusionPipeline.from_pretrained('google/ncsnpp-church-256').to(torch.device('cuda:0'))\n    image = sde_ve()[0]\n    image[0].save(output_path)\n    return os.path.abspath(output_path)\n", "executable": true}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "facebook/timesformer-hr-finetuned-k600", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "TimeSformer model pre-trained on Kinetics-600. It was introduced in the paper TimeSformer: Is Space-Time Attention All You Need for Video Understanding? by Tong et al. and first released in this repository. The model can be used for video classification into one of the 600 possible Kinetics-600 labels.", "function_name": "facebook_timesformer_hr_finetuned_k600", "function_arguments": {"video_path": {"type": "str", "description": "The path to the video file that needs to be classified", "default_value": ""}}, "function_code": "def facebook_timesformer_hr_finetuned_k600(video_path):\n    from transformers import AutoImageProcessor, TimesformerForVideoClassification\n    import numpy as np\n    from decord import VideoReader, cpu\n    import torch\n\n    # load the video frames from path\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n\n    # transform video to (time, channel, height, width) format and set height and width to 224\n    video = np.transpose(video, (0, 3, 1, 2))\n    # normalize the video frames\n    min_values = video.min(axis=(1, 2, 3), keepdims=True)\n    max_values = video.max(axis=(1, 2, 3), keepdims=True)\n\n    # Normalize each frame to the range [0, 1]\n    video = (video - min_values) / (max_values - min_values)\n\n    # video = video[:, :, 0:224, 0:224]\n    print(video.shape)\n\n    processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-hr-finetuned-k600\", do_rescale=False)\n    model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-hr-finetuned-k600\").to(torch.device(\"cuda\"))\n    inputs = [processor(images=frame, return_tensors=\"pt\", image_size=(448, 448)).pixel_values for frame in video]\n    inputs = torch.stack(inputs).to(torch.device(\"cuda\")).squeeze()\n    print(inputs.shape)\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n\n    predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": false}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Classification", "api_name": "videomae-small-finetuned-ssv2", "python_environment_requirements": ["transformers", "numpy", "torch"], "description": "VideoMAE is an extension of Masked Autoencoders (MAE) to video. The architecture of the model is very similar to that of a standard Vision Transformer (ViT), with a decoder on top for predicting pixel values for masked patches. Videos are presented to the model as a sequence of fixed-size patches (resolution 16x16), which are linearly embedded. One also adds a [CLS] token to the beginning of a sequence to use it for classification tasks. One also adds fixed sinus/cosinus position embeddings before feeding the sequence to the layers of the Transformer encoder. By pre-training the model, it learns an inner representation of videos that can then be used to extract features useful for downstream tasks: if you have a dataset of labeled videos for instance, you can train a standard classifier by placing a linear layer on top of the pre-trained encoder. One typically places a linear layer on top of the [CLS] token, as the last hidden state of this token can be seen as a representation of an entire video.", "function_name": "videomae_small_finetuned_ssv2", "function_arguments": {"video_path": {"type": "str", "description": "The file path of the video to be processed by the VideoMAE model.", "default_value": ""}}, "function_code": "def videomae_small_finetuned_ssv2(video_path):\n    import numpy as np\n    import torch\n    from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\n    from decord import VideoReader, cpu\n\n    videoreader = VideoReader(video_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n    video = videoreader.get_batch([i for i in range(len(videoreader))]).asnumpy()\n    print(video.shape)\n\n    # transform video to (time, channel, height, width) format and set height and width to 224\n    video = np.transpose(video, (0, 3, 1, 2))\n    # video = video[:, :, 0:224, 0:224]\n    print(video.shape)\n    video = list(np.random.randn(16, 3, 224, 224))\n\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"MCG-NJU/videomae-small-finetuned-ssv2\")\n    model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-small-finetuned-ssv2\")\n    inputs = feature_extractor(video, return_tensors=\"pt\")\n    with torch.no_grad():\n        outputs = model(**inputs)\n        logits = outputs.logits\n        predicted_class_idx = logits.argmax(-1).item()\n    return model.config.id2label[predicted_class_idx]\n", "executable": false}
{"domain": "Computer Vision Video Classification", "framework": "Hugging Face Transformers", "functionality": "Video Action Recognition", "api_name": "videomae-base-finetuned-ucf101", "python_environment_requirements": ["transformers", "decord", "huggingface_hub"], "description": "VideoMAE Base model fine tuned on UCF101 for Video Action Recognition", "function_name": "videomae_base_finetuned_ucf101", "function_arguments": {"file_path": {"type": "str", "description": "The path to the file that needs to be processed", "default_value": ""}, "clip_len": {"type": "int", "description": "The length of each video clip in frames", "default_value": "16"}}, "function_code": "def videomae_base_finetuned_ucf101(file_path: str, clip_len: int=16) -> str:\n    from decord import VideoReader, cpu\n    import torch\n    import numpy as np\n    from transformers import VideoMAEFeatureExtractor, VideoMAEForVideoClassification\n    from huggingface_hub import hf_hub_download\n\n    np.random.seed(0)\n\n    def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n        converted_len = int(clip_len * frame_sample_rate)\n        end_idx = np.random.randint(converted_len, seg_len)\n        start_idx = end_idx - converted_len\n        indices = np.linspace(start_idx, end_idx, num=clip_len)\n        indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n        return indices\n\n\n    videoreader = VideoReader(file_path, num_threads=1, ctx=cpu(0))\n    videoreader.seek(0)\n\n    frame_sample_rate = int(len(videoreader) // clip_len)\n\n    indices = sample_frame_indices(clip_len=clip_len, frame_sample_rate=frame_sample_rate, seg_len=len(videoreader))\n    video = videoreader.get_batch(indices).asnumpy()\n\n    feature_extractor = VideoMAEFeatureExtractor.from_pretrained(\"nateraw/videomae-base-finetuned-ucf101\")\n    model = VideoMAEForVideoClassification.from_pretrained(\"nateraw/videomae-base-finetuned-ucf101\")\n\n    inputs = feature_extractor(list(video), return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    logits = outputs.logits\n    predicted_label = logits.argmax(-1).item()\n\n    return model.config.id2label[predicted_label]\n", "executable": true}
{"domain": "Computer Vision Zero-Shot Image Classification", "framework": "Hugging Face", "functionality": "Zero-Shot Image Classification", "api_name": "CLIPModel.from_pretrained('laion/CLIP-convnext_base_w-laion2B-s13B-b82K')", "python_environment_requirements": ["transformers"], "description": "A series of CLIP ConvNeXt-Base (w/ wide embed dim) models trained on subsets LAION-5B using OpenCLIP. The models achieve between 70.8 and 71.7 zero-shot top-1 accuracy on ImageNet-1k. The models can be used for zero-shot image classification, image and text retrieval, and other related tasks.", "function_name": "laion_CLIP_convnext_base_w_laion2B_s13B_b82K", "function_arguments": {"image_path": {"type": "str", "description": "The file path to the image file for classification, retrieval, or other related tasks.", "default_value": ""}, "labels": {"type": "str", "description": "The labels to be used for zero-shot image classification and other related tasks.", "default_value": ""}}, "function_code": "def laion_CLIP_convnext_base_w_laion2B_s13B_b82K(image_path, labels):\n    from transformers import pipeline\n    clip = pipeline('image-classification', model='laion/CLIP-convnext_base_w-laion2B-s13B-b82K')\n    return clip(image_path, labels)\n", "executable": false}
{"domain": "Computer Vision Unconditional Image Generation", "framework": "Hugging Face Transformers", "functionality": "Unconditional Image Generation", "api_name": "google/ddpm-cat-256", "python_environment_requirements": ["diffusers"], "description": "Our Denoising Diffusion Probabilistic Models (DDPM), inspired by nonequilibrium thermodynamics, excel in generating high-quality images through discrete noise schedulers like scheduling_ddpm, scheduling_ddim, and scheduling_pndm. Trained on both the unconditional CIFAR10 dataset and the challenging 256x256 LSUN dataset, the model achieves impressive scores, boasting an Inception score of 9.46 and a leading FID score of 3.17. Notably, our model effectively utilizes this technique to generate top-notch cat images.", "function_name": "google_ddpm_cat_256", "function_arguments": {"output_image_path": {"type": "str", "description": "The path to the location where the generated output image will be saved", "default_value": ""}}, "function_code": "def google_ddpm_cat_256(output_image_path):\n    import os\n    import torch\n    from PIL import Image\n    from diffusers import DDPMPipeline\n\n    # Install diffusers if not already installed\n    try:\n        import diffusers\n    except ImportError:\n        import diffusers\n\n    model_id = \"google/ddpm-cat-256\"\n\n    # Load the DDPMPipeline model\n    ddpm = DDPMPipeline.from_pretrained(model_id).to(torch.device(\"cuda\"))\n\n    # Generate an image using the DDPMPipeline model\n    image = ddpm().images[0]\n\n    # Save the generated image to the specified output path\n    image.save(output_image_path)\n\n    # Return the path of the saved image\n    return os.path.abspath(output_image_path)\n", "executable": true}
{"domain": "Computer Vision Image Classification", "framework": "Hugging Face Transformers", "functionality": "Image Classification", "api_name": "julien-c/hotdog-not-hotdog", "python_environment_requirements": ["transformers"], "description": "A model that classifies images as hotdog or not hotdog.", "function_name": "hotdog_not_hotdog", "function_arguments": {"image_path": {"type": "str", "description": "The path to the image file", "default_value": ""}}, "function_code": "def hotdog_not_hotdog(image_path):\n    import torch\n    from PIL import Image\n    from torchvision.transforms import functional as F\n    from transformers import ViTFeatureExtractor, ViTForImageClassification\n\n    # Load the pre-trained model and feature extractor\n    model_name = \"julien-c/hotdog-not-hotdog\"\n    feature_extractor = ViTFeatureExtractor.from_pretrained(model_name)\n    model = ViTForImageClassification.from_pretrained(model_name)\n\n    # Load and preprocess the image\n    image = Image.open(image_path)\n    image = F.to_tensor(image)\n    inputs = feature_extractor(images=image, return_tensors=\"pt\")\n\n    # Make predictions\n    outputs = model(**inputs)\n    predictions = torch.argmax(outputs.logits, dim=1)\n\n    # Return the predicted label\n    return predictions.item()\n", "executable": true}
